{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e2cc7a2",
   "metadata": {},
   "source": [
    "<img src=\"velocity_network.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca36ac",
   "metadata": {},
   "source": [
    "<img src=\"radius_network.jpg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a26ca",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4453a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load TKInversion.py\n",
    "################################################\n",
    "########        IMPORT LIBARIES         ########\n",
    "################################################\n",
    "from ParamConfig import *\n",
    "from PathConfig import *\n",
    "from LibConfig import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0146deb2",
   "metadata": {},
   "source": [
    "# Set Velocity Inversion Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1ff672",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "########             NETWORK            ########\n",
    "################################################\n",
    "\n",
    "# Here indicating the GPU you want to use. if you don't have GPU, just leave it.\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device         = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "\n",
    "net = NetModel_v(in_channels=Inchannels,is_deconv=True,is_batchnorm=True,data_dim=DataDim,label_dim=ModelDim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "\n",
    "# Optimizer we want to use\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=LearnRate)\n",
    "\n",
    "# If ReUse, it will load saved model from premodelfilepath and continue to train\n",
    "if ReUse:\n",
    "    print('***************** Loading the pre-trained model *****************')\n",
    "    print('')\n",
    "    premodel_file = models_dir + premodelname + '.pkl'\n",
    "    ##Load generator parameters\n",
    "    net.load_state_dict(torch.load(premodel_file))\n",
    "    net.to(device)\n",
    "    print('Finish downloading:',str(premodel_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74952e3",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "035b0bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** Loading Training DataSet *****************\n",
      "***************** Loading Testing DataSet *****************\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "########        LOADING  DATA           ########\n",
    "################################################\n",
    "print('***************** Loading Training DataSet *****************')\n",
    "train_set,label_set,data_dsp_dim,label_dsp_dim,train_radius  = DataLoad_Train(train_size=TrainSize,train_data_dir=train_data_dir, \\\n",
    "                                                                 data_dim=DataDim, \\\n",
    "                                                                 model_dim=ModelDim,data_dsp_blk=data_dsp_blk, \\\n",
    "                                                                 label_dsp_blk=label_dsp_blk, \\\n",
    "                                                                 truthfilename=truthfilename,velocity_flag=velocity_flag, \\\n",
    "                                                                 position = position)\n",
    "# Change data type (numpy --> tensor)\n",
    "train        = data_utils.TensorDataset(torch.from_numpy(train_set),torch.from_numpy(label_set),torch.from_numpy(train_radius))\n",
    "train_loader = data_utils.DataLoader(train,batch_size=BatchSize,shuffle=True)\n",
    "\n",
    "print('***************** Loading Testing DataSet *****************')\n",
    "\n",
    "test_set,label_set_t,data_dsp_dim_t,label_dsp_dim_t,test_radius = DataLoad_Test(test_size=TestSize,test_data_dir=test_data_dir, \\\n",
    "                                                              data_dim=DataDim, \\\n",
    "                                                              model_dim=ModelDim,data_dsp_blk=data_dsp_blk, \\\n",
    "                                                              label_dsp_blk=label_dsp_blk, \\\n",
    "                                                              truthfilename=truthfilename,velocity_flag=velocity_flag, \\\n",
    "                                                              position = position)\n",
    "\n",
    "test        = data_utils.TensorDataset(torch.from_numpy(test_set),torch.from_numpy(label_set_t),torch.from_numpy(test_radius))\n",
    "test_loader = data_utils.DataLoader(test,batch_size=TestBatchSize,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f356c",
   "metadata": {},
   "source": [
    "# Train Velocity Inversion Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19affd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******************************************\n",
      "*******************************************\n",
      "           START TRAINING                  \n",
      "*******************************************\n",
      "*******************************************\n",
      "\n",
      "Original data dimention:[37, 5001]\n",
      "Downsampled data dimention:(1, 2501) \n",
      "Original label dimention:[40, 2500]\n",
      "Downsampled label dimention:(1, 2500)\n",
      "Training size:500\n",
      "Traning batch size:10\n",
      "Number of epochs:200\n",
      "Learning rate:0.00500\n",
      "Epoch: 1/200, Iteration: 2/10000 --- Training Loss:107.223900\n",
      "Epoch: 1/200, Iteration: 4/10000 --- Training Loss:15.050270\n",
      "Epoch: 1/200, Iteration: 6/10000 --- Training Loss:5.812725\n",
      "Epoch: 1/200, Iteration: 8/10000 --- Training Loss:4.412106\n",
      "Epoch: 1/200, Iteration: 10/10000 --- Training Loss:4.207848\n",
      "Epoch: 1/200, Iteration: 12/10000 --- Training Loss:0.187778\n",
      "Epoch: 1/200, Iteration: 14/10000 --- Training Loss:1.923652\n",
      "Epoch: 1/200, Iteration: 16/10000 --- Training Loss:0.505655\n",
      "Epoch: 1/200, Iteration: 18/10000 --- Training Loss:2.341868\n",
      "Epoch: 1/200, Iteration: 20/10000 --- Training Loss:0.263598\n",
      "Epoch: 1/200, Iteration: 22/10000 --- Training Loss:0.364714\n",
      "Epoch: 1/200, Iteration: 24/10000 --- Training Loss:0.213856\n",
      "Epoch: 1/200, Iteration: 26/10000 --- Training Loss:0.746676\n",
      "Epoch: 1/200, Iteration: 28/10000 --- Training Loss:0.524295\n",
      "Epoch: 1/200, Iteration: 30/10000 --- Training Loss:0.247773\n",
      "Epoch: 1/200, Iteration: 32/10000 --- Training Loss:0.180673\n",
      "Epoch: 1/200, Iteration: 34/10000 --- Training Loss:0.351928\n",
      "Epoch: 1/200, Iteration: 36/10000 --- Training Loss:1.287706\n",
      "Epoch: 1/200, Iteration: 38/10000 --- Training Loss:0.359873\n",
      "Epoch: 1/200, Iteration: 40/10000 --- Training Loss:0.249124\n",
      "Epoch: 1/200, Iteration: 42/10000 --- Training Loss:0.398466\n",
      "Epoch: 1/200, Iteration: 44/10000 --- Training Loss:0.635948\n",
      "Epoch: 1/200, Iteration: 46/10000 --- Training Loss:1.615657\n",
      "Epoch: 1/200, Iteration: 48/10000 --- Training Loss:0.187922\n",
      "Epoch: 1/200, Iteration: 50/10000 --- Training Loss:0.606971\n",
      "Epoch: 1 finished ! Train Loss: 3.66413, Test Loss: 1.21556\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 0 percent completed\n",
      "Epoch: 2/200, Iteration: 52/10000 --- Training Loss:0.097559\n",
      "Epoch: 2/200, Iteration: 54/10000 --- Training Loss:0.618507\n",
      "Epoch: 2/200, Iteration: 56/10000 --- Training Loss:0.123127\n",
      "Epoch: 2/200, Iteration: 58/10000 --- Training Loss:0.068131\n",
      "Epoch: 2/200, Iteration: 60/10000 --- Training Loss:0.358911\n",
      "Epoch: 2/200, Iteration: 62/10000 --- Training Loss:0.069456\n",
      "Epoch: 2/200, Iteration: 64/10000 --- Training Loss:0.299452\n",
      "Epoch: 2/200, Iteration: 66/10000 --- Training Loss:0.038087\n",
      "Epoch: 2/200, Iteration: 68/10000 --- Training Loss:0.350935\n",
      "Epoch: 2/200, Iteration: 70/10000 --- Training Loss:0.111007\n",
      "Epoch: 2/200, Iteration: 72/10000 --- Training Loss:0.344688\n",
      "Epoch: 2/200, Iteration: 74/10000 --- Training Loss:0.023190\n",
      "Epoch: 2/200, Iteration: 76/10000 --- Training Loss:0.261122\n",
      "Epoch: 2/200, Iteration: 78/10000 --- Training Loss:0.067899\n",
      "Epoch: 2/200, Iteration: 80/10000 --- Training Loss:0.310216\n",
      "Epoch: 2/200, Iteration: 82/10000 --- Training Loss:0.040091\n",
      "Epoch: 2/200, Iteration: 84/10000 --- Training Loss:0.129229\n",
      "Epoch: 2/200, Iteration: 86/10000 --- Training Loss:0.048316\n",
      "Epoch: 2/200, Iteration: 88/10000 --- Training Loss:0.162598\n",
      "Epoch: 2/200, Iteration: 90/10000 --- Training Loss:0.019029\n",
      "Epoch: 2/200, Iteration: 92/10000 --- Training Loss:0.068513\n",
      "Epoch: 2/200, Iteration: 94/10000 --- Training Loss:0.022741\n",
      "Epoch: 2/200, Iteration: 96/10000 --- Training Loss:0.268998\n",
      "Epoch: 2/200, Iteration: 98/10000 --- Training Loss:0.031033\n",
      "Epoch: 2/200, Iteration: 100/10000 --- Training Loss:0.042827\n",
      "Epoch: 2 finished ! Train Loss: 0.20479, Test Loss: 0.17575\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 1 percent completed\n",
      "Epoch: 3/200, Iteration: 102/10000 --- Training Loss:0.031360\n",
      "Epoch: 3/200, Iteration: 104/10000 --- Training Loss:0.024936\n",
      "Epoch: 3/200, Iteration: 106/10000 --- Training Loss:0.032951\n",
      "Epoch: 3/200, Iteration: 108/10000 --- Training Loss:0.041389\n",
      "Epoch: 3/200, Iteration: 110/10000 --- Training Loss:0.048417\n",
      "Epoch: 3/200, Iteration: 112/10000 --- Training Loss:0.019076\n",
      "Epoch: 3/200, Iteration: 114/10000 --- Training Loss:0.019526\n",
      "Epoch: 3/200, Iteration: 116/10000 --- Training Loss:0.027044\n",
      "Epoch: 3/200, Iteration: 118/10000 --- Training Loss:0.018366\n",
      "Epoch: 3/200, Iteration: 120/10000 --- Training Loss:0.015022\n",
      "Epoch: 3/200, Iteration: 122/10000 --- Training Loss:0.022162\n",
      "Epoch: 3/200, Iteration: 124/10000 --- Training Loss:0.017712\n",
      "Epoch: 3/200, Iteration: 126/10000 --- Training Loss:0.017062\n",
      "Epoch: 3/200, Iteration: 128/10000 --- Training Loss:0.018104\n",
      "Epoch: 3/200, Iteration: 130/10000 --- Training Loss:0.019652\n",
      "Epoch: 3/200, Iteration: 132/10000 --- Training Loss:0.016045\n",
      "Epoch: 3/200, Iteration: 134/10000 --- Training Loss:0.019450\n",
      "Epoch: 3/200, Iteration: 136/10000 --- Training Loss:0.036233\n",
      "Epoch: 3/200, Iteration: 138/10000 --- Training Loss:0.020268\n",
      "Epoch: 3/200, Iteration: 140/10000 --- Training Loss:0.016841\n",
      "Epoch: 3/200, Iteration: 142/10000 --- Training Loss:0.062309\n",
      "Epoch: 3/200, Iteration: 144/10000 --- Training Loss:0.018618\n",
      "Epoch: 3/200, Iteration: 146/10000 --- Training Loss:0.026256\n",
      "Epoch: 3/200, Iteration: 148/10000 --- Training Loss:0.018591\n",
      "Epoch: 3/200, Iteration: 150/10000 --- Training Loss:0.011814\n",
      "Epoch: 3 finished ! Train Loss: 0.02396, Test Loss: 0.03423\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 1 percent completed\n",
      "Epoch: 4/200, Iteration: 152/10000 --- Training Loss:0.010672\n",
      "Epoch: 4/200, Iteration: 154/10000 --- Training Loss:0.019719\n",
      "Epoch: 4/200, Iteration: 156/10000 --- Training Loss:0.009336\n",
      "Epoch: 4/200, Iteration: 158/10000 --- Training Loss:0.010074\n",
      "Epoch: 4/200, Iteration: 160/10000 --- Training Loss:0.014463\n",
      "Epoch: 4/200, Iteration: 162/10000 --- Training Loss:0.014061\n",
      "Epoch: 4/200, Iteration: 164/10000 --- Training Loss:0.008711\n",
      "Epoch: 4/200, Iteration: 166/10000 --- Training Loss:0.007676\n",
      "Epoch: 4/200, Iteration: 168/10000 --- Training Loss:0.016507\n",
      "Epoch: 4/200, Iteration: 170/10000 --- Training Loss:0.021903\n",
      "Epoch: 4/200, Iteration: 172/10000 --- Training Loss:0.014442\n",
      "Epoch: 4/200, Iteration: 174/10000 --- Training Loss:0.014873\n",
      "Epoch: 4/200, Iteration: 176/10000 --- Training Loss:0.020872\n",
      "Epoch: 4/200, Iteration: 178/10000 --- Training Loss:0.015382\n",
      "Epoch: 4/200, Iteration: 180/10000 --- Training Loss:0.018491\n",
      "Epoch: 4/200, Iteration: 182/10000 --- Training Loss:0.011365\n",
      "Epoch: 4/200, Iteration: 184/10000 --- Training Loss:0.015041\n",
      "Epoch: 4/200, Iteration: 186/10000 --- Training Loss:0.013213\n",
      "Epoch: 4/200, Iteration: 188/10000 --- Training Loss:0.029257\n",
      "Epoch: 4/200, Iteration: 190/10000 --- Training Loss:0.016314\n",
      "Epoch: 4/200, Iteration: 192/10000 --- Training Loss:0.019348\n",
      "Epoch: 4/200, Iteration: 194/10000 --- Training Loss:0.007171\n",
      "Epoch: 4/200, Iteration: 196/10000 --- Training Loss:0.009997\n",
      "Epoch: 4/200, Iteration: 198/10000 --- Training Loss:0.008598\n",
      "Epoch: 4/200, Iteration: 200/10000 --- Training Loss:0.008228\n",
      "Epoch: 4 finished ! Train Loss: 0.01381, Test Loss: 0.01557\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 2 percent completed\n",
      "Epoch: 5/200, Iteration: 202/10000 --- Training Loss:0.009175\n",
      "Epoch: 5/200, Iteration: 204/10000 --- Training Loss:0.009214\n",
      "Epoch: 5/200, Iteration: 206/10000 --- Training Loss:0.011157\n",
      "Epoch: 5/200, Iteration: 208/10000 --- Training Loss:0.009246\n",
      "Epoch: 5/200, Iteration: 210/10000 --- Training Loss:0.058981\n",
      "Epoch: 5/200, Iteration: 212/10000 --- Training Loss:0.015528\n",
      "Epoch: 5/200, Iteration: 214/10000 --- Training Loss:0.014381\n",
      "Epoch: 5/200, Iteration: 216/10000 --- Training Loss:0.017242\n",
      "Epoch: 5/200, Iteration: 218/10000 --- Training Loss:0.018974\n",
      "Epoch: 5/200, Iteration: 220/10000 --- Training Loss:0.021811\n",
      "Epoch: 5/200, Iteration: 222/10000 --- Training Loss:0.012603\n",
      "Epoch: 5/200, Iteration: 224/10000 --- Training Loss:0.015133\n",
      "Epoch: 5/200, Iteration: 226/10000 --- Training Loss:0.012253\n",
      "Epoch: 5/200, Iteration: 228/10000 --- Training Loss:0.014938\n",
      "Epoch: 5/200, Iteration: 230/10000 --- Training Loss:0.014433\n",
      "Epoch: 5/200, Iteration: 232/10000 --- Training Loss:0.008294\n",
      "Epoch: 5/200, Iteration: 234/10000 --- Training Loss:0.007007\n",
      "Epoch: 5/200, Iteration: 236/10000 --- Training Loss:0.026553\n",
      "Epoch: 5/200, Iteration: 238/10000 --- Training Loss:0.015510\n",
      "Epoch: 5/200, Iteration: 240/10000 --- Training Loss:0.005388\n",
      "Epoch: 5/200, Iteration: 242/10000 --- Training Loss:0.008724\n",
      "Epoch: 5/200, Iteration: 244/10000 --- Training Loss:0.018571\n",
      "Epoch: 5/200, Iteration: 246/10000 --- Training Loss:0.010029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/200, Iteration: 248/10000 --- Training Loss:0.011978\n",
      "Epoch: 5/200, Iteration: 250/10000 --- Training Loss:0.029269\n",
      "Epoch: 5 finished ! Train Loss: 0.01477, Test Loss: 0.01177\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 2 percent completed\n",
      "Epoch: 6/200, Iteration: 252/10000 --- Training Loss:0.007527\n",
      "Epoch: 6/200, Iteration: 254/10000 --- Training Loss:0.007901\n",
      "Epoch: 6/200, Iteration: 256/10000 --- Training Loss:0.011368\n",
      "Epoch: 6/200, Iteration: 258/10000 --- Training Loss:0.014096\n",
      "Epoch: 6/200, Iteration: 260/10000 --- Training Loss:0.006134\n",
      "Epoch: 6/200, Iteration: 262/10000 --- Training Loss:0.012918\n",
      "Epoch: 6/200, Iteration: 264/10000 --- Training Loss:0.009460\n",
      "Epoch: 6/200, Iteration: 266/10000 --- Training Loss:0.063768\n",
      "Epoch: 6/200, Iteration: 268/10000 --- Training Loss:0.044405\n",
      "Epoch: 6/200, Iteration: 270/10000 --- Training Loss:0.100174\n",
      "Epoch: 6/200, Iteration: 272/10000 --- Training Loss:0.047015\n",
      "Epoch: 6/200, Iteration: 274/10000 --- Training Loss:0.078694\n",
      "Epoch: 6/200, Iteration: 276/10000 --- Training Loss:0.087205\n",
      "Epoch: 6/200, Iteration: 278/10000 --- Training Loss:0.073398\n",
      "Epoch: 6/200, Iteration: 280/10000 --- Training Loss:0.094291\n",
      "Epoch: 6/200, Iteration: 282/10000 --- Training Loss:0.099946\n",
      "Epoch: 6/200, Iteration: 284/10000 --- Training Loss:0.063999\n",
      "Epoch: 6/200, Iteration: 286/10000 --- Training Loss:0.131268\n",
      "Epoch: 6/200, Iteration: 288/10000 --- Training Loss:0.017512\n",
      "Epoch: 6/200, Iteration: 290/10000 --- Training Loss:0.045527\n",
      "Epoch: 6/200, Iteration: 292/10000 --- Training Loss:0.062876\n",
      "Epoch: 6/200, Iteration: 294/10000 --- Training Loss:0.033144\n",
      "Epoch: 6/200, Iteration: 296/10000 --- Training Loss:0.027043\n",
      "Epoch: 6/200, Iteration: 298/10000 --- Training Loss:0.043694\n",
      "Epoch: 6/200, Iteration: 300/10000 --- Training Loss:0.025373\n",
      "Epoch: 6 finished ! Train Loss: 0.05853, Test Loss: 0.03330\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 7/200, Iteration: 302/10000 --- Training Loss:0.014402\n",
      "Epoch: 7/200, Iteration: 304/10000 --- Training Loss:0.021476\n",
      "Epoch: 7/200, Iteration: 306/10000 --- Training Loss:0.022082\n",
      "Epoch: 7/200, Iteration: 308/10000 --- Training Loss:0.013927\n",
      "Epoch: 7/200, Iteration: 310/10000 --- Training Loss:0.022595\n",
      "Epoch: 7/200, Iteration: 312/10000 --- Training Loss:0.054989\n",
      "Epoch: 7/200, Iteration: 314/10000 --- Training Loss:0.021119\n",
      "Epoch: 7/200, Iteration: 316/10000 --- Training Loss:0.023823\n",
      "Epoch: 7/200, Iteration: 318/10000 --- Training Loss:0.016856\n",
      "Epoch: 7/200, Iteration: 320/10000 --- Training Loss:0.015193\n",
      "Epoch: 7/200, Iteration: 322/10000 --- Training Loss:0.015118\n",
      "Epoch: 7/200, Iteration: 324/10000 --- Training Loss:0.009616\n",
      "Epoch: 7/200, Iteration: 326/10000 --- Training Loss:0.018891\n",
      "Epoch: 7/200, Iteration: 328/10000 --- Training Loss:0.015994\n",
      "Epoch: 7/200, Iteration: 330/10000 --- Training Loss:0.014637\n",
      "Epoch: 7/200, Iteration: 332/10000 --- Training Loss:0.022803\n",
      "Epoch: 7/200, Iteration: 334/10000 --- Training Loss:0.017681\n",
      "Epoch: 7/200, Iteration: 336/10000 --- Training Loss:0.009387\n",
      "Epoch: 7/200, Iteration: 338/10000 --- Training Loss:0.006496\n",
      "Epoch: 7/200, Iteration: 340/10000 --- Training Loss:0.010809\n",
      "Epoch: 7/200, Iteration: 342/10000 --- Training Loss:0.006803\n",
      "Epoch: 7/200, Iteration: 344/10000 --- Training Loss:0.011149\n",
      "Epoch: 7/200, Iteration: 346/10000 --- Training Loss:0.007339\n",
      "Epoch: 7/200, Iteration: 348/10000 --- Training Loss:0.012809\n",
      "Epoch: 7/200, Iteration: 350/10000 --- Training Loss:0.010503\n",
      "Epoch: 7 finished ! Train Loss: 0.01867, Test Loss: 0.17356\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 8/200, Iteration: 352/10000 --- Training Loss:0.005077\n",
      "Epoch: 8/200, Iteration: 354/10000 --- Training Loss:0.012952\n",
      "Epoch: 8/200, Iteration: 356/10000 --- Training Loss:0.043105\n",
      "Epoch: 8/200, Iteration: 358/10000 --- Training Loss:0.057230\n",
      "Epoch: 8/200, Iteration: 360/10000 --- Training Loss:0.010214\n",
      "Epoch: 8/200, Iteration: 362/10000 --- Training Loss:0.007559\n",
      "Epoch: 8/200, Iteration: 364/10000 --- Training Loss:0.010806\n",
      "Epoch: 8/200, Iteration: 366/10000 --- Training Loss:0.008337\n",
      "Epoch: 8/200, Iteration: 368/10000 --- Training Loss:0.011958\n",
      "Epoch: 8/200, Iteration: 370/10000 --- Training Loss:0.014491\n",
      "Epoch: 8/200, Iteration: 372/10000 --- Training Loss:0.008379\n",
      "Epoch: 8/200, Iteration: 374/10000 --- Training Loss:0.010605\n",
      "Epoch: 8/200, Iteration: 376/10000 --- Training Loss:0.011418\n",
      "Epoch: 8/200, Iteration: 378/10000 --- Training Loss:0.012925\n",
      "Epoch: 8/200, Iteration: 380/10000 --- Training Loss:0.006145\n",
      "Epoch: 8/200, Iteration: 382/10000 --- Training Loss:0.013100\n",
      "Epoch: 8/200, Iteration: 384/10000 --- Training Loss:0.008912\n",
      "Epoch: 8/200, Iteration: 386/10000 --- Training Loss:0.028953\n",
      "Epoch: 8/200, Iteration: 388/10000 --- Training Loss:0.011269\n",
      "Epoch: 8/200, Iteration: 390/10000 --- Training Loss:0.007556\n",
      "Epoch: 8/200, Iteration: 392/10000 --- Training Loss:0.007798\n",
      "Epoch: 8/200, Iteration: 394/10000 --- Training Loss:0.014660\n",
      "Epoch: 8/200, Iteration: 396/10000 --- Training Loss:0.009285\n",
      "Epoch: 8/200, Iteration: 398/10000 --- Training Loss:0.028158\n",
      "Epoch: 8/200, Iteration: 400/10000 --- Training Loss:0.022413\n",
      "Epoch: 8 finished ! Train Loss: 0.01380, Test Loss: 0.07217\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 9/200, Iteration: 402/10000 --- Training Loss:0.015016\n",
      "Epoch: 9/200, Iteration: 404/10000 --- Training Loss:0.004957\n",
      "Epoch: 9/200, Iteration: 406/10000 --- Training Loss:0.009755\n",
      "Epoch: 9/200, Iteration: 408/10000 --- Training Loss:0.012941\n",
      "Epoch: 9/200, Iteration: 410/10000 --- Training Loss:0.010964\n",
      "Epoch: 9/200, Iteration: 412/10000 --- Training Loss:0.010030\n",
      "Epoch: 9/200, Iteration: 414/10000 --- Training Loss:0.006104\n",
      "Epoch: 9/200, Iteration: 416/10000 --- Training Loss:0.006231\n",
      "Epoch: 9/200, Iteration: 418/10000 --- Training Loss:0.008318\n",
      "Epoch: 9/200, Iteration: 420/10000 --- Training Loss:0.006287\n",
      "Epoch: 9/200, Iteration: 422/10000 --- Training Loss:0.007014\n",
      "Epoch: 9/200, Iteration: 424/10000 --- Training Loss:0.006799\n",
      "Epoch: 9/200, Iteration: 426/10000 --- Training Loss:0.005779\n",
      "Epoch: 9/200, Iteration: 428/10000 --- Training Loss:0.005310\n",
      "Epoch: 9/200, Iteration: 430/10000 --- Training Loss:0.006613\n",
      "Epoch: 9/200, Iteration: 432/10000 --- Training Loss:0.006882\n",
      "Epoch: 9/200, Iteration: 434/10000 --- Training Loss:0.005071\n",
      "Epoch: 9/200, Iteration: 436/10000 --- Training Loss:0.006429\n",
      "Epoch: 9/200, Iteration: 438/10000 --- Training Loss:0.004754\n",
      "Epoch: 9/200, Iteration: 440/10000 --- Training Loss:0.005973\n",
      "Epoch: 9/200, Iteration: 442/10000 --- Training Loss:0.008064\n",
      "Epoch: 9/200, Iteration: 444/10000 --- Training Loss:0.005428\n",
      "Epoch: 9/200, Iteration: 446/10000 --- Training Loss:0.003847\n",
      "Epoch: 9/200, Iteration: 448/10000 --- Training Loss:0.007820\n",
      "Epoch: 9/200, Iteration: 450/10000 --- Training Loss:0.005052\n",
      "Epoch: 9 finished ! Train Loss: 0.00725, Test Loss: 0.16455\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 10/200, Iteration: 452/10000 --- Training Loss:0.006244\n",
      "Epoch: 10/200, Iteration: 454/10000 --- Training Loss:0.015261\n",
      "Epoch: 10/200, Iteration: 456/10000 --- Training Loss:0.014794\n",
      "Epoch: 10/200, Iteration: 458/10000 --- Training Loss:0.009053\n",
      "Epoch: 10/200, Iteration: 460/10000 --- Training Loss:0.004966\n",
      "Epoch: 10/200, Iteration: 462/10000 --- Training Loss:0.013178\n",
      "Epoch: 10/200, Iteration: 464/10000 --- Training Loss:0.006129\n",
      "Epoch: 10/200, Iteration: 466/10000 --- Training Loss:0.007193\n",
      "Epoch: 10/200, Iteration: 468/10000 --- Training Loss:0.006587\n",
      "Epoch: 10/200, Iteration: 470/10000 --- Training Loss:0.002581\n",
      "Epoch: 10/200, Iteration: 472/10000 --- Training Loss:0.005772\n",
      "Epoch: 10/200, Iteration: 474/10000 --- Training Loss:0.003590\n",
      "Epoch: 10/200, Iteration: 476/10000 --- Training Loss:0.003508\n",
      "Epoch: 10/200, Iteration: 478/10000 --- Training Loss:0.005380\n",
      "Epoch: 10/200, Iteration: 480/10000 --- Training Loss:0.002935\n",
      "Epoch: 10/200, Iteration: 482/10000 --- Training Loss:0.003658\n",
      "Epoch: 10/200, Iteration: 484/10000 --- Training Loss:0.009674\n",
      "Epoch: 10/200, Iteration: 486/10000 --- Training Loss:0.009650\n",
      "Epoch: 10/200, Iteration: 488/10000 --- Training Loss:0.010690\n",
      "Epoch: 10/200, Iteration: 490/10000 --- Training Loss:0.005451\n",
      "Epoch: 10/200, Iteration: 492/10000 --- Training Loss:0.003651\n",
      "Epoch: 10/200, Iteration: 494/10000 --- Training Loss:0.005899\n",
      "Epoch: 10/200, Iteration: 496/10000 --- Training Loss:0.006714\n",
      "Epoch: 10/200, Iteration: 498/10000 --- Training Loss:0.003830\n",
      "Epoch: 10/200, Iteration: 500/10000 --- Training Loss:0.004383\n",
      "Epoch: 10 finished ! Train Loss: 0.00681, Test Loss: 0.12185\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 11/200, Iteration: 502/10000 --- Training Loss:0.011023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/200, Iteration: 504/10000 --- Training Loss:0.008037\n",
      "Epoch: 11/200, Iteration: 506/10000 --- Training Loss:0.009540\n",
      "Epoch: 11/200, Iteration: 508/10000 --- Training Loss:0.010215\n",
      "Epoch: 11/200, Iteration: 510/10000 --- Training Loss:0.024504\n",
      "Epoch: 11/200, Iteration: 512/10000 --- Training Loss:0.005805\n",
      "Epoch: 11/200, Iteration: 514/10000 --- Training Loss:0.006163\n",
      "Epoch: 11/200, Iteration: 516/10000 --- Training Loss:0.006090\n",
      "Epoch: 11/200, Iteration: 518/10000 --- Training Loss:0.011341\n",
      "Epoch: 11/200, Iteration: 520/10000 --- Training Loss:0.008731\n",
      "Epoch: 11/200, Iteration: 522/10000 --- Training Loss:0.008673\n",
      "Epoch: 11/200, Iteration: 524/10000 --- Training Loss:0.007020\n",
      "Epoch: 11/200, Iteration: 526/10000 --- Training Loss:0.015808\n",
      "Epoch: 11/200, Iteration: 528/10000 --- Training Loss:0.011150\n",
      "Epoch: 11/200, Iteration: 530/10000 --- Training Loss:0.005774\n",
      "Epoch: 11/200, Iteration: 532/10000 --- Training Loss:0.004733\n",
      "Epoch: 11/200, Iteration: 534/10000 --- Training Loss:0.005074\n",
      "Epoch: 11/200, Iteration: 536/10000 --- Training Loss:0.006063\n",
      "Epoch: 11/200, Iteration: 538/10000 --- Training Loss:0.004876\n",
      "Epoch: 11/200, Iteration: 540/10000 --- Training Loss:0.005517\n",
      "Epoch: 11/200, Iteration: 542/10000 --- Training Loss:0.006129\n",
      "Epoch: 11/200, Iteration: 544/10000 --- Training Loss:0.007226\n",
      "Epoch: 11/200, Iteration: 546/10000 --- Training Loss:0.004217\n",
      "Epoch: 11/200, Iteration: 548/10000 --- Training Loss:0.008913\n",
      "Epoch: 11/200, Iteration: 550/10000 --- Training Loss:0.003804\n",
      "Epoch: 11 finished ! Train Loss: 0.00776, Test Loss: 0.04226\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 12/200, Iteration: 552/10000 --- Training Loss:0.005602\n",
      "Epoch: 12/200, Iteration: 554/10000 --- Training Loss:0.034817\n",
      "Epoch: 12/200, Iteration: 556/10000 --- Training Loss:0.009235\n",
      "Epoch: 12/200, Iteration: 558/10000 --- Training Loss:0.007699\n",
      "Epoch: 12/200, Iteration: 560/10000 --- Training Loss:0.003013\n",
      "Epoch: 12/200, Iteration: 562/10000 --- Training Loss:0.019431\n",
      "Epoch: 12/200, Iteration: 564/10000 --- Training Loss:0.007159\n",
      "Epoch: 12/200, Iteration: 566/10000 --- Training Loss:0.006037\n",
      "Epoch: 12/200, Iteration: 568/10000 --- Training Loss:0.004423\n",
      "Epoch: 12/200, Iteration: 570/10000 --- Training Loss:0.005506\n",
      "Epoch: 12/200, Iteration: 572/10000 --- Training Loss:0.007852\n",
      "Epoch: 12/200, Iteration: 574/10000 --- Training Loss:0.019089\n",
      "Epoch: 12/200, Iteration: 576/10000 --- Training Loss:0.007443\n",
      "Epoch: 12/200, Iteration: 578/10000 --- Training Loss:0.024744\n",
      "Epoch: 12/200, Iteration: 580/10000 --- Training Loss:0.021673\n",
      "Epoch: 12/200, Iteration: 582/10000 --- Training Loss:0.029057\n",
      "Epoch: 12/200, Iteration: 584/10000 --- Training Loss:0.024435\n",
      "Epoch: 12/200, Iteration: 586/10000 --- Training Loss:0.010285\n",
      "Epoch: 12/200, Iteration: 588/10000 --- Training Loss:0.004654\n",
      "Epoch: 12/200, Iteration: 590/10000 --- Training Loss:0.011520\n",
      "Epoch: 12/200, Iteration: 592/10000 --- Training Loss:0.016631\n",
      "Epoch: 12/200, Iteration: 594/10000 --- Training Loss:0.015016\n",
      "Epoch: 12/200, Iteration: 596/10000 --- Training Loss:0.007952\n",
      "Epoch: 12/200, Iteration: 598/10000 --- Training Loss:0.006916\n",
      "Epoch: 12/200, Iteration: 600/10000 --- Training Loss:0.082734\n",
      "Epoch: 12 finished ! Train Loss: 0.01496, Test Loss: 0.80994\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 13/200, Iteration: 602/10000 --- Training Loss:0.019118\n",
      "Epoch: 13/200, Iteration: 604/10000 --- Training Loss:0.064834\n",
      "Epoch: 13/200, Iteration: 606/10000 --- Training Loss:0.021795\n",
      "Epoch: 13/200, Iteration: 608/10000 --- Training Loss:0.017584\n",
      "Epoch: 13/200, Iteration: 610/10000 --- Training Loss:0.017731\n",
      "Epoch: 13/200, Iteration: 612/10000 --- Training Loss:0.015712\n",
      "Epoch: 13/200, Iteration: 614/10000 --- Training Loss:0.015368\n",
      "Epoch: 13/200, Iteration: 616/10000 --- Training Loss:0.007784\n",
      "Epoch: 13/200, Iteration: 618/10000 --- Training Loss:0.006525\n",
      "Epoch: 13/200, Iteration: 620/10000 --- Training Loss:0.008276\n",
      "Epoch: 13/200, Iteration: 622/10000 --- Training Loss:0.027625\n",
      "Epoch: 13/200, Iteration: 624/10000 --- Training Loss:0.017860\n",
      "Epoch: 13/200, Iteration: 626/10000 --- Training Loss:0.023271\n",
      "Epoch: 13/200, Iteration: 628/10000 --- Training Loss:0.016330\n",
      "Epoch: 13/200, Iteration: 630/10000 --- Training Loss:0.009682\n",
      "Epoch: 13/200, Iteration: 632/10000 --- Training Loss:0.010875\n",
      "Epoch: 13/200, Iteration: 634/10000 --- Training Loss:0.011321\n",
      "Epoch: 13/200, Iteration: 636/10000 --- Training Loss:0.018599\n",
      "Epoch: 13/200, Iteration: 638/10000 --- Training Loss:0.011524\n",
      "Epoch: 13/200, Iteration: 640/10000 --- Training Loss:0.021756\n",
      "Epoch: 13/200, Iteration: 642/10000 --- Training Loss:0.012439\n",
      "Epoch: 13/200, Iteration: 644/10000 --- Training Loss:0.004498\n",
      "Epoch: 13/200, Iteration: 646/10000 --- Training Loss:0.010984\n",
      "Epoch: 13/200, Iteration: 648/10000 --- Training Loss:0.044334\n",
      "Epoch: 13/200, Iteration: 650/10000 --- Training Loss:0.092872\n",
      "Epoch: 13 finished ! Train Loss: 0.02812, Test Loss: 0.03846\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 14/200, Iteration: 652/10000 --- Training Loss:0.067744\n",
      "Epoch: 14/200, Iteration: 654/10000 --- Training Loss:0.017495\n",
      "Epoch: 14/200, Iteration: 656/10000 --- Training Loss:0.010117\n",
      "Epoch: 14/200, Iteration: 658/10000 --- Training Loss:0.023437\n",
      "Epoch: 14/200, Iteration: 660/10000 --- Training Loss:0.020807\n",
      "Epoch: 14/200, Iteration: 662/10000 --- Training Loss:0.012506\n",
      "Epoch: 14/200, Iteration: 664/10000 --- Training Loss:0.016484\n",
      "Epoch: 14/200, Iteration: 666/10000 --- Training Loss:0.006579\n",
      "Epoch: 14/200, Iteration: 668/10000 --- Training Loss:0.004151\n",
      "Epoch: 14/200, Iteration: 670/10000 --- Training Loss:0.003980\n",
      "Epoch: 14/200, Iteration: 672/10000 --- Training Loss:0.006813\n",
      "Epoch: 14/200, Iteration: 674/10000 --- Training Loss:0.005212\n",
      "Epoch: 14/200, Iteration: 676/10000 --- Training Loss:0.005746\n",
      "Epoch: 14/200, Iteration: 678/10000 --- Training Loss:0.004107\n",
      "Epoch: 14/200, Iteration: 680/10000 --- Training Loss:0.005931\n",
      "Epoch: 14/200, Iteration: 682/10000 --- Training Loss:0.010333\n",
      "Epoch: 14/200, Iteration: 684/10000 --- Training Loss:0.005134\n",
      "Epoch: 14/200, Iteration: 686/10000 --- Training Loss:0.010563\n",
      "Epoch: 14/200, Iteration: 688/10000 --- Training Loss:0.006340\n",
      "Epoch: 14/200, Iteration: 690/10000 --- Training Loss:0.006494\n",
      "Epoch: 14/200, Iteration: 692/10000 --- Training Loss:0.009008\n",
      "Epoch: 14/200, Iteration: 694/10000 --- Training Loss:0.006535\n",
      "Epoch: 14/200, Iteration: 696/10000 --- Training Loss:0.011232\n",
      "Epoch: 14/200, Iteration: 698/10000 --- Training Loss:0.011876\n",
      "Epoch: 14/200, Iteration: 700/10000 --- Training Loss:0.006753\n",
      "Epoch: 14 finished ! Train Loss: 0.01196, Test Loss: 0.00878\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 7 percent completed\n",
      "Epoch: 15/200, Iteration: 702/10000 --- Training Loss:0.004830\n",
      "Epoch: 15/200, Iteration: 704/10000 --- Training Loss:0.004698\n",
      "Epoch: 15/200, Iteration: 706/10000 --- Training Loss:0.005168\n",
      "Epoch: 15/200, Iteration: 708/10000 --- Training Loss:0.005841\n",
      "Epoch: 15/200, Iteration: 710/10000 --- Training Loss:0.006123\n",
      "Epoch: 15/200, Iteration: 712/10000 --- Training Loss:0.011131\n",
      "Epoch: 15/200, Iteration: 714/10000 --- Training Loss:0.011389\n",
      "Epoch: 15/200, Iteration: 716/10000 --- Training Loss:0.011632\n",
      "Epoch: 15/200, Iteration: 718/10000 --- Training Loss:0.004290\n",
      "Epoch: 15/200, Iteration: 720/10000 --- Training Loss:0.005104\n",
      "Epoch: 15/200, Iteration: 722/10000 --- Training Loss:0.006015\n",
      "Epoch: 15/200, Iteration: 724/10000 --- Training Loss:0.008439\n",
      "Epoch: 15/200, Iteration: 726/10000 --- Training Loss:0.010742\n",
      "Epoch: 15/200, Iteration: 728/10000 --- Training Loss:0.005451\n",
      "Epoch: 15/200, Iteration: 730/10000 --- Training Loss:0.008619\n",
      "Epoch: 15/200, Iteration: 732/10000 --- Training Loss:0.004898\n",
      "Epoch: 15/200, Iteration: 734/10000 --- Training Loss:0.005558\n",
      "Epoch: 15/200, Iteration: 736/10000 --- Training Loss:0.005889\n",
      "Epoch: 15/200, Iteration: 738/10000 --- Training Loss:0.009230\n",
      "Epoch: 15/200, Iteration: 740/10000 --- Training Loss:0.006816\n",
      "Epoch: 15/200, Iteration: 742/10000 --- Training Loss:0.003975\n",
      "Epoch: 15/200, Iteration: 744/10000 --- Training Loss:0.005596\n",
      "Epoch: 15/200, Iteration: 746/10000 --- Training Loss:0.002289\n",
      "Epoch: 15/200, Iteration: 748/10000 --- Training Loss:0.004297\n",
      "Epoch: 15/200, Iteration: 750/10000 --- Training Loss:0.006878\n",
      "Epoch: 15 finished ! Train Loss: 0.00804, Test Loss: 0.00386\n",
      "Epoch consuming time: 0m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved: 7 percent completed\n",
      "Epoch: 16/200, Iteration: 752/10000 --- Training Loss:0.008461\n",
      "Epoch: 16/200, Iteration: 754/10000 --- Training Loss:0.004278\n",
      "Epoch: 16/200, Iteration: 756/10000 --- Training Loss:0.004628\n",
      "Epoch: 16/200, Iteration: 758/10000 --- Training Loss:0.004539\n",
      "Epoch: 16/200, Iteration: 760/10000 --- Training Loss:0.007682\n",
      "Epoch: 16/200, Iteration: 762/10000 --- Training Loss:0.002863\n",
      "Epoch: 16/200, Iteration: 764/10000 --- Training Loss:0.005426\n",
      "Epoch: 16/200, Iteration: 766/10000 --- Training Loss:0.007876\n",
      "Epoch: 16/200, Iteration: 768/10000 --- Training Loss:0.004795\n",
      "Epoch: 16/200, Iteration: 770/10000 --- Training Loss:0.007667\n",
      "Epoch: 16/200, Iteration: 772/10000 --- Training Loss:0.007658\n",
      "Epoch: 16/200, Iteration: 774/10000 --- Training Loss:0.005318\n",
      "Epoch: 16/200, Iteration: 776/10000 --- Training Loss:0.004135\n",
      "Epoch: 16/200, Iteration: 778/10000 --- Training Loss:0.004438\n",
      "Epoch: 16/200, Iteration: 780/10000 --- Training Loss:0.004673\n",
      "Epoch: 16/200, Iteration: 782/10000 --- Training Loss:0.003501\n",
      "Epoch: 16/200, Iteration: 784/10000 --- Training Loss:0.005225\n",
      "Epoch: 16/200, Iteration: 786/10000 --- Training Loss:0.006444\n",
      "Epoch: 16/200, Iteration: 788/10000 --- Training Loss:0.004532\n",
      "Epoch: 16/200, Iteration: 790/10000 --- Training Loss:0.003011\n",
      "Epoch: 16/200, Iteration: 792/10000 --- Training Loss:0.002485\n",
      "Epoch: 16/200, Iteration: 794/10000 --- Training Loss:0.023473\n",
      "Epoch: 16/200, Iteration: 796/10000 --- Training Loss:0.006969\n",
      "Epoch: 16/200, Iteration: 798/10000 --- Training Loss:0.014557\n",
      "Epoch: 16/200, Iteration: 800/10000 --- Training Loss:0.017349\n",
      "Epoch: 16 finished ! Train Loss: 0.00783, Test Loss: 0.04147\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 17/200, Iteration: 802/10000 --- Training Loss:0.035569\n",
      "Epoch: 17/200, Iteration: 804/10000 --- Training Loss:0.051143\n",
      "Epoch: 17/200, Iteration: 806/10000 --- Training Loss:0.008420\n",
      "Epoch: 17/200, Iteration: 808/10000 --- Training Loss:0.010047\n",
      "Epoch: 17/200, Iteration: 810/10000 --- Training Loss:0.004706\n",
      "Epoch: 17/200, Iteration: 812/10000 --- Training Loss:0.006050\n",
      "Epoch: 17/200, Iteration: 814/10000 --- Training Loss:0.005055\n",
      "Epoch: 17/200, Iteration: 816/10000 --- Training Loss:0.004958\n",
      "Epoch: 17/200, Iteration: 818/10000 --- Training Loss:0.005806\n",
      "Epoch: 17/200, Iteration: 820/10000 --- Training Loss:0.003705\n",
      "Epoch: 17/200, Iteration: 822/10000 --- Training Loss:0.004261\n",
      "Epoch: 17/200, Iteration: 824/10000 --- Training Loss:0.010040\n",
      "Epoch: 17/200, Iteration: 826/10000 --- Training Loss:0.005706\n",
      "Epoch: 17/200, Iteration: 828/10000 --- Training Loss:0.003350\n",
      "Epoch: 17/200, Iteration: 830/10000 --- Training Loss:0.003638\n",
      "Epoch: 17/200, Iteration: 832/10000 --- Training Loss:0.003342\n",
      "Epoch: 17/200, Iteration: 834/10000 --- Training Loss:0.002560\n",
      "Epoch: 17/200, Iteration: 836/10000 --- Training Loss:0.003108\n",
      "Epoch: 17/200, Iteration: 838/10000 --- Training Loss:0.002576\n",
      "Epoch: 17/200, Iteration: 840/10000 --- Training Loss:0.003550\n",
      "Epoch: 17/200, Iteration: 842/10000 --- Training Loss:0.004600\n",
      "Epoch: 17/200, Iteration: 844/10000 --- Training Loss:0.004727\n",
      "Epoch: 17/200, Iteration: 846/10000 --- Training Loss:0.005572\n",
      "Epoch: 17/200, Iteration: 848/10000 --- Training Loss:0.003724\n",
      "Epoch: 17/200, Iteration: 850/10000 --- Training Loss:0.002923\n",
      "Epoch: 17 finished ! Train Loss: 0.00705, Test Loss: 0.01114\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 18/200, Iteration: 852/10000 --- Training Loss:0.002790\n",
      "Epoch: 18/200, Iteration: 854/10000 --- Training Loss:0.003201\n",
      "Epoch: 18/200, Iteration: 856/10000 --- Training Loss:0.002888\n",
      "Epoch: 18/200, Iteration: 858/10000 --- Training Loss:0.002693\n",
      "Epoch: 18/200, Iteration: 860/10000 --- Training Loss:0.004069\n",
      "Epoch: 18/200, Iteration: 862/10000 --- Training Loss:0.019968\n",
      "Epoch: 18/200, Iteration: 864/10000 --- Training Loss:0.017657\n",
      "Epoch: 18/200, Iteration: 866/10000 --- Training Loss:0.013735\n",
      "Epoch: 18/200, Iteration: 868/10000 --- Training Loss:0.012026\n",
      "Epoch: 18/200, Iteration: 870/10000 --- Training Loss:0.010423\n",
      "Epoch: 18/200, Iteration: 872/10000 --- Training Loss:0.006983\n",
      "Epoch: 18/200, Iteration: 874/10000 --- Training Loss:0.007808\n",
      "Epoch: 18/200, Iteration: 876/10000 --- Training Loss:0.003144\n",
      "Epoch: 18/200, Iteration: 878/10000 --- Training Loss:0.008494\n",
      "Epoch: 18/200, Iteration: 880/10000 --- Training Loss:0.008489\n",
      "Epoch: 18/200, Iteration: 882/10000 --- Training Loss:0.006773\n",
      "Epoch: 18/200, Iteration: 884/10000 --- Training Loss:0.004447\n",
      "Epoch: 18/200, Iteration: 886/10000 --- Training Loss:0.003719\n",
      "Epoch: 18/200, Iteration: 888/10000 --- Training Loss:0.004769\n",
      "Epoch: 18/200, Iteration: 890/10000 --- Training Loss:0.003447\n",
      "Epoch: 18/200, Iteration: 892/10000 --- Training Loss:0.003214\n",
      "Epoch: 18/200, Iteration: 894/10000 --- Training Loss:0.005799\n",
      "Epoch: 18/200, Iteration: 896/10000 --- Training Loss:0.007103\n",
      "Epoch: 18/200, Iteration: 898/10000 --- Training Loss:0.006018\n",
      "Epoch: 18/200, Iteration: 900/10000 --- Training Loss:0.003105\n",
      "Epoch: 18 finished ! Train Loss: 0.00738, Test Loss: 0.05798\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 19/200, Iteration: 902/10000 --- Training Loss:0.006316\n",
      "Epoch: 19/200, Iteration: 904/10000 --- Training Loss:0.008981\n",
      "Epoch: 19/200, Iteration: 906/10000 --- Training Loss:0.003461\n",
      "Epoch: 19/200, Iteration: 908/10000 --- Training Loss:0.006651\n",
      "Epoch: 19/200, Iteration: 910/10000 --- Training Loss:0.008392\n",
      "Epoch: 19/200, Iteration: 912/10000 --- Training Loss:0.022356\n",
      "Epoch: 19/200, Iteration: 914/10000 --- Training Loss:0.005975\n",
      "Epoch: 19/200, Iteration: 916/10000 --- Training Loss:0.008773\n",
      "Epoch: 19/200, Iteration: 918/10000 --- Training Loss:0.009724\n",
      "Epoch: 19/200, Iteration: 920/10000 --- Training Loss:0.008896\n",
      "Epoch: 19/200, Iteration: 922/10000 --- Training Loss:0.004452\n",
      "Epoch: 19/200, Iteration: 924/10000 --- Training Loss:0.010258\n",
      "Epoch: 19/200, Iteration: 926/10000 --- Training Loss:0.011991\n",
      "Epoch: 19/200, Iteration: 928/10000 --- Training Loss:0.010362\n",
      "Epoch: 19/200, Iteration: 930/10000 --- Training Loss:0.005224\n",
      "Epoch: 19/200, Iteration: 932/10000 --- Training Loss:0.007145\n",
      "Epoch: 19/200, Iteration: 934/10000 --- Training Loss:0.011855\n",
      "Epoch: 19/200, Iteration: 936/10000 --- Training Loss:0.013299\n",
      "Epoch: 19/200, Iteration: 938/10000 --- Training Loss:0.010671\n",
      "Epoch: 19/200, Iteration: 940/10000 --- Training Loss:0.006313\n",
      "Epoch: 19/200, Iteration: 942/10000 --- Training Loss:0.006244\n",
      "Epoch: 19/200, Iteration: 944/10000 --- Training Loss:0.008184\n",
      "Epoch: 19/200, Iteration: 946/10000 --- Training Loss:0.004901\n",
      "Epoch: 19/200, Iteration: 948/10000 --- Training Loss:0.002932\n",
      "Epoch: 19/200, Iteration: 950/10000 --- Training Loss:0.007334\n",
      "Epoch: 19 finished ! Train Loss: 0.00791, Test Loss: 0.01622\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 20/200, Iteration: 952/10000 --- Training Loss:0.002392\n",
      "Epoch: 20/200, Iteration: 954/10000 --- Training Loss:0.003634\n",
      "Epoch: 20/200, Iteration: 956/10000 --- Training Loss:0.007704\n",
      "Epoch: 20/200, Iteration: 958/10000 --- Training Loss:0.004904\n",
      "Epoch: 20/200, Iteration: 960/10000 --- Training Loss:0.004539\n",
      "Epoch: 20/200, Iteration: 962/10000 --- Training Loss:0.004778\n",
      "Epoch: 20/200, Iteration: 964/10000 --- Training Loss:0.006826\n",
      "Epoch: 20/200, Iteration: 966/10000 --- Training Loss:0.003584\n",
      "Epoch: 20/200, Iteration: 968/10000 --- Training Loss:0.005921\n",
      "Epoch: 20/200, Iteration: 970/10000 --- Training Loss:0.004291\n",
      "Epoch: 20/200, Iteration: 972/10000 --- Training Loss:0.006286\n",
      "Epoch: 20/200, Iteration: 974/10000 --- Training Loss:0.005021\n",
      "Epoch: 20/200, Iteration: 976/10000 --- Training Loss:0.003924\n",
      "Epoch: 20/200, Iteration: 978/10000 --- Training Loss:0.005641\n",
      "Epoch: 20/200, Iteration: 980/10000 --- Training Loss:0.002921\n",
      "Epoch: 20/200, Iteration: 982/10000 --- Training Loss:0.005383\n",
      "Epoch: 20/200, Iteration: 984/10000 --- Training Loss:0.006321\n",
      "Epoch: 20/200, Iteration: 986/10000 --- Training Loss:0.005701\n",
      "Epoch: 20/200, Iteration: 988/10000 --- Training Loss:0.003848\n",
      "Epoch: 20/200, Iteration: 990/10000 --- Training Loss:0.002671\n",
      "Epoch: 20/200, Iteration: 992/10000 --- Training Loss:0.005005\n",
      "Epoch: 20/200, Iteration: 994/10000 --- Training Loss:0.004714\n",
      "Epoch: 20/200, Iteration: 996/10000 --- Training Loss:0.004533\n",
      "Epoch: 20/200, Iteration: 998/10000 --- Training Loss:0.006189\n",
      "Epoch: 20/200, Iteration: 1000/10000 --- Training Loss:0.003017\n",
      "Epoch: 20 finished ! Train Loss: 0.00612, Test Loss: 0.00497\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 21/200, Iteration: 1002/10000 --- Training Loss:0.004059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/200, Iteration: 1004/10000 --- Training Loss:0.003695\n",
      "Epoch: 21/200, Iteration: 1006/10000 --- Training Loss:0.003369\n",
      "Epoch: 21/200, Iteration: 1008/10000 --- Training Loss:0.001887\n",
      "Epoch: 21/200, Iteration: 1010/10000 --- Training Loss:0.004951\n",
      "Epoch: 21/200, Iteration: 1012/10000 --- Training Loss:0.003908\n",
      "Epoch: 21/200, Iteration: 1014/10000 --- Training Loss:0.004137\n",
      "Epoch: 21/200, Iteration: 1016/10000 --- Training Loss:0.004028\n",
      "Epoch: 21/200, Iteration: 1018/10000 --- Training Loss:0.003674\n",
      "Epoch: 21/200, Iteration: 1020/10000 --- Training Loss:0.003974\n",
      "Epoch: 21/200, Iteration: 1022/10000 --- Training Loss:0.003442\n",
      "Epoch: 21/200, Iteration: 1024/10000 --- Training Loss:0.003288\n",
      "Epoch: 21/200, Iteration: 1026/10000 --- Training Loss:0.004021\n",
      "Epoch: 21/200, Iteration: 1028/10000 --- Training Loss:0.010494\n",
      "Epoch: 21/200, Iteration: 1030/10000 --- Training Loss:0.005784\n",
      "Epoch: 21/200, Iteration: 1032/10000 --- Training Loss:0.004505\n",
      "Epoch: 21/200, Iteration: 1034/10000 --- Training Loss:0.005095\n",
      "Epoch: 21/200, Iteration: 1036/10000 --- Training Loss:0.004781\n",
      "Epoch: 21/200, Iteration: 1038/10000 --- Training Loss:0.005109\n",
      "Epoch: 21/200, Iteration: 1040/10000 --- Training Loss:0.003058\n",
      "Epoch: 21/200, Iteration: 1042/10000 --- Training Loss:0.002813\n",
      "Epoch: 21/200, Iteration: 1044/10000 --- Training Loss:0.002840\n",
      "Epoch: 21/200, Iteration: 1046/10000 --- Training Loss:0.004054\n",
      "Epoch: 21/200, Iteration: 1048/10000 --- Training Loss:0.002947\n",
      "Epoch: 21/200, Iteration: 1050/10000 --- Training Loss:0.001899\n",
      "Epoch: 21 finished ! Train Loss: 0.00387, Test Loss: 0.00602\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 22/200, Iteration: 1052/10000 --- Training Loss:0.001835\n",
      "Epoch: 22/200, Iteration: 1054/10000 --- Training Loss:0.001909\n",
      "Epoch: 22/200, Iteration: 1056/10000 --- Training Loss:0.002227\n",
      "Epoch: 22/200, Iteration: 1058/10000 --- Training Loss:0.002499\n",
      "Epoch: 22/200, Iteration: 1060/10000 --- Training Loss:0.006298\n",
      "Epoch: 22/200, Iteration: 1062/10000 --- Training Loss:0.005481\n",
      "Epoch: 22/200, Iteration: 1064/10000 --- Training Loss:0.002118\n",
      "Epoch: 22/200, Iteration: 1066/10000 --- Training Loss:0.002778\n",
      "Epoch: 22/200, Iteration: 1068/10000 --- Training Loss:0.003146\n",
      "Epoch: 22/200, Iteration: 1070/10000 --- Training Loss:0.004346\n",
      "Epoch: 22/200, Iteration: 1072/10000 --- Training Loss:0.008616\n",
      "Epoch: 22/200, Iteration: 1074/10000 --- Training Loss:0.007678\n",
      "Epoch: 22/200, Iteration: 1076/10000 --- Training Loss:0.004822\n",
      "Epoch: 22/200, Iteration: 1078/10000 --- Training Loss:0.002485\n",
      "Epoch: 22/200, Iteration: 1080/10000 --- Training Loss:0.002577\n",
      "Epoch: 22/200, Iteration: 1082/10000 --- Training Loss:0.001832\n",
      "Epoch: 22/200, Iteration: 1084/10000 --- Training Loss:0.002694\n",
      "Epoch: 22/200, Iteration: 1086/10000 --- Training Loss:0.004657\n",
      "Epoch: 22/200, Iteration: 1088/10000 --- Training Loss:0.003140\n",
      "Epoch: 22/200, Iteration: 1090/10000 --- Training Loss:0.002264\n",
      "Epoch: 22/200, Iteration: 1092/10000 --- Training Loss:0.001946\n",
      "Epoch: 22/200, Iteration: 1094/10000 --- Training Loss:0.002825\n",
      "Epoch: 22/200, Iteration: 1096/10000 --- Training Loss:0.002147\n",
      "Epoch: 22/200, Iteration: 1098/10000 --- Training Loss:0.004462\n",
      "Epoch: 22/200, Iteration: 1100/10000 --- Training Loss:0.005491\n",
      "Epoch: 22 finished ! Train Loss: 0.00364, Test Loss: 0.00469\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 23/200, Iteration: 1102/10000 --- Training Loss:0.006262\n",
      "Epoch: 23/200, Iteration: 1104/10000 --- Training Loss:0.005285\n",
      "Epoch: 23/200, Iteration: 1106/10000 --- Training Loss:0.004870\n",
      "Epoch: 23/200, Iteration: 1108/10000 --- Training Loss:0.005964\n",
      "Epoch: 23/200, Iteration: 1110/10000 --- Training Loss:0.006471\n",
      "Epoch: 23/200, Iteration: 1112/10000 --- Training Loss:0.003515\n",
      "Epoch: 23/200, Iteration: 1114/10000 --- Training Loss:0.003830\n",
      "Epoch: 23/200, Iteration: 1116/10000 --- Training Loss:0.004605\n",
      "Epoch: 23/200, Iteration: 1118/10000 --- Training Loss:0.004283\n",
      "Epoch: 23/200, Iteration: 1120/10000 --- Training Loss:0.002176\n",
      "Epoch: 23/200, Iteration: 1122/10000 --- Training Loss:0.001944\n",
      "Epoch: 23/200, Iteration: 1124/10000 --- Training Loss:0.004425\n",
      "Epoch: 23/200, Iteration: 1126/10000 --- Training Loss:0.004382\n",
      "Epoch: 23/200, Iteration: 1128/10000 --- Training Loss:0.004451\n",
      "Epoch: 23/200, Iteration: 1130/10000 --- Training Loss:0.004761\n",
      "Epoch: 23/200, Iteration: 1132/10000 --- Training Loss:0.004423\n",
      "Epoch: 23/200, Iteration: 1134/10000 --- Training Loss:0.003807\n",
      "Epoch: 23/200, Iteration: 1136/10000 --- Training Loss:0.002376\n",
      "Epoch: 23/200, Iteration: 1138/10000 --- Training Loss:0.001353\n",
      "Epoch: 23/200, Iteration: 1140/10000 --- Training Loss:0.003371\n",
      "Epoch: 23/200, Iteration: 1142/10000 --- Training Loss:0.002806\n",
      "Epoch: 23/200, Iteration: 1144/10000 --- Training Loss:0.003261\n",
      "Epoch: 23/200, Iteration: 1146/10000 --- Training Loss:0.002922\n",
      "Epoch: 23/200, Iteration: 1148/10000 --- Training Loss:0.003784\n",
      "Epoch: 23/200, Iteration: 1150/10000 --- Training Loss:0.002693\n",
      "Epoch: 23 finished ! Train Loss: 0.00363, Test Loss: 0.00370\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 11 percent completed\n",
      "Epoch: 24/200, Iteration: 1152/10000 --- Training Loss:0.001991\n",
      "Epoch: 24/200, Iteration: 1154/10000 --- Training Loss:0.002907\n",
      "Epoch: 24/200, Iteration: 1156/10000 --- Training Loss:0.002104\n",
      "Epoch: 24/200, Iteration: 1158/10000 --- Training Loss:0.002345\n",
      "Epoch: 24/200, Iteration: 1160/10000 --- Training Loss:0.002595\n",
      "Epoch: 24/200, Iteration: 1162/10000 --- Training Loss:0.002011\n",
      "Epoch: 24/200, Iteration: 1164/10000 --- Training Loss:0.001969\n",
      "Epoch: 24/200, Iteration: 1166/10000 --- Training Loss:0.002072\n",
      "Epoch: 24/200, Iteration: 1168/10000 --- Training Loss:0.002498\n",
      "Epoch: 24/200, Iteration: 1170/10000 --- Training Loss:0.002447\n",
      "Epoch: 24/200, Iteration: 1172/10000 --- Training Loss:0.001690\n",
      "Epoch: 24/200, Iteration: 1174/10000 --- Training Loss:0.002114\n",
      "Epoch: 24/200, Iteration: 1176/10000 --- Training Loss:0.003366\n",
      "Epoch: 24/200, Iteration: 1178/10000 --- Training Loss:0.001877\n",
      "Epoch: 24/200, Iteration: 1180/10000 --- Training Loss:0.001886\n",
      "Epoch: 24/200, Iteration: 1182/10000 --- Training Loss:0.003488\n",
      "Epoch: 24/200, Iteration: 1184/10000 --- Training Loss:0.002555\n",
      "Epoch: 24/200, Iteration: 1186/10000 --- Training Loss:0.004834\n",
      "Epoch: 24/200, Iteration: 1188/10000 --- Training Loss:0.006564\n",
      "Epoch: 24/200, Iteration: 1190/10000 --- Training Loss:0.005526\n",
      "Epoch: 24/200, Iteration: 1192/10000 --- Training Loss:0.004424\n",
      "Epoch: 24/200, Iteration: 1194/10000 --- Training Loss:0.002695\n",
      "Epoch: 24/200, Iteration: 1196/10000 --- Training Loss:0.004261\n",
      "Epoch: 24/200, Iteration: 1198/10000 --- Training Loss:0.003366\n",
      "Epoch: 24/200, Iteration: 1200/10000 --- Training Loss:0.002345\n",
      "Epoch: 24 finished ! Train Loss: 0.00291, Test Loss: 0.00500\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 25/200, Iteration: 1202/10000 --- Training Loss:0.004030\n",
      "Epoch: 25/200, Iteration: 1204/10000 --- Training Loss:0.002946\n",
      "Epoch: 25/200, Iteration: 1206/10000 --- Training Loss:0.003023\n",
      "Epoch: 25/200, Iteration: 1208/10000 --- Training Loss:0.002680\n",
      "Epoch: 25/200, Iteration: 1210/10000 --- Training Loss:0.002703\n",
      "Epoch: 25/200, Iteration: 1212/10000 --- Training Loss:0.001484\n",
      "Epoch: 25/200, Iteration: 1214/10000 --- Training Loss:0.003277\n",
      "Epoch: 25/200, Iteration: 1216/10000 --- Training Loss:0.001718\n",
      "Epoch: 25/200, Iteration: 1218/10000 --- Training Loss:0.001491\n",
      "Epoch: 25/200, Iteration: 1220/10000 --- Training Loss:0.002507\n",
      "Epoch: 25/200, Iteration: 1222/10000 --- Training Loss:0.002864\n",
      "Epoch: 25/200, Iteration: 1224/10000 --- Training Loss:0.003401\n",
      "Epoch: 25/200, Iteration: 1226/10000 --- Training Loss:0.004039\n",
      "Epoch: 25/200, Iteration: 1228/10000 --- Training Loss:0.002915\n",
      "Epoch: 25/200, Iteration: 1230/10000 --- Training Loss:0.002807\n",
      "Epoch: 25/200, Iteration: 1232/10000 --- Training Loss:0.003279\n",
      "Epoch: 25/200, Iteration: 1234/10000 --- Training Loss:0.003424\n",
      "Epoch: 25/200, Iteration: 1236/10000 --- Training Loss:0.002842\n",
      "Epoch: 25/200, Iteration: 1238/10000 --- Training Loss:0.002827\n",
      "Epoch: 25/200, Iteration: 1240/10000 --- Training Loss:0.002206\n",
      "Epoch: 25/200, Iteration: 1242/10000 --- Training Loss:0.003289\n",
      "Epoch: 25/200, Iteration: 1244/10000 --- Training Loss:0.003193\n",
      "Epoch: 25/200, Iteration: 1246/10000 --- Training Loss:0.002743\n",
      "Epoch: 25/200, Iteration: 1248/10000 --- Training Loss:0.001612\n",
      "Epoch: 25/200, Iteration: 1250/10000 --- Training Loss:0.002073\n",
      "Epoch: 25 finished ! Train Loss: 0.00284, Test Loss: 0.00416\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 26/200, Iteration: 1252/10000 --- Training Loss:0.002727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/200, Iteration: 1254/10000 --- Training Loss:0.002340\n",
      "Epoch: 26/200, Iteration: 1256/10000 --- Training Loss:0.001961\n",
      "Epoch: 26/200, Iteration: 1258/10000 --- Training Loss:0.002067\n",
      "Epoch: 26/200, Iteration: 1260/10000 --- Training Loss:0.001934\n",
      "Epoch: 26/200, Iteration: 1262/10000 --- Training Loss:0.003647\n",
      "Epoch: 26/200, Iteration: 1264/10000 --- Training Loss:0.004483\n",
      "Epoch: 26/200, Iteration: 1266/10000 --- Training Loss:0.004030\n",
      "Epoch: 26/200, Iteration: 1268/10000 --- Training Loss:0.002363\n",
      "Epoch: 26/200, Iteration: 1270/10000 --- Training Loss:0.001980\n",
      "Epoch: 26/200, Iteration: 1272/10000 --- Training Loss:0.007105\n",
      "Epoch: 26/200, Iteration: 1274/10000 --- Training Loss:0.015705\n",
      "Epoch: 26/200, Iteration: 1276/10000 --- Training Loss:0.009497\n",
      "Epoch: 26/200, Iteration: 1278/10000 --- Training Loss:0.007970\n",
      "Epoch: 26/200, Iteration: 1280/10000 --- Training Loss:0.009375\n",
      "Epoch: 26/200, Iteration: 1282/10000 --- Training Loss:0.012665\n",
      "Epoch: 26/200, Iteration: 1284/10000 --- Training Loss:0.005867\n",
      "Epoch: 26/200, Iteration: 1286/10000 --- Training Loss:0.032366\n",
      "Epoch: 26/200, Iteration: 1288/10000 --- Training Loss:0.015800\n",
      "Epoch: 26/200, Iteration: 1290/10000 --- Training Loss:0.013678\n",
      "Epoch: 26/200, Iteration: 1292/10000 --- Training Loss:0.006053\n",
      "Epoch: 26/200, Iteration: 1294/10000 --- Training Loss:0.002866\n",
      "Epoch: 26/200, Iteration: 1296/10000 --- Training Loss:0.002763\n",
      "Epoch: 26/200, Iteration: 1298/10000 --- Training Loss:0.004047\n",
      "Epoch: 26/200, Iteration: 1300/10000 --- Training Loss:0.012502\n",
      "Epoch: 26 finished ! Train Loss: 0.00774, Test Loss: 0.00401\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 27/200, Iteration: 1302/10000 --- Training Loss:0.007497\n",
      "Epoch: 27/200, Iteration: 1304/10000 --- Training Loss:0.009911\n",
      "Epoch: 27/200, Iteration: 1306/10000 --- Training Loss:0.004485\n",
      "Epoch: 27/200, Iteration: 1308/10000 --- Training Loss:0.006235\n",
      "Epoch: 27/200, Iteration: 1310/10000 --- Training Loss:0.027679\n",
      "Epoch: 27/200, Iteration: 1312/10000 --- Training Loss:0.012490\n",
      "Epoch: 27/200, Iteration: 1314/10000 --- Training Loss:0.005868\n",
      "Epoch: 27/200, Iteration: 1316/10000 --- Training Loss:0.010442\n",
      "Epoch: 27/200, Iteration: 1318/10000 --- Training Loss:0.004462\n",
      "Epoch: 27/200, Iteration: 1320/10000 --- Training Loss:0.005072\n",
      "Epoch: 27/200, Iteration: 1322/10000 --- Training Loss:0.003184\n",
      "Epoch: 27/200, Iteration: 1324/10000 --- Training Loss:0.010822\n",
      "Epoch: 27/200, Iteration: 1326/10000 --- Training Loss:0.004451\n",
      "Epoch: 27/200, Iteration: 1328/10000 --- Training Loss:0.004084\n",
      "Epoch: 27/200, Iteration: 1330/10000 --- Training Loss:0.004622\n",
      "Epoch: 27/200, Iteration: 1332/10000 --- Training Loss:0.009640\n",
      "Epoch: 27/200, Iteration: 1334/10000 --- Training Loss:0.008635\n",
      "Epoch: 27/200, Iteration: 1336/10000 --- Training Loss:0.003172\n",
      "Epoch: 27/200, Iteration: 1338/10000 --- Training Loss:0.003314\n",
      "Epoch: 27/200, Iteration: 1340/10000 --- Training Loss:0.002367\n",
      "Epoch: 27/200, Iteration: 1342/10000 --- Training Loss:0.002461\n",
      "Epoch: 27/200, Iteration: 1344/10000 --- Training Loss:0.002857\n",
      "Epoch: 27/200, Iteration: 1346/10000 --- Training Loss:0.004749\n",
      "Epoch: 27/200, Iteration: 1348/10000 --- Training Loss:0.003690\n",
      "Epoch: 27/200, Iteration: 1350/10000 --- Training Loss:0.003931\n",
      "Epoch: 27 finished ! Train Loss: 0.00668, Test Loss: 0.00507\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 28/200, Iteration: 1352/10000 --- Training Loss:0.003600\n",
      "Epoch: 28/200, Iteration: 1354/10000 --- Training Loss:0.005163\n",
      "Epoch: 28/200, Iteration: 1356/10000 --- Training Loss:0.003232\n",
      "Epoch: 28/200, Iteration: 1358/10000 --- Training Loss:0.001610\n",
      "Epoch: 28/200, Iteration: 1360/10000 --- Training Loss:0.004949\n",
      "Epoch: 28/200, Iteration: 1362/10000 --- Training Loss:0.005158\n",
      "Epoch: 28/200, Iteration: 1364/10000 --- Training Loss:0.005394\n",
      "Epoch: 28/200, Iteration: 1366/10000 --- Training Loss:0.006191\n",
      "Epoch: 28/200, Iteration: 1368/10000 --- Training Loss:0.003346\n",
      "Epoch: 28/200, Iteration: 1370/10000 --- Training Loss:0.002948\n",
      "Epoch: 28/200, Iteration: 1372/10000 --- Training Loss:0.002734\n",
      "Epoch: 28/200, Iteration: 1374/10000 --- Training Loss:0.003117\n",
      "Epoch: 28/200, Iteration: 1376/10000 --- Training Loss:0.001722\n",
      "Epoch: 28/200, Iteration: 1378/10000 --- Training Loss:0.006390\n",
      "Epoch: 28/200, Iteration: 1380/10000 --- Training Loss:0.013054\n",
      "Epoch: 28/200, Iteration: 1382/10000 --- Training Loss:0.025122\n",
      "Epoch: 28/200, Iteration: 1384/10000 --- Training Loss:0.009371\n",
      "Epoch: 28/200, Iteration: 1386/10000 --- Training Loss:0.003626\n",
      "Epoch: 28/200, Iteration: 1388/10000 --- Training Loss:0.014730\n",
      "Epoch: 28/200, Iteration: 1390/10000 --- Training Loss:0.011861\n",
      "Epoch: 28/200, Iteration: 1392/10000 --- Training Loss:0.011191\n",
      "Epoch: 28/200, Iteration: 1394/10000 --- Training Loss:0.009443\n",
      "Epoch: 28/200, Iteration: 1396/10000 --- Training Loss:0.011008\n",
      "Epoch: 28/200, Iteration: 1398/10000 --- Training Loss:0.016641\n",
      "Epoch: 28/200, Iteration: 1400/10000 --- Training Loss:0.017155\n",
      "Epoch: 28 finished ! Train Loss: 0.00794, Test Loss: 0.03001\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 29/200, Iteration: 1402/10000 --- Training Loss:0.012205\n",
      "Epoch: 29/200, Iteration: 1404/10000 --- Training Loss:0.021414\n",
      "Epoch: 29/200, Iteration: 1406/10000 --- Training Loss:0.004582\n",
      "Epoch: 29/200, Iteration: 1408/10000 --- Training Loss:0.008586\n",
      "Epoch: 29/200, Iteration: 1410/10000 --- Training Loss:0.005892\n",
      "Epoch: 29/200, Iteration: 1412/10000 --- Training Loss:0.007480\n",
      "Epoch: 29/200, Iteration: 1414/10000 --- Training Loss:0.032273\n",
      "Epoch: 29/200, Iteration: 1416/10000 --- Training Loss:0.144223\n",
      "Epoch: 29/200, Iteration: 1418/10000 --- Training Loss:0.115176\n",
      "Epoch: 29/200, Iteration: 1420/10000 --- Training Loss:0.129578\n",
      "Epoch: 29/200, Iteration: 1422/10000 --- Training Loss:0.112818\n",
      "Epoch: 29/200, Iteration: 1424/10000 --- Training Loss:0.065231\n",
      "Epoch: 29/200, Iteration: 1426/10000 --- Training Loss:0.076573\n",
      "Epoch: 29/200, Iteration: 1428/10000 --- Training Loss:0.081468\n",
      "Epoch: 29/200, Iteration: 1430/10000 --- Training Loss:0.023738\n",
      "Epoch: 29/200, Iteration: 1432/10000 --- Training Loss:0.024040\n",
      "Epoch: 29/200, Iteration: 1434/10000 --- Training Loss:0.098830\n",
      "Epoch: 29/200, Iteration: 1436/10000 --- Training Loss:0.286747\n",
      "Epoch: 29/200, Iteration: 1438/10000 --- Training Loss:0.230473\n",
      "Epoch: 29/200, Iteration: 1440/10000 --- Training Loss:0.168971\n",
      "Epoch: 29/200, Iteration: 1442/10000 --- Training Loss:0.033651\n",
      "Epoch: 29/200, Iteration: 1444/10000 --- Training Loss:0.042554\n",
      "Epoch: 29/200, Iteration: 1446/10000 --- Training Loss:0.040285\n",
      "Epoch: 29/200, Iteration: 1448/10000 --- Training Loss:0.015170\n",
      "Epoch: 29/200, Iteration: 1450/10000 --- Training Loss:0.039885\n",
      "Epoch: 29 finished ! Train Loss: 0.05616, Test Loss: 0.03067\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 30/200, Iteration: 1452/10000 --- Training Loss:0.044381\n",
      "Epoch: 30/200, Iteration: 1454/10000 --- Training Loss:0.009834\n",
      "Epoch: 30/200, Iteration: 1456/10000 --- Training Loss:0.005377\n",
      "Epoch: 30/200, Iteration: 1458/10000 --- Training Loss:0.007681\n",
      "Epoch: 30/200, Iteration: 1460/10000 --- Training Loss:0.007079\n",
      "Epoch: 30/200, Iteration: 1462/10000 --- Training Loss:0.012968\n",
      "Epoch: 30/200, Iteration: 1464/10000 --- Training Loss:0.005686\n",
      "Epoch: 30/200, Iteration: 1466/10000 --- Training Loss:0.009555\n",
      "Epoch: 30/200, Iteration: 1468/10000 --- Training Loss:0.007311\n",
      "Epoch: 30/200, Iteration: 1470/10000 --- Training Loss:0.006299\n",
      "Epoch: 30/200, Iteration: 1472/10000 --- Training Loss:0.016573\n",
      "Epoch: 30/200, Iteration: 1474/10000 --- Training Loss:0.030830\n",
      "Epoch: 30/200, Iteration: 1476/10000 --- Training Loss:0.005158\n",
      "Epoch: 30/200, Iteration: 1478/10000 --- Training Loss:0.020169\n",
      "Epoch: 30/200, Iteration: 1480/10000 --- Training Loss:0.019069\n",
      "Epoch: 30/200, Iteration: 1482/10000 --- Training Loss:0.038688\n",
      "Epoch: 30/200, Iteration: 1484/10000 --- Training Loss:0.023522\n",
      "Epoch: 30/200, Iteration: 1486/10000 --- Training Loss:0.048433\n",
      "Epoch: 30/200, Iteration: 1488/10000 --- Training Loss:0.029289\n",
      "Epoch: 30/200, Iteration: 1490/10000 --- Training Loss:0.038767\n",
      "Epoch: 30/200, Iteration: 1492/10000 --- Training Loss:0.077149\n",
      "Epoch: 30/200, Iteration: 1494/10000 --- Training Loss:0.019327\n",
      "Epoch: 30/200, Iteration: 1496/10000 --- Training Loss:0.017551\n",
      "Epoch: 30/200, Iteration: 1498/10000 --- Training Loss:0.015088\n",
      "Epoch: 30/200, Iteration: 1500/10000 --- Training Loss:0.014696\n",
      "Epoch: 30 finished ! Train Loss: 0.02644, Test Loss: 0.02494\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 31/200, Iteration: 1502/10000 --- Training Loss:0.036091\n",
      "Epoch: 31/200, Iteration: 1504/10000 --- Training Loss:0.074770\n",
      "Epoch: 31/200, Iteration: 1506/10000 --- Training Loss:0.051226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/200, Iteration: 1508/10000 --- Training Loss:0.040067\n",
      "Epoch: 31/200, Iteration: 1510/10000 --- Training Loss:0.013909\n",
      "Epoch: 31/200, Iteration: 1512/10000 --- Training Loss:0.035615\n",
      "Epoch: 31/200, Iteration: 1514/10000 --- Training Loss:0.021795\n",
      "Epoch: 31/200, Iteration: 1516/10000 --- Training Loss:0.018317\n",
      "Epoch: 31/200, Iteration: 1518/10000 --- Training Loss:0.010681\n",
      "Epoch: 31/200, Iteration: 1520/10000 --- Training Loss:0.006740\n",
      "Epoch: 31/200, Iteration: 1522/10000 --- Training Loss:0.006317\n",
      "Epoch: 31/200, Iteration: 1524/10000 --- Training Loss:0.012181\n",
      "Epoch: 31/200, Iteration: 1526/10000 --- Training Loss:0.016339\n",
      "Epoch: 31/200, Iteration: 1528/10000 --- Training Loss:0.013363\n",
      "Epoch: 31/200, Iteration: 1530/10000 --- Training Loss:0.008204\n",
      "Epoch: 31/200, Iteration: 1532/10000 --- Training Loss:0.006965\n",
      "Epoch: 31/200, Iteration: 1534/10000 --- Training Loss:0.007300\n",
      "Epoch: 31/200, Iteration: 1536/10000 --- Training Loss:0.007924\n",
      "Epoch: 31/200, Iteration: 1538/10000 --- Training Loss:0.020475\n",
      "Epoch: 31/200, Iteration: 1540/10000 --- Training Loss:0.011771\n",
      "Epoch: 31/200, Iteration: 1542/10000 --- Training Loss:0.009157\n",
      "Epoch: 31/200, Iteration: 1544/10000 --- Training Loss:0.006527\n",
      "Epoch: 31/200, Iteration: 1546/10000 --- Training Loss:0.006209\n",
      "Epoch: 31/200, Iteration: 1548/10000 --- Training Loss:0.007547\n",
      "Epoch: 31/200, Iteration: 1550/10000 --- Training Loss:0.006565\n",
      "Epoch: 31 finished ! Train Loss: 0.01735, Test Loss: 0.02378\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 32/200, Iteration: 1552/10000 --- Training Loss:0.013264\n",
      "Epoch: 32/200, Iteration: 1554/10000 --- Training Loss:0.008493\n",
      "Epoch: 32/200, Iteration: 1556/10000 --- Training Loss:0.005707\n",
      "Epoch: 32/200, Iteration: 1558/10000 --- Training Loss:0.006268\n",
      "Epoch: 32/200, Iteration: 1560/10000 --- Training Loss:0.006563\n",
      "Epoch: 32/200, Iteration: 1562/10000 --- Training Loss:0.008484\n",
      "Epoch: 32/200, Iteration: 1564/10000 --- Training Loss:0.017570\n",
      "Epoch: 32/200, Iteration: 1566/10000 --- Training Loss:0.015466\n",
      "Epoch: 32/200, Iteration: 1568/10000 --- Training Loss:0.008892\n",
      "Epoch: 32/200, Iteration: 1570/10000 --- Training Loss:0.006978\n",
      "Epoch: 32/200, Iteration: 1572/10000 --- Training Loss:0.006925\n",
      "Epoch: 32/200, Iteration: 1574/10000 --- Training Loss:0.003493\n",
      "Epoch: 32/200, Iteration: 1576/10000 --- Training Loss:0.002878\n",
      "Epoch: 32/200, Iteration: 1578/10000 --- Training Loss:0.002835\n",
      "Epoch: 32/200, Iteration: 1580/10000 --- Training Loss:0.003017\n",
      "Epoch: 32/200, Iteration: 1582/10000 --- Training Loss:0.014074\n",
      "Epoch: 32/200, Iteration: 1584/10000 --- Training Loss:0.007026\n",
      "Epoch: 32/200, Iteration: 1586/10000 --- Training Loss:0.004776\n",
      "Epoch: 32/200, Iteration: 1588/10000 --- Training Loss:0.013196\n",
      "Epoch: 32/200, Iteration: 1590/10000 --- Training Loss:0.012820\n",
      "Epoch: 32/200, Iteration: 1592/10000 --- Training Loss:0.022356\n",
      "Epoch: 32/200, Iteration: 1594/10000 --- Training Loss:0.009629\n",
      "Epoch: 32/200, Iteration: 1596/10000 --- Training Loss:0.004834\n",
      "Epoch: 32/200, Iteration: 1598/10000 --- Training Loss:0.002890\n",
      "Epoch: 32/200, Iteration: 1600/10000 --- Training Loss:0.004060\n",
      "Epoch: 32 finished ! Train Loss: 0.00791, Test Loss: 0.00577\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 33/200, Iteration: 1602/10000 --- Training Loss:0.004265\n",
      "Epoch: 33/200, Iteration: 1604/10000 --- Training Loss:0.002807\n",
      "Epoch: 33/200, Iteration: 1606/10000 --- Training Loss:0.003873\n",
      "Epoch: 33/200, Iteration: 1608/10000 --- Training Loss:0.004983\n",
      "Epoch: 33/200, Iteration: 1610/10000 --- Training Loss:0.004315\n",
      "Epoch: 33/200, Iteration: 1612/10000 --- Training Loss:0.003186\n",
      "Epoch: 33/200, Iteration: 1614/10000 --- Training Loss:0.002561\n",
      "Epoch: 33/200, Iteration: 1616/10000 --- Training Loss:0.002676\n",
      "Epoch: 33/200, Iteration: 1618/10000 --- Training Loss:0.002284\n",
      "Epoch: 33/200, Iteration: 1620/10000 --- Training Loss:0.002606\n",
      "Epoch: 33/200, Iteration: 1622/10000 --- Training Loss:0.003006\n",
      "Epoch: 33/200, Iteration: 1624/10000 --- Training Loss:0.002326\n",
      "Epoch: 33/200, Iteration: 1626/10000 --- Training Loss:0.003120\n",
      "Epoch: 33/200, Iteration: 1628/10000 --- Training Loss:0.004982\n",
      "Epoch: 33/200, Iteration: 1630/10000 --- Training Loss:0.002663\n",
      "Epoch: 33/200, Iteration: 1632/10000 --- Training Loss:0.003045\n",
      "Epoch: 33/200, Iteration: 1634/10000 --- Training Loss:0.001976\n",
      "Epoch: 33/200, Iteration: 1636/10000 --- Training Loss:0.004425\n",
      "Epoch: 33/200, Iteration: 1638/10000 --- Training Loss:0.005308\n",
      "Epoch: 33/200, Iteration: 1640/10000 --- Training Loss:0.005476\n",
      "Epoch: 33/200, Iteration: 1642/10000 --- Training Loss:0.002175\n",
      "Epoch: 33/200, Iteration: 1644/10000 --- Training Loss:0.003010\n",
      "Epoch: 33/200, Iteration: 1646/10000 --- Training Loss:0.002804\n",
      "Epoch: 33/200, Iteration: 1648/10000 --- Training Loss:0.002858\n",
      "Epoch: 33/200, Iteration: 1650/10000 --- Training Loss:0.006249\n",
      "Epoch: 33 finished ! Train Loss: 0.00397, Test Loss: 0.00692\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 34/200, Iteration: 1652/10000 --- Training Loss:0.003094\n",
      "Epoch: 34/200, Iteration: 1654/10000 --- Training Loss:0.002971\n",
      "Epoch: 34/200, Iteration: 1656/10000 --- Training Loss:0.003114\n",
      "Epoch: 34/200, Iteration: 1658/10000 --- Training Loss:0.002380\n",
      "Epoch: 34/200, Iteration: 1660/10000 --- Training Loss:0.007668\n",
      "Epoch: 34/200, Iteration: 1662/10000 --- Training Loss:0.005184\n",
      "Epoch: 34/200, Iteration: 1664/10000 --- Training Loss:0.006772\n",
      "Epoch: 34/200, Iteration: 1666/10000 --- Training Loss:0.006945\n",
      "Epoch: 34/200, Iteration: 1668/10000 --- Training Loss:0.011542\n",
      "Epoch: 34/200, Iteration: 1670/10000 --- Training Loss:0.007503\n",
      "Epoch: 34/200, Iteration: 1672/10000 --- Training Loss:0.007489\n",
      "Epoch: 34/200, Iteration: 1674/10000 --- Training Loss:0.003698\n",
      "Epoch: 34/200, Iteration: 1676/10000 --- Training Loss:0.003071\n",
      "Epoch: 34/200, Iteration: 1678/10000 --- Training Loss:0.004330\n",
      "Epoch: 34/200, Iteration: 1680/10000 --- Training Loss:0.004115\n",
      "Epoch: 34/200, Iteration: 1682/10000 --- Training Loss:0.005340\n",
      "Epoch: 34/200, Iteration: 1684/10000 --- Training Loss:0.002532\n",
      "Epoch: 34/200, Iteration: 1686/10000 --- Training Loss:0.002928\n",
      "Epoch: 34/200, Iteration: 1688/10000 --- Training Loss:0.005884\n",
      "Epoch: 34/200, Iteration: 1690/10000 --- Training Loss:0.008720\n",
      "Epoch: 34/200, Iteration: 1692/10000 --- Training Loss:0.009011\n",
      "Epoch: 34/200, Iteration: 1694/10000 --- Training Loss:0.007488\n",
      "Epoch: 34/200, Iteration: 1696/10000 --- Training Loss:0.006456\n",
      "Epoch: 34/200, Iteration: 1698/10000 --- Training Loss:0.008887\n",
      "Epoch: 34/200, Iteration: 1700/10000 --- Training Loss:0.004256\n",
      "Epoch: 34 finished ! Train Loss: 0.00591, Test Loss: 0.00629\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 35/200, Iteration: 1702/10000 --- Training Loss:0.003633\n",
      "Epoch: 35/200, Iteration: 1704/10000 --- Training Loss:0.003668\n",
      "Epoch: 35/200, Iteration: 1706/10000 --- Training Loss:0.002110\n",
      "Epoch: 35/200, Iteration: 1708/10000 --- Training Loss:0.003054\n",
      "Epoch: 35/200, Iteration: 1710/10000 --- Training Loss:0.003842\n",
      "Epoch: 35/200, Iteration: 1712/10000 --- Training Loss:0.004684\n",
      "Epoch: 35/200, Iteration: 1714/10000 --- Training Loss:0.001713\n",
      "Epoch: 35/200, Iteration: 1716/10000 --- Training Loss:0.003338\n",
      "Epoch: 35/200, Iteration: 1718/10000 --- Training Loss:0.002637\n",
      "Epoch: 35/200, Iteration: 1720/10000 --- Training Loss:0.028843\n",
      "Epoch: 35/200, Iteration: 1722/10000 --- Training Loss:0.002519\n",
      "Epoch: 35/200, Iteration: 1724/10000 --- Training Loss:0.008273\n",
      "Epoch: 35/200, Iteration: 1726/10000 --- Training Loss:0.006291\n",
      "Epoch: 35/200, Iteration: 1728/10000 --- Training Loss:0.003815\n",
      "Epoch: 35/200, Iteration: 1730/10000 --- Training Loss:0.002418\n",
      "Epoch: 35/200, Iteration: 1732/10000 --- Training Loss:0.006716\n",
      "Epoch: 35/200, Iteration: 1734/10000 --- Training Loss:0.010450\n",
      "Epoch: 35/200, Iteration: 1736/10000 --- Training Loss:0.004235\n",
      "Epoch: 35/200, Iteration: 1738/10000 --- Training Loss:0.002820\n",
      "Epoch: 35/200, Iteration: 1740/10000 --- Training Loss:0.004468\n",
      "Epoch: 35/200, Iteration: 1742/10000 --- Training Loss:0.002796\n",
      "Epoch: 35/200, Iteration: 1744/10000 --- Training Loss:0.002584\n",
      "Epoch: 35/200, Iteration: 1746/10000 --- Training Loss:0.005261\n",
      "Epoch: 35/200, Iteration: 1748/10000 --- Training Loss:0.002351\n",
      "Epoch: 35/200, Iteration: 1750/10000 --- Training Loss:0.002257\n",
      "Epoch: 35 finished ! Train Loss: 0.00487, Test Loss: 0.00525\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 36/200, Iteration: 1752/10000 --- Training Loss:0.001892\n",
      "Epoch: 36/200, Iteration: 1754/10000 --- Training Loss:0.132378\n",
      "Epoch: 36/200, Iteration: 1756/10000 --- Training Loss:0.113064\n",
      "Epoch: 36/200, Iteration: 1758/10000 --- Training Loss:0.082061\n",
      "Epoch: 36/200, Iteration: 1760/10000 --- Training Loss:0.091493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36/200, Iteration: 1762/10000 --- Training Loss:0.054299\n",
      "Epoch: 36/200, Iteration: 1764/10000 --- Training Loss:0.141315\n",
      "Epoch: 36/200, Iteration: 1766/10000 --- Training Loss:0.148275\n",
      "Epoch: 36/200, Iteration: 1768/10000 --- Training Loss:0.046987\n",
      "Epoch: 36/200, Iteration: 1770/10000 --- Training Loss:0.059564\n",
      "Epoch: 36/200, Iteration: 1772/10000 --- Training Loss:0.073785\n",
      "Epoch: 36/200, Iteration: 1774/10000 --- Training Loss:0.030775\n",
      "Epoch: 36/200, Iteration: 1776/10000 --- Training Loss:0.015055\n",
      "Epoch: 36/200, Iteration: 1778/10000 --- Training Loss:0.015417\n",
      "Epoch: 36/200, Iteration: 1780/10000 --- Training Loss:0.009954\n",
      "Epoch: 36/200, Iteration: 1782/10000 --- Training Loss:0.013596\n",
      "Epoch: 36/200, Iteration: 1784/10000 --- Training Loss:0.013782\n",
      "Epoch: 36/200, Iteration: 1786/10000 --- Training Loss:0.011668\n",
      "Epoch: 36/200, Iteration: 1788/10000 --- Training Loss:0.015107\n",
      "Epoch: 36/200, Iteration: 1790/10000 --- Training Loss:0.005457\n",
      "Epoch: 36/200, Iteration: 1792/10000 --- Training Loss:0.006181\n",
      "Epoch: 36/200, Iteration: 1794/10000 --- Training Loss:0.005917\n",
      "Epoch: 36/200, Iteration: 1796/10000 --- Training Loss:0.004978\n",
      "Epoch: 36/200, Iteration: 1798/10000 --- Training Loss:0.005218\n",
      "Epoch: 36/200, Iteration: 1800/10000 --- Training Loss:0.003654\n",
      "Epoch: 36 finished ! Train Loss: 0.03218, Test Loss: 0.04929\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 37/200, Iteration: 1802/10000 --- Training Loss:0.006567\n",
      "Epoch: 37/200, Iteration: 1804/10000 --- Training Loss:0.003738\n",
      "Epoch: 37/200, Iteration: 1806/10000 --- Training Loss:0.002980\n",
      "Epoch: 37/200, Iteration: 1808/10000 --- Training Loss:0.002431\n",
      "Epoch: 37/200, Iteration: 1810/10000 --- Training Loss:0.005246\n",
      "Epoch: 37/200, Iteration: 1812/10000 --- Training Loss:0.002450\n",
      "Epoch: 37/200, Iteration: 1814/10000 --- Training Loss:0.005365\n",
      "Epoch: 37/200, Iteration: 1816/10000 --- Training Loss:0.007339\n",
      "Epoch: 37/200, Iteration: 1818/10000 --- Training Loss:0.004358\n",
      "Epoch: 37/200, Iteration: 1820/10000 --- Training Loss:0.005358\n",
      "Epoch: 37/200, Iteration: 1822/10000 --- Training Loss:0.004858\n",
      "Epoch: 37/200, Iteration: 1824/10000 --- Training Loss:0.003747\n",
      "Epoch: 37/200, Iteration: 1826/10000 --- Training Loss:0.003036\n",
      "Epoch: 37/200, Iteration: 1828/10000 --- Training Loss:0.001901\n",
      "Epoch: 37/200, Iteration: 1830/10000 --- Training Loss:0.003271\n",
      "Epoch: 37/200, Iteration: 1832/10000 --- Training Loss:0.004722\n",
      "Epoch: 37/200, Iteration: 1834/10000 --- Training Loss:0.006788\n",
      "Epoch: 37/200, Iteration: 1836/10000 --- Training Loss:0.005667\n",
      "Epoch: 37/200, Iteration: 1838/10000 --- Training Loss:0.001890\n",
      "Epoch: 37/200, Iteration: 1840/10000 --- Training Loss:0.001454\n",
      "Epoch: 37/200, Iteration: 1842/10000 --- Training Loss:0.002283\n",
      "Epoch: 37/200, Iteration: 1844/10000 --- Training Loss:0.003275\n",
      "Epoch: 37/200, Iteration: 1846/10000 --- Training Loss:0.003760\n",
      "Epoch: 37/200, Iteration: 1848/10000 --- Training Loss:0.004655\n",
      "Epoch: 37/200, Iteration: 1850/10000 --- Training Loss:0.003922\n",
      "Epoch: 37 finished ! Train Loss: 0.00424, Test Loss: 0.00855\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 38/200, Iteration: 1852/10000 --- Training Loss:0.004374\n",
      "Epoch: 38/200, Iteration: 1854/10000 --- Training Loss:0.002218\n",
      "Epoch: 38/200, Iteration: 1856/10000 --- Training Loss:0.003986\n",
      "Epoch: 38/200, Iteration: 1858/10000 --- Training Loss:0.002832\n",
      "Epoch: 38/200, Iteration: 1860/10000 --- Training Loss:0.002435\n",
      "Epoch: 38/200, Iteration: 1862/10000 --- Training Loss:0.002685\n",
      "Epoch: 38/200, Iteration: 1864/10000 --- Training Loss:0.040326\n",
      "Epoch: 38/200, Iteration: 1866/10000 --- Training Loss:0.026684\n",
      "Epoch: 38/200, Iteration: 1868/10000 --- Training Loss:0.020975\n",
      "Epoch: 38/200, Iteration: 1870/10000 --- Training Loss:0.017845\n",
      "Epoch: 38/200, Iteration: 1872/10000 --- Training Loss:0.011620\n",
      "Epoch: 38/200, Iteration: 1874/10000 --- Training Loss:0.010377\n",
      "Epoch: 38/200, Iteration: 1876/10000 --- Training Loss:0.021429\n",
      "Epoch: 38/200, Iteration: 1878/10000 --- Training Loss:0.018984\n",
      "Epoch: 38/200, Iteration: 1880/10000 --- Training Loss:0.004227\n",
      "Epoch: 38/200, Iteration: 1882/10000 --- Training Loss:0.002157\n",
      "Epoch: 38/200, Iteration: 1884/10000 --- Training Loss:0.003905\n",
      "Epoch: 38/200, Iteration: 1886/10000 --- Training Loss:0.004814\n",
      "Epoch: 38/200, Iteration: 1888/10000 --- Training Loss:0.004374\n",
      "Epoch: 38/200, Iteration: 1890/10000 --- Training Loss:0.016111\n",
      "Epoch: 38/200, Iteration: 1892/10000 --- Training Loss:0.002871\n",
      "Epoch: 38/200, Iteration: 1894/10000 --- Training Loss:0.004659\n",
      "Epoch: 38/200, Iteration: 1896/10000 --- Training Loss:0.004228\n",
      "Epoch: 38/200, Iteration: 1898/10000 --- Training Loss:0.004536\n",
      "Epoch: 38/200, Iteration: 1900/10000 --- Training Loss:0.003324\n",
      "Epoch: 38 finished ! Train Loss: 0.00912, Test Loss: 0.01671\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 39/200, Iteration: 1902/10000 --- Training Loss:0.002351\n",
      "Epoch: 39/200, Iteration: 1904/10000 --- Training Loss:0.002579\n",
      "Epoch: 39/200, Iteration: 1906/10000 --- Training Loss:0.005814\n",
      "Epoch: 39/200, Iteration: 1908/10000 --- Training Loss:0.004965\n",
      "Epoch: 39/200, Iteration: 1910/10000 --- Training Loss:0.003864\n",
      "Epoch: 39/200, Iteration: 1912/10000 --- Training Loss:0.007000\n",
      "Epoch: 39/200, Iteration: 1914/10000 --- Training Loss:0.003587\n",
      "Epoch: 39/200, Iteration: 1916/10000 --- Training Loss:0.005543\n",
      "Epoch: 39/200, Iteration: 1918/10000 --- Training Loss:0.004729\n",
      "Epoch: 39/200, Iteration: 1920/10000 --- Training Loss:0.004849\n",
      "Epoch: 39/200, Iteration: 1922/10000 --- Training Loss:0.012194\n",
      "Epoch: 39/200, Iteration: 1924/10000 --- Training Loss:0.005000\n",
      "Epoch: 39/200, Iteration: 1926/10000 --- Training Loss:0.012922\n",
      "Epoch: 39/200, Iteration: 1928/10000 --- Training Loss:0.017283\n",
      "Epoch: 39/200, Iteration: 1930/10000 --- Training Loss:0.014876\n",
      "Epoch: 39/200, Iteration: 1932/10000 --- Training Loss:0.010984\n",
      "Epoch: 39/200, Iteration: 1934/10000 --- Training Loss:0.018896\n",
      "Epoch: 39/200, Iteration: 1936/10000 --- Training Loss:0.013098\n",
      "Epoch: 39/200, Iteration: 1938/10000 --- Training Loss:0.009501\n",
      "Epoch: 39/200, Iteration: 1940/10000 --- Training Loss:0.006335\n",
      "Epoch: 39/200, Iteration: 1942/10000 --- Training Loss:0.010391\n",
      "Epoch: 39/200, Iteration: 1944/10000 --- Training Loss:0.019290\n",
      "Epoch: 39/200, Iteration: 1946/10000 --- Training Loss:0.007172\n",
      "Epoch: 39/200, Iteration: 1948/10000 --- Training Loss:0.023409\n",
      "Epoch: 39/200, Iteration: 1950/10000 --- Training Loss:0.016822\n",
      "Epoch: 39 finished ! Train Loss: 0.01021, Test Loss: 0.03637\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 40/200, Iteration: 1952/10000 --- Training Loss:0.010885\n",
      "Epoch: 40/200, Iteration: 1954/10000 --- Training Loss:0.011862\n",
      "Epoch: 40/200, Iteration: 1956/10000 --- Training Loss:0.013381\n",
      "Epoch: 40/200, Iteration: 1958/10000 --- Training Loss:0.005918\n",
      "Epoch: 40/200, Iteration: 1960/10000 --- Training Loss:0.007782\n",
      "Epoch: 40/200, Iteration: 1962/10000 --- Training Loss:0.004430\n",
      "Epoch: 40/200, Iteration: 1964/10000 --- Training Loss:0.004596\n",
      "Epoch: 40/200, Iteration: 1966/10000 --- Training Loss:0.002136\n",
      "Epoch: 40/200, Iteration: 1968/10000 --- Training Loss:0.006946\n",
      "Epoch: 40/200, Iteration: 1970/10000 --- Training Loss:0.002540\n",
      "Epoch: 40/200, Iteration: 1972/10000 --- Training Loss:0.005599\n",
      "Epoch: 40/200, Iteration: 1974/10000 --- Training Loss:0.003787\n",
      "Epoch: 40/200, Iteration: 1976/10000 --- Training Loss:0.006216\n",
      "Epoch: 40/200, Iteration: 1978/10000 --- Training Loss:0.003717\n",
      "Epoch: 40/200, Iteration: 1980/10000 --- Training Loss:0.004370\n",
      "Epoch: 40/200, Iteration: 1982/10000 --- Training Loss:0.003390\n",
      "Epoch: 40/200, Iteration: 1984/10000 --- Training Loss:0.004779\n",
      "Epoch: 40/200, Iteration: 1986/10000 --- Training Loss:0.003379\n",
      "Epoch: 40/200, Iteration: 1988/10000 --- Training Loss:0.006629\n",
      "Epoch: 40/200, Iteration: 1990/10000 --- Training Loss:0.005127\n",
      "Epoch: 40/200, Iteration: 1992/10000 --- Training Loss:0.016629\n",
      "Epoch: 40/200, Iteration: 1994/10000 --- Training Loss:0.016667\n",
      "Epoch: 40/200, Iteration: 1996/10000 --- Training Loss:0.014355\n",
      "Epoch: 40/200, Iteration: 1998/10000 --- Training Loss:0.010966\n",
      "Epoch: 40/200, Iteration: 2000/10000 --- Training Loss:0.013010\n",
      "Epoch: 40 finished ! Train Loss: 0.00683, Test Loss: 0.00466\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 41/200, Iteration: 2002/10000 --- Training Loss:0.008723\n",
      "Epoch: 41/200, Iteration: 2004/10000 --- Training Loss:0.048158\n",
      "Epoch: 41/200, Iteration: 2006/10000 --- Training Loss:0.032219\n",
      "Epoch: 41/200, Iteration: 2008/10000 --- Training Loss:0.027185\n",
      "Epoch: 41/200, Iteration: 2010/10000 --- Training Loss:0.006500\n",
      "Epoch: 41/200, Iteration: 2012/10000 --- Training Loss:0.002792\n",
      "Epoch: 41/200, Iteration: 2014/10000 --- Training Loss:0.003655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41/200, Iteration: 2016/10000 --- Training Loss:0.002814\n",
      "Epoch: 41/200, Iteration: 2018/10000 --- Training Loss:0.003735\n",
      "Epoch: 41/200, Iteration: 2020/10000 --- Training Loss:0.006429\n",
      "Epoch: 41/200, Iteration: 2022/10000 --- Training Loss:0.003585\n",
      "Epoch: 41/200, Iteration: 2024/10000 --- Training Loss:0.002671\n",
      "Epoch: 41/200, Iteration: 2026/10000 --- Training Loss:0.004065\n",
      "Epoch: 41/200, Iteration: 2028/10000 --- Training Loss:0.005799\n",
      "Epoch: 41/200, Iteration: 2030/10000 --- Training Loss:0.003353\n",
      "Epoch: 41/200, Iteration: 2032/10000 --- Training Loss:0.003256\n",
      "Epoch: 41/200, Iteration: 2034/10000 --- Training Loss:0.005956\n",
      "Epoch: 41/200, Iteration: 2036/10000 --- Training Loss:0.004597\n",
      "Epoch: 41/200, Iteration: 2038/10000 --- Training Loss:0.008811\n",
      "Epoch: 41/200, Iteration: 2040/10000 --- Training Loss:0.005894\n",
      "Epoch: 41/200, Iteration: 2042/10000 --- Training Loss:0.002369\n",
      "Epoch: 41/200, Iteration: 2044/10000 --- Training Loss:0.003631\n",
      "Epoch: 41/200, Iteration: 2046/10000 --- Training Loss:0.005556\n",
      "Epoch: 41/200, Iteration: 2048/10000 --- Training Loss:0.004043\n",
      "Epoch: 41/200, Iteration: 2050/10000 --- Training Loss:0.002720\n",
      "Epoch: 41 finished ! Train Loss: 0.00967, Test Loss: 0.00904\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 42/200, Iteration: 2052/10000 --- Training Loss:0.003416\n",
      "Epoch: 42/200, Iteration: 2054/10000 --- Training Loss:0.005721\n",
      "Epoch: 42/200, Iteration: 2056/10000 --- Training Loss:0.004584\n",
      "Epoch: 42/200, Iteration: 2058/10000 --- Training Loss:0.004212\n",
      "Epoch: 42/200, Iteration: 2060/10000 --- Training Loss:0.002133\n",
      "Epoch: 42/200, Iteration: 2062/10000 --- Training Loss:0.001319\n",
      "Epoch: 42/200, Iteration: 2064/10000 --- Training Loss:0.001140\n",
      "Epoch: 42/200, Iteration: 2066/10000 --- Training Loss:0.005738\n",
      "Epoch: 42/200, Iteration: 2068/10000 --- Training Loss:0.001512\n",
      "Epoch: 42/200, Iteration: 2070/10000 --- Training Loss:0.002439\n",
      "Epoch: 42/200, Iteration: 2072/10000 --- Training Loss:0.002259\n",
      "Epoch: 42/200, Iteration: 2074/10000 --- Training Loss:0.001664\n",
      "Epoch: 42/200, Iteration: 2076/10000 --- Training Loss:0.001268\n",
      "Epoch: 42/200, Iteration: 2078/10000 --- Training Loss:0.005448\n",
      "Epoch: 42/200, Iteration: 2080/10000 --- Training Loss:0.002182\n",
      "Epoch: 42/200, Iteration: 2082/10000 --- Training Loss:0.004466\n",
      "Epoch: 42/200, Iteration: 2084/10000 --- Training Loss:0.008202\n",
      "Epoch: 42/200, Iteration: 2086/10000 --- Training Loss:0.004550\n",
      "Epoch: 42/200, Iteration: 2088/10000 --- Training Loss:0.004046\n",
      "Epoch: 42/200, Iteration: 2090/10000 --- Training Loss:0.003815\n",
      "Epoch: 42/200, Iteration: 2092/10000 --- Training Loss:0.005179\n",
      "Epoch: 42/200, Iteration: 2094/10000 --- Training Loss:0.003090\n",
      "Epoch: 42/200, Iteration: 2096/10000 --- Training Loss:0.006385\n",
      "Epoch: 42/200, Iteration: 2098/10000 --- Training Loss:0.002953\n",
      "Epoch: 42/200, Iteration: 2100/10000 --- Training Loss:0.002787\n",
      "Epoch: 42 finished ! Train Loss: 0.00321, Test Loss: 0.01230\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 43/200, Iteration: 2102/10000 --- Training Loss:0.002517\n",
      "Epoch: 43/200, Iteration: 2104/10000 --- Training Loss:0.002031\n",
      "Epoch: 43/200, Iteration: 2106/10000 --- Training Loss:0.002444\n",
      "Epoch: 43/200, Iteration: 2108/10000 --- Training Loss:0.003425\n",
      "Epoch: 43/200, Iteration: 2110/10000 --- Training Loss:0.003553\n",
      "Epoch: 43/200, Iteration: 2112/10000 --- Training Loss:0.042630\n",
      "Epoch: 43/200, Iteration: 2114/10000 --- Training Loss:0.016901\n",
      "Epoch: 43/200, Iteration: 2116/10000 --- Training Loss:0.007077\n",
      "Epoch: 43/200, Iteration: 2118/10000 --- Training Loss:0.012453\n",
      "Epoch: 43/200, Iteration: 2120/10000 --- Training Loss:0.065092\n",
      "Epoch: 43/200, Iteration: 2122/10000 --- Training Loss:0.029555\n",
      "Epoch: 43/200, Iteration: 2124/10000 --- Training Loss:0.009503\n",
      "Epoch: 43/200, Iteration: 2126/10000 --- Training Loss:0.038787\n",
      "Epoch: 43/200, Iteration: 2128/10000 --- Training Loss:0.036426\n",
      "Epoch: 43/200, Iteration: 2130/10000 --- Training Loss:0.061130\n",
      "Epoch: 43/200, Iteration: 2132/10000 --- Training Loss:0.035770\n",
      "Epoch: 43/200, Iteration: 2134/10000 --- Training Loss:0.073218\n",
      "Epoch: 43/200, Iteration: 2136/10000 --- Training Loss:0.070604\n",
      "Epoch: 43/200, Iteration: 2138/10000 --- Training Loss:0.037291\n",
      "Epoch: 43/200, Iteration: 2140/10000 --- Training Loss:0.043280\n",
      "Epoch: 43/200, Iteration: 2142/10000 --- Training Loss:0.043905\n",
      "Epoch: 43/200, Iteration: 2144/10000 --- Training Loss:0.062947\n",
      "Epoch: 43/200, Iteration: 2146/10000 --- Training Loss:0.028824\n",
      "Epoch: 43/200, Iteration: 2148/10000 --- Training Loss:0.010350\n",
      "Epoch: 43/200, Iteration: 2150/10000 --- Training Loss:0.010757\n",
      "Epoch: 43 finished ! Train Loss: 0.02830, Test Loss: 0.20090\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 44/200, Iteration: 2152/10000 --- Training Loss:0.034723\n",
      "Epoch: 44/200, Iteration: 2154/10000 --- Training Loss:0.056651\n",
      "Epoch: 44/200, Iteration: 2156/10000 --- Training Loss:0.049432\n",
      "Epoch: 44/200, Iteration: 2158/10000 --- Training Loss:0.012074\n",
      "Epoch: 44/200, Iteration: 2160/10000 --- Training Loss:0.003409\n",
      "Epoch: 44/200, Iteration: 2162/10000 --- Training Loss:0.019975\n",
      "Epoch: 44/200, Iteration: 2164/10000 --- Training Loss:0.006530\n",
      "Epoch: 44/200, Iteration: 2166/10000 --- Training Loss:0.034435\n",
      "Epoch: 44/200, Iteration: 2168/10000 --- Training Loss:0.131318\n",
      "Epoch: 44/200, Iteration: 2170/10000 --- Training Loss:0.095747\n",
      "Epoch: 44/200, Iteration: 2172/10000 --- Training Loss:0.086758\n",
      "Epoch: 44/200, Iteration: 2174/10000 --- Training Loss:0.081870\n",
      "Epoch: 44/200, Iteration: 2176/10000 --- Training Loss:0.056496\n",
      "Epoch: 44/200, Iteration: 2178/10000 --- Training Loss:0.102591\n",
      "Epoch: 44/200, Iteration: 2180/10000 --- Training Loss:0.048988\n",
      "Epoch: 44/200, Iteration: 2182/10000 --- Training Loss:0.107426\n",
      "Epoch: 44/200, Iteration: 2184/10000 --- Training Loss:0.094685\n",
      "Epoch: 44/200, Iteration: 2186/10000 --- Training Loss:0.230919\n",
      "Epoch: 44/200, Iteration: 2188/10000 --- Training Loss:0.333184\n",
      "Epoch: 44/200, Iteration: 2190/10000 --- Training Loss:0.197363\n",
      "Epoch: 44/200, Iteration: 2192/10000 --- Training Loss:0.330585\n",
      "Epoch: 44/200, Iteration: 2194/10000 --- Training Loss:0.323442\n",
      "Epoch: 44/200, Iteration: 2196/10000 --- Training Loss:0.305881\n",
      "Epoch: 44/200, Iteration: 2198/10000 --- Training Loss:0.290753\n",
      "Epoch: 44/200, Iteration: 2200/10000 --- Training Loss:0.060606\n",
      "Epoch: 44 finished ! Train Loss: 0.12605, Test Loss: 0.24722\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 45/200, Iteration: 2202/10000 --- Training Loss:0.095468\n",
      "Epoch: 45/200, Iteration: 2204/10000 --- Training Loss:0.168551\n",
      "Epoch: 45/200, Iteration: 2206/10000 --- Training Loss:0.083927\n",
      "Epoch: 45/200, Iteration: 2208/10000 --- Training Loss:0.045449\n",
      "Epoch: 45/200, Iteration: 2210/10000 --- Training Loss:0.115272\n",
      "Epoch: 45/200, Iteration: 2212/10000 --- Training Loss:0.111688\n",
      "Epoch: 45/200, Iteration: 2214/10000 --- Training Loss:0.056280\n",
      "Epoch: 45/200, Iteration: 2216/10000 --- Training Loss:0.045310\n",
      "Epoch: 45/200, Iteration: 2218/10000 --- Training Loss:0.121679\n",
      "Epoch: 45/200, Iteration: 2220/10000 --- Training Loss:0.037162\n",
      "Epoch: 45/200, Iteration: 2222/10000 --- Training Loss:0.088563\n",
      "Epoch: 45/200, Iteration: 2224/10000 --- Training Loss:0.073025\n",
      "Epoch: 45/200, Iteration: 2226/10000 --- Training Loss:0.023697\n",
      "Epoch: 45/200, Iteration: 2228/10000 --- Training Loss:0.043006\n",
      "Epoch: 45/200, Iteration: 2230/10000 --- Training Loss:0.031009\n",
      "Epoch: 45/200, Iteration: 2232/10000 --- Training Loss:0.036596\n",
      "Epoch: 45/200, Iteration: 2234/10000 --- Training Loss:0.021926\n",
      "Epoch: 45/200, Iteration: 2236/10000 --- Training Loss:0.016475\n",
      "Epoch: 45/200, Iteration: 2238/10000 --- Training Loss:0.014003\n",
      "Epoch: 45/200, Iteration: 2240/10000 --- Training Loss:0.013635\n",
      "Epoch: 45/200, Iteration: 2242/10000 --- Training Loss:0.021827\n",
      "Epoch: 45/200, Iteration: 2244/10000 --- Training Loss:0.011979\n",
      "Epoch: 45/200, Iteration: 2246/10000 --- Training Loss:0.016239\n",
      "Epoch: 45/200, Iteration: 2248/10000 --- Training Loss:0.011119\n",
      "Epoch: 45/200, Iteration: 2250/10000 --- Training Loss:0.010787\n",
      "Epoch: 45 finished ! Train Loss: 0.06280, Test Loss: 0.02283\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 46/200, Iteration: 2252/10000 --- Training Loss:0.006625\n",
      "Epoch: 46/200, Iteration: 2254/10000 --- Training Loss:0.007862\n",
      "Epoch: 46/200, Iteration: 2256/10000 --- Training Loss:0.005221\n",
      "Epoch: 46/200, Iteration: 2258/10000 --- Training Loss:0.010869\n",
      "Epoch: 46/200, Iteration: 2260/10000 --- Training Loss:0.008613\n",
      "Epoch: 46/200, Iteration: 2262/10000 --- Training Loss:0.007938\n",
      "Epoch: 46/200, Iteration: 2264/10000 --- Training Loss:0.009124\n",
      "Epoch: 46/200, Iteration: 2266/10000 --- Training Loss:0.005739\n",
      "Epoch: 46/200, Iteration: 2268/10000 --- Training Loss:0.004724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46/200, Iteration: 2270/10000 --- Training Loss:0.007148\n",
      "Epoch: 46/200, Iteration: 2272/10000 --- Training Loss:0.004514\n",
      "Epoch: 46/200, Iteration: 2274/10000 --- Training Loss:0.004340\n",
      "Epoch: 46/200, Iteration: 2276/10000 --- Training Loss:0.006526\n",
      "Epoch: 46/200, Iteration: 2278/10000 --- Training Loss:0.018358\n",
      "Epoch: 46/200, Iteration: 2280/10000 --- Training Loss:0.102391\n",
      "Epoch: 46/200, Iteration: 2282/10000 --- Training Loss:0.189274\n",
      "Epoch: 46/200, Iteration: 2284/10000 --- Training Loss:0.162503\n",
      "Epoch: 46/200, Iteration: 2286/10000 --- Training Loss:0.047001\n",
      "Epoch: 46/200, Iteration: 2288/10000 --- Training Loss:0.034091\n",
      "Epoch: 46/200, Iteration: 2290/10000 --- Training Loss:0.046920\n",
      "Epoch: 46/200, Iteration: 2292/10000 --- Training Loss:0.054722\n",
      "Epoch: 46/200, Iteration: 2294/10000 --- Training Loss:0.023610\n",
      "Epoch: 46/200, Iteration: 2296/10000 --- Training Loss:0.026466\n",
      "Epoch: 46/200, Iteration: 2298/10000 --- Training Loss:0.029317\n",
      "Epoch: 46/200, Iteration: 2300/10000 --- Training Loss:0.023969\n",
      "Epoch: 46 finished ! Train Loss: 0.05813, Test Loss: 0.18859\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 47/200, Iteration: 2302/10000 --- Training Loss:0.039451\n",
      "Epoch: 47/200, Iteration: 2304/10000 --- Training Loss:0.040705\n",
      "Epoch: 47/200, Iteration: 2306/10000 --- Training Loss:0.031640\n",
      "Epoch: 47/200, Iteration: 2308/10000 --- Training Loss:0.021185\n",
      "Epoch: 47/200, Iteration: 2310/10000 --- Training Loss:0.018350\n",
      "Epoch: 47/200, Iteration: 2312/10000 --- Training Loss:0.013082\n",
      "Epoch: 47/200, Iteration: 2314/10000 --- Training Loss:0.014021\n",
      "Epoch: 47/200, Iteration: 2316/10000 --- Training Loss:0.013871\n",
      "Epoch: 47/200, Iteration: 2318/10000 --- Training Loss:0.014661\n",
      "Epoch: 47/200, Iteration: 2320/10000 --- Training Loss:0.016677\n",
      "Epoch: 47/200, Iteration: 2322/10000 --- Training Loss:0.045405\n",
      "Epoch: 47/200, Iteration: 2324/10000 --- Training Loss:0.024031\n",
      "Epoch: 47/200, Iteration: 2326/10000 --- Training Loss:0.013666\n",
      "Epoch: 47/200, Iteration: 2328/10000 --- Training Loss:0.036457\n",
      "Epoch: 47/200, Iteration: 2330/10000 --- Training Loss:0.013462\n",
      "Epoch: 47/200, Iteration: 2332/10000 --- Training Loss:0.022230\n",
      "Epoch: 47/200, Iteration: 2334/10000 --- Training Loss:0.010965\n",
      "Epoch: 47/200, Iteration: 2336/10000 --- Training Loss:0.017991\n",
      "Epoch: 47/200, Iteration: 2338/10000 --- Training Loss:0.012782\n",
      "Epoch: 47/200, Iteration: 2340/10000 --- Training Loss:0.016346\n",
      "Epoch: 47/200, Iteration: 2342/10000 --- Training Loss:0.012635\n",
      "Epoch: 47/200, Iteration: 2344/10000 --- Training Loss:0.015820\n",
      "Epoch: 47/200, Iteration: 2346/10000 --- Training Loss:0.012438\n",
      "Epoch: 47/200, Iteration: 2348/10000 --- Training Loss:0.009727\n",
      "Epoch: 47/200, Iteration: 2350/10000 --- Training Loss:0.021756\n",
      "Epoch: 47 finished ! Train Loss: 0.02471, Test Loss: 0.08040\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 48/200, Iteration: 2352/10000 --- Training Loss:0.046096\n",
      "Epoch: 48/200, Iteration: 2354/10000 --- Training Loss:0.020296\n",
      "Epoch: 48/200, Iteration: 2356/10000 --- Training Loss:0.054312\n",
      "Epoch: 48/200, Iteration: 2358/10000 --- Training Loss:0.056273\n",
      "Epoch: 48/200, Iteration: 2360/10000 --- Training Loss:0.058890\n",
      "Epoch: 48/200, Iteration: 2362/10000 --- Training Loss:0.019926\n",
      "Epoch: 48/200, Iteration: 2364/10000 --- Training Loss:0.057224\n",
      "Epoch: 48/200, Iteration: 2366/10000 --- Training Loss:0.031727\n",
      "Epoch: 48/200, Iteration: 2368/10000 --- Training Loss:0.021154\n",
      "Epoch: 48/200, Iteration: 2370/10000 --- Training Loss:0.027690\n",
      "Epoch: 48/200, Iteration: 2372/10000 --- Training Loss:0.019296\n",
      "Epoch: 48/200, Iteration: 2374/10000 --- Training Loss:0.019399\n",
      "Epoch: 48/200, Iteration: 2376/10000 --- Training Loss:0.012703\n",
      "Epoch: 48/200, Iteration: 2378/10000 --- Training Loss:0.538357\n",
      "Epoch: 48/200, Iteration: 2380/10000 --- Training Loss:0.013163\n",
      "Epoch: 48/200, Iteration: 2382/10000 --- Training Loss:0.013307\n",
      "Epoch: 48/200, Iteration: 2384/10000 --- Training Loss:0.034597\n",
      "Epoch: 48/200, Iteration: 2386/10000 --- Training Loss:0.027784\n",
      "Epoch: 48/200, Iteration: 2388/10000 --- Training Loss:0.026884\n",
      "Epoch: 48/200, Iteration: 2390/10000 --- Training Loss:0.064399\n",
      "Epoch: 48/200, Iteration: 2392/10000 --- Training Loss:0.018647\n",
      "Epoch: 48/200, Iteration: 2394/10000 --- Training Loss:0.023664\n",
      "Epoch: 48/200, Iteration: 2396/10000 --- Training Loss:0.024198\n",
      "Epoch: 48/200, Iteration: 2398/10000 --- Training Loss:0.008753\n",
      "Epoch: 48/200, Iteration: 2400/10000 --- Training Loss:0.010647\n",
      "Epoch: 48 finished ! Train Loss: 0.03784, Test Loss: 0.45827\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 49/200, Iteration: 2402/10000 --- Training Loss:0.010792\n",
      "Epoch: 49/200, Iteration: 2404/10000 --- Training Loss:0.013633\n",
      "Epoch: 49/200, Iteration: 2406/10000 --- Training Loss:0.027559\n",
      "Epoch: 49/200, Iteration: 2408/10000 --- Training Loss:0.016480\n",
      "Epoch: 49/200, Iteration: 2410/10000 --- Training Loss:0.016574\n",
      "Epoch: 49/200, Iteration: 2412/10000 --- Training Loss:0.009034\n",
      "Epoch: 49/200, Iteration: 2414/10000 --- Training Loss:0.012274\n",
      "Epoch: 49/200, Iteration: 2416/10000 --- Training Loss:0.007172\n",
      "Epoch: 49/200, Iteration: 2418/10000 --- Training Loss:0.012293\n",
      "Epoch: 49/200, Iteration: 2420/10000 --- Training Loss:0.010978\n",
      "Epoch: 49/200, Iteration: 2422/10000 --- Training Loss:0.013831\n",
      "Epoch: 49/200, Iteration: 2424/10000 --- Training Loss:0.013926\n",
      "Epoch: 49/200, Iteration: 2426/10000 --- Training Loss:0.023665\n",
      "Epoch: 49/200, Iteration: 2428/10000 --- Training Loss:0.010518\n",
      "Epoch: 49/200, Iteration: 2430/10000 --- Training Loss:0.019210\n",
      "Epoch: 49/200, Iteration: 2432/10000 --- Training Loss:0.011582\n",
      "Epoch: 49/200, Iteration: 2434/10000 --- Training Loss:0.051468\n",
      "Epoch: 49/200, Iteration: 2436/10000 --- Training Loss:0.009798\n",
      "Epoch: 49/200, Iteration: 2438/10000 --- Training Loss:0.025669\n",
      "Epoch: 49/200, Iteration: 2440/10000 --- Training Loss:0.005842\n",
      "Epoch: 49/200, Iteration: 2442/10000 --- Training Loss:0.010528\n",
      "Epoch: 49/200, Iteration: 2444/10000 --- Training Loss:0.008042\n",
      "Epoch: 49/200, Iteration: 2446/10000 --- Training Loss:0.005750\n",
      "Epoch: 49/200, Iteration: 2448/10000 --- Training Loss:0.006502\n",
      "Epoch: 49/200, Iteration: 2450/10000 --- Training Loss:0.005484\n",
      "Epoch: 49 finished ! Train Loss: 0.01527, Test Loss: 0.03320\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 50/200, Iteration: 2452/10000 --- Training Loss:0.007303\n",
      "Epoch: 50/200, Iteration: 2454/10000 --- Training Loss:0.007179\n",
      "Epoch: 50/200, Iteration: 2456/10000 --- Training Loss:0.017175\n",
      "Epoch: 50/200, Iteration: 2458/10000 --- Training Loss:0.005669\n",
      "Epoch: 50/200, Iteration: 2460/10000 --- Training Loss:0.003430\n",
      "Epoch: 50/200, Iteration: 2462/10000 --- Training Loss:0.003453\n",
      "Epoch: 50/200, Iteration: 2464/10000 --- Training Loss:0.004575\n",
      "Epoch: 50/200, Iteration: 2466/10000 --- Training Loss:0.005555\n",
      "Epoch: 50/200, Iteration: 2468/10000 --- Training Loss:0.002340\n",
      "Epoch: 50/200, Iteration: 2470/10000 --- Training Loss:0.002932\n",
      "Epoch: 50/200, Iteration: 2472/10000 --- Training Loss:0.001842\n",
      "Epoch: 50/200, Iteration: 2474/10000 --- Training Loss:0.003238\n",
      "Epoch: 50/200, Iteration: 2476/10000 --- Training Loss:0.003159\n",
      "Epoch: 50/200, Iteration: 2478/10000 --- Training Loss:0.002345\n",
      "Epoch: 50/200, Iteration: 2480/10000 --- Training Loss:0.002053\n",
      "Epoch: 50/200, Iteration: 2482/10000 --- Training Loss:0.004984\n",
      "Epoch: 50/200, Iteration: 2484/10000 --- Training Loss:0.001878\n",
      "Epoch: 50/200, Iteration: 2486/10000 --- Training Loss:0.002638\n",
      "Epoch: 50/200, Iteration: 2488/10000 --- Training Loss:0.001865\n",
      "Epoch: 50/200, Iteration: 2490/10000 --- Training Loss:0.002069\n",
      "Epoch: 50/200, Iteration: 2492/10000 --- Training Loss:0.001345\n",
      "Epoch: 50/200, Iteration: 2494/10000 --- Training Loss:0.002519\n",
      "Epoch: 50/200, Iteration: 2496/10000 --- Training Loss:0.001267\n",
      "Epoch: 50/200, Iteration: 2498/10000 --- Training Loss:0.001810\n",
      "Epoch: 50/200, Iteration: 2500/10000 --- Training Loss:0.001709\n",
      "Epoch: 50 finished ! Train Loss: 0.00384, Test Loss: 0.00315\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 25 percent completed\n",
      "Epoch: 51/200, Iteration: 2502/10000 --- Training Loss:0.001237\n",
      "Epoch: 51/200, Iteration: 2504/10000 --- Training Loss:0.001793\n",
      "Epoch: 51/200, Iteration: 2506/10000 --- Training Loss:0.001392\n",
      "Epoch: 51/200, Iteration: 2508/10000 --- Training Loss:0.003079\n",
      "Epoch: 51/200, Iteration: 2510/10000 --- Training Loss:0.001985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51/200, Iteration: 2512/10000 --- Training Loss:0.003975\n",
      "Epoch: 51/200, Iteration: 2514/10000 --- Training Loss:0.002938\n",
      "Epoch: 51/200, Iteration: 2516/10000 --- Training Loss:0.003061\n",
      "Epoch: 51/200, Iteration: 2518/10000 --- Training Loss:0.002965\n",
      "Epoch: 51/200, Iteration: 2520/10000 --- Training Loss:0.002125\n",
      "Epoch: 51/200, Iteration: 2522/10000 --- Training Loss:0.001844\n",
      "Epoch: 51/200, Iteration: 2524/10000 --- Training Loss:0.001510\n",
      "Epoch: 51/200, Iteration: 2526/10000 --- Training Loss:0.001724\n",
      "Epoch: 51/200, Iteration: 2528/10000 --- Training Loss:0.002680\n",
      "Epoch: 51/200, Iteration: 2530/10000 --- Training Loss:0.001017\n",
      "Epoch: 51/200, Iteration: 2532/10000 --- Training Loss:0.001171\n",
      "Epoch: 51/200, Iteration: 2534/10000 --- Training Loss:0.001528\n",
      "Epoch: 51/200, Iteration: 2536/10000 --- Training Loss:0.001198\n",
      "Epoch: 51/200, Iteration: 2538/10000 --- Training Loss:0.003502\n",
      "Epoch: 51/200, Iteration: 2540/10000 --- Training Loss:0.003493\n",
      "Epoch: 51/200, Iteration: 2542/10000 --- Training Loss:0.001398\n",
      "Epoch: 51/200, Iteration: 2544/10000 --- Training Loss:0.001645\n",
      "Epoch: 51/200, Iteration: 2546/10000 --- Training Loss:0.001104\n",
      "Epoch: 51/200, Iteration: 2548/10000 --- Training Loss:0.003490\n",
      "Epoch: 51/200, Iteration: 2550/10000 --- Training Loss:0.003526\n",
      "Epoch: 51 finished ! Train Loss: 0.00229, Test Loss: 0.00202\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 25 percent completed\n",
      "Epoch: 52/200, Iteration: 2552/10000 --- Training Loss:0.001639\n",
      "Epoch: 52/200, Iteration: 2554/10000 --- Training Loss:0.002181\n",
      "Epoch: 52/200, Iteration: 2556/10000 --- Training Loss:0.002613\n",
      "Epoch: 52/200, Iteration: 2558/10000 --- Training Loss:0.001695\n",
      "Epoch: 52/200, Iteration: 2560/10000 --- Training Loss:0.001312\n",
      "Epoch: 52/200, Iteration: 2562/10000 --- Training Loss:0.003393\n",
      "Epoch: 52/200, Iteration: 2564/10000 --- Training Loss:0.002130\n",
      "Epoch: 52/200, Iteration: 2566/10000 --- Training Loss:0.001099\n",
      "Epoch: 52/200, Iteration: 2568/10000 --- Training Loss:0.001582\n",
      "Epoch: 52/200, Iteration: 2570/10000 --- Training Loss:0.001917\n",
      "Epoch: 52/200, Iteration: 2572/10000 --- Training Loss:0.004497\n",
      "Epoch: 52/200, Iteration: 2574/10000 --- Training Loss:0.002570\n",
      "Epoch: 52/200, Iteration: 2576/10000 --- Training Loss:0.001849\n",
      "Epoch: 52/200, Iteration: 2578/10000 --- Training Loss:0.001744\n",
      "Epoch: 52/200, Iteration: 2580/10000 --- Training Loss:0.001516\n",
      "Epoch: 52/200, Iteration: 2582/10000 --- Training Loss:0.001094\n",
      "Epoch: 52/200, Iteration: 2584/10000 --- Training Loss:0.002200\n",
      "Epoch: 52/200, Iteration: 2586/10000 --- Training Loss:0.001606\n",
      "Epoch: 52/200, Iteration: 2588/10000 --- Training Loss:0.000977\n",
      "Epoch: 52/200, Iteration: 2590/10000 --- Training Loss:0.001392\n",
      "Epoch: 52/200, Iteration: 2592/10000 --- Training Loss:0.000933\n",
      "Epoch: 52/200, Iteration: 2594/10000 --- Training Loss:0.001822\n",
      "Epoch: 52/200, Iteration: 2596/10000 --- Training Loss:0.001956\n",
      "Epoch: 52/200, Iteration: 2598/10000 --- Training Loss:0.001549\n",
      "Epoch: 52/200, Iteration: 2600/10000 --- Training Loss:0.001638\n",
      "Epoch: 52 finished ! Train Loss: 0.00176, Test Loss: 0.00224\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 53/200, Iteration: 2602/10000 --- Training Loss:0.003792\n",
      "Epoch: 53/200, Iteration: 2604/10000 --- Training Loss:0.001139\n",
      "Epoch: 53/200, Iteration: 2606/10000 --- Training Loss:0.001920\n",
      "Epoch: 53/200, Iteration: 2608/10000 --- Training Loss:0.001815\n",
      "Epoch: 53/200, Iteration: 2610/10000 --- Training Loss:0.002349\n",
      "Epoch: 53/200, Iteration: 2612/10000 --- Training Loss:0.001620\n",
      "Epoch: 53/200, Iteration: 2614/10000 --- Training Loss:0.002141\n",
      "Epoch: 53/200, Iteration: 2616/10000 --- Training Loss:0.001061\n",
      "Epoch: 53/200, Iteration: 2618/10000 --- Training Loss:0.001203\n",
      "Epoch: 53/200, Iteration: 2620/10000 --- Training Loss:0.002049\n",
      "Epoch: 53/200, Iteration: 2622/10000 --- Training Loss:0.000808\n",
      "Epoch: 53/200, Iteration: 2624/10000 --- Training Loss:0.001754\n",
      "Epoch: 53/200, Iteration: 2626/10000 --- Training Loss:0.001193\n",
      "Epoch: 53/200, Iteration: 2628/10000 --- Training Loss:0.001440\n",
      "Epoch: 53/200, Iteration: 2630/10000 --- Training Loss:0.001246\n",
      "Epoch: 53/200, Iteration: 2632/10000 --- Training Loss:0.001535\n",
      "Epoch: 53/200, Iteration: 2634/10000 --- Training Loss:0.000905\n",
      "Epoch: 53/200, Iteration: 2636/10000 --- Training Loss:0.001510\n",
      "Epoch: 53/200, Iteration: 2638/10000 --- Training Loss:0.001220\n",
      "Epoch: 53/200, Iteration: 2640/10000 --- Training Loss:0.000984\n",
      "Epoch: 53/200, Iteration: 2642/10000 --- Training Loss:0.000929\n",
      "Epoch: 53/200, Iteration: 2644/10000 --- Training Loss:0.001520\n",
      "Epoch: 53/200, Iteration: 2646/10000 --- Training Loss:0.001380\n",
      "Epoch: 53/200, Iteration: 2648/10000 --- Training Loss:0.000928\n",
      "Epoch: 53/200, Iteration: 2650/10000 --- Training Loss:0.000808\n",
      "Epoch: 53 finished ! Train Loss: 0.00151, Test Loss: 0.00183\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 26 percent completed\n",
      "Epoch: 54/200, Iteration: 2652/10000 --- Training Loss:0.001117\n",
      "Epoch: 54/200, Iteration: 2654/10000 --- Training Loss:0.000809\n",
      "Epoch: 54/200, Iteration: 2656/10000 --- Training Loss:0.001416\n",
      "Epoch: 54/200, Iteration: 2658/10000 --- Training Loss:0.000946\n",
      "Epoch: 54/200, Iteration: 2660/10000 --- Training Loss:0.000894\n",
      "Epoch: 54/200, Iteration: 2662/10000 --- Training Loss:0.001151\n",
      "Epoch: 54/200, Iteration: 2664/10000 --- Training Loss:0.001647\n",
      "Epoch: 54/200, Iteration: 2666/10000 --- Training Loss:0.001404\n",
      "Epoch: 54/200, Iteration: 2668/10000 --- Training Loss:0.001382\n",
      "Epoch: 54/200, Iteration: 2670/10000 --- Training Loss:0.001834\n",
      "Epoch: 54/200, Iteration: 2672/10000 --- Training Loss:0.001298\n",
      "Epoch: 54/200, Iteration: 2674/10000 --- Training Loss:0.000891\n",
      "Epoch: 54/200, Iteration: 2676/10000 --- Training Loss:0.001996\n",
      "Epoch: 54/200, Iteration: 2678/10000 --- Training Loss:0.001014\n",
      "Epoch: 54/200, Iteration: 2680/10000 --- Training Loss:0.002086\n",
      "Epoch: 54/200, Iteration: 2682/10000 --- Training Loss:0.001323\n",
      "Epoch: 54/200, Iteration: 2684/10000 --- Training Loss:0.001134\n",
      "Epoch: 54/200, Iteration: 2686/10000 --- Training Loss:0.001278\n",
      "Epoch: 54/200, Iteration: 2688/10000 --- Training Loss:0.000801\n",
      "Epoch: 54/200, Iteration: 2690/10000 --- Training Loss:0.000798\n",
      "Epoch: 54/200, Iteration: 2692/10000 --- Training Loss:0.000995\n",
      "Epoch: 54/200, Iteration: 2694/10000 --- Training Loss:0.001698\n",
      "Epoch: 54/200, Iteration: 2696/10000 --- Training Loss:0.002029\n",
      "Epoch: 54/200, Iteration: 2698/10000 --- Training Loss:0.000557\n",
      "Epoch: 54/200, Iteration: 2700/10000 --- Training Loss:0.001802\n",
      "Epoch: 54 finished ! Train Loss: 0.00140, Test Loss: 0.00223\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 55/200, Iteration: 2702/10000 --- Training Loss:0.000738\n",
      "Epoch: 55/200, Iteration: 2704/10000 --- Training Loss:0.001394\n",
      "Epoch: 55/200, Iteration: 2706/10000 --- Training Loss:0.002244\n",
      "Epoch: 55/200, Iteration: 2708/10000 --- Training Loss:0.000715\n",
      "Epoch: 55/200, Iteration: 2710/10000 --- Training Loss:0.003286\n",
      "Epoch: 55/200, Iteration: 2712/10000 --- Training Loss:0.001345\n",
      "Epoch: 55/200, Iteration: 2714/10000 --- Training Loss:0.001470\n",
      "Epoch: 55/200, Iteration: 2716/10000 --- Training Loss:0.001083\n",
      "Epoch: 55/200, Iteration: 2718/10000 --- Training Loss:0.001295\n",
      "Epoch: 55/200, Iteration: 2720/10000 --- Training Loss:0.001240\n",
      "Epoch: 55/200, Iteration: 2722/10000 --- Training Loss:0.000813\n",
      "Epoch: 55/200, Iteration: 2724/10000 --- Training Loss:0.001201\n",
      "Epoch: 55/200, Iteration: 2726/10000 --- Training Loss:0.001463\n",
      "Epoch: 55/200, Iteration: 2728/10000 --- Training Loss:0.001201\n",
      "Epoch: 55/200, Iteration: 2730/10000 --- Training Loss:0.001552\n",
      "Epoch: 55/200, Iteration: 2732/10000 --- Training Loss:0.001735\n",
      "Epoch: 55/200, Iteration: 2734/10000 --- Training Loss:0.001474\n",
      "Epoch: 55/200, Iteration: 2736/10000 --- Training Loss:0.001218\n",
      "Epoch: 55/200, Iteration: 2738/10000 --- Training Loss:0.000948\n",
      "Epoch: 55/200, Iteration: 2740/10000 --- Training Loss:0.001401\n",
      "Epoch: 55/200, Iteration: 2742/10000 --- Training Loss:0.001077\n",
      "Epoch: 55/200, Iteration: 2744/10000 --- Training Loss:0.000930\n",
      "Epoch: 55/200, Iteration: 2746/10000 --- Training Loss:0.001983\n",
      "Epoch: 55/200, Iteration: 2748/10000 --- Training Loss:0.001676\n",
      "Epoch: 55/200, Iteration: 2750/10000 --- Training Loss:0.000947\n",
      "Epoch: 55 finished ! Train Loss: 0.00129, Test Loss: 0.00171\n",
      "Epoch consuming time: 0m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved: 27 percent completed\n",
      "Epoch: 56/200, Iteration: 2752/10000 --- Training Loss:0.000987\n",
      "Epoch: 56/200, Iteration: 2754/10000 --- Training Loss:0.001353\n",
      "Epoch: 56/200, Iteration: 2756/10000 --- Training Loss:0.001719\n",
      "Epoch: 56/200, Iteration: 2758/10000 --- Training Loss:0.000735\n",
      "Epoch: 56/200, Iteration: 2760/10000 --- Training Loss:0.000889\n",
      "Epoch: 56/200, Iteration: 2762/10000 --- Training Loss:0.001091\n",
      "Epoch: 56/200, Iteration: 2764/10000 --- Training Loss:0.002120\n",
      "Epoch: 56/200, Iteration: 2766/10000 --- Training Loss:0.001036\n",
      "Epoch: 56/200, Iteration: 2768/10000 --- Training Loss:0.001529\n",
      "Epoch: 56/200, Iteration: 2770/10000 --- Training Loss:0.000751\n",
      "Epoch: 56/200, Iteration: 2772/10000 --- Training Loss:0.000567\n",
      "Epoch: 56/200, Iteration: 2774/10000 --- Training Loss:0.001903\n",
      "Epoch: 56/200, Iteration: 2776/10000 --- Training Loss:0.000878\n",
      "Epoch: 56/200, Iteration: 2778/10000 --- Training Loss:0.001696\n",
      "Epoch: 56/200, Iteration: 2780/10000 --- Training Loss:0.002448\n",
      "Epoch: 56/200, Iteration: 2782/10000 --- Training Loss:0.000781\n",
      "Epoch: 56/200, Iteration: 2784/10000 --- Training Loss:0.001290\n",
      "Epoch: 56/200, Iteration: 2786/10000 --- Training Loss:0.001225\n",
      "Epoch: 56/200, Iteration: 2788/10000 --- Training Loss:0.001051\n",
      "Epoch: 56/200, Iteration: 2790/10000 --- Training Loss:0.000758\n",
      "Epoch: 56/200, Iteration: 2792/10000 --- Training Loss:0.000594\n",
      "Epoch: 56/200, Iteration: 2794/10000 --- Training Loss:0.000834\n",
      "Epoch: 56/200, Iteration: 2796/10000 --- Training Loss:0.000606\n",
      "Epoch: 56/200, Iteration: 2798/10000 --- Training Loss:0.000826\n",
      "Epoch: 56/200, Iteration: 2800/10000 --- Training Loss:0.000954\n",
      "Epoch: 56 finished ! Train Loss: 0.00107, Test Loss: 0.00186\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 57/200, Iteration: 2802/10000 --- Training Loss:0.000547\n",
      "Epoch: 57/200, Iteration: 2804/10000 --- Training Loss:0.001080\n",
      "Epoch: 57/200, Iteration: 2806/10000 --- Training Loss:0.000832\n",
      "Epoch: 57/200, Iteration: 2808/10000 --- Training Loss:0.001564\n",
      "Epoch: 57/200, Iteration: 2810/10000 --- Training Loss:0.000750\n",
      "Epoch: 57/200, Iteration: 2812/10000 --- Training Loss:0.000701\n",
      "Epoch: 57/200, Iteration: 2814/10000 --- Training Loss:0.000754\n",
      "Epoch: 57/200, Iteration: 2816/10000 --- Training Loss:0.000558\n",
      "Epoch: 57/200, Iteration: 2818/10000 --- Training Loss:0.000770\n",
      "Epoch: 57/200, Iteration: 2820/10000 --- Training Loss:0.001032\n",
      "Epoch: 57/200, Iteration: 2822/10000 --- Training Loss:0.000670\n",
      "Epoch: 57/200, Iteration: 2824/10000 --- Training Loss:0.000651\n",
      "Epoch: 57/200, Iteration: 2826/10000 --- Training Loss:0.000754\n",
      "Epoch: 57/200, Iteration: 2828/10000 --- Training Loss:0.000826\n",
      "Epoch: 57/200, Iteration: 2830/10000 --- Training Loss:0.001036\n",
      "Epoch: 57/200, Iteration: 2832/10000 --- Training Loss:0.000582\n",
      "Epoch: 57/200, Iteration: 2834/10000 --- Training Loss:0.000812\n",
      "Epoch: 57/200, Iteration: 2836/10000 --- Training Loss:0.001199\n",
      "Epoch: 57/200, Iteration: 2838/10000 --- Training Loss:0.001302\n",
      "Epoch: 57/200, Iteration: 2840/10000 --- Training Loss:0.000895\n",
      "Epoch: 57/200, Iteration: 2842/10000 --- Training Loss:0.001331\n",
      "Epoch: 57/200, Iteration: 2844/10000 --- Training Loss:0.001841\n",
      "Epoch: 57/200, Iteration: 2846/10000 --- Training Loss:0.000788\n",
      "Epoch: 57/200, Iteration: 2848/10000 --- Training Loss:0.000725\n",
      "Epoch: 57/200, Iteration: 2850/10000 --- Training Loss:0.000533\n",
      "Epoch: 57 finished ! Train Loss: 0.00097, Test Loss: 0.00252\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 58/200, Iteration: 2852/10000 --- Training Loss:0.000750\n",
      "Epoch: 58/200, Iteration: 2854/10000 --- Training Loss:0.000861\n",
      "Epoch: 58/200, Iteration: 2856/10000 --- Training Loss:0.000777\n",
      "Epoch: 58/200, Iteration: 2858/10000 --- Training Loss:0.000983\n",
      "Epoch: 58/200, Iteration: 2860/10000 --- Training Loss:0.000856\n",
      "Epoch: 58/200, Iteration: 2862/10000 --- Training Loss:0.000592\n",
      "Epoch: 58/200, Iteration: 2864/10000 --- Training Loss:0.000552\n",
      "Epoch: 58/200, Iteration: 2866/10000 --- Training Loss:0.000745\n",
      "Epoch: 58/200, Iteration: 2868/10000 --- Training Loss:0.000562\n",
      "Epoch: 58/200, Iteration: 2870/10000 --- Training Loss:0.000508\n",
      "Epoch: 58/200, Iteration: 2872/10000 --- Training Loss:0.000964\n",
      "Epoch: 58/200, Iteration: 2874/10000 --- Training Loss:0.000791\n",
      "Epoch: 58/200, Iteration: 2876/10000 --- Training Loss:0.000815\n",
      "Epoch: 58/200, Iteration: 2878/10000 --- Training Loss:0.000676\n",
      "Epoch: 58/200, Iteration: 2880/10000 --- Training Loss:0.000517\n",
      "Epoch: 58/200, Iteration: 2882/10000 --- Training Loss:0.000707\n",
      "Epoch: 58/200, Iteration: 2884/10000 --- Training Loss:0.000849\n",
      "Epoch: 58/200, Iteration: 2886/10000 --- Training Loss:0.000502\n",
      "Epoch: 58/200, Iteration: 2888/10000 --- Training Loss:0.000603\n",
      "Epoch: 58/200, Iteration: 2890/10000 --- Training Loss:0.003382\n",
      "Epoch: 58/200, Iteration: 2892/10000 --- Training Loss:0.000676\n",
      "Epoch: 58/200, Iteration: 2894/10000 --- Training Loss:0.000716\n",
      "Epoch: 58/200, Iteration: 2896/10000 --- Training Loss:0.000752\n",
      "Epoch: 58/200, Iteration: 2898/10000 --- Training Loss:0.000616\n",
      "Epoch: 58/200, Iteration: 2900/10000 --- Training Loss:0.000575\n",
      "Epoch: 58 finished ! Train Loss: 0.00086, Test Loss: 0.00196\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 59/200, Iteration: 2902/10000 --- Training Loss:0.000650\n",
      "Epoch: 59/200, Iteration: 2904/10000 --- Training Loss:0.000728\n",
      "Epoch: 59/200, Iteration: 2906/10000 --- Training Loss:0.001447\n",
      "Epoch: 59/200, Iteration: 2908/10000 --- Training Loss:0.000575\n",
      "Epoch: 59/200, Iteration: 2910/10000 --- Training Loss:0.000515\n",
      "Epoch: 59/200, Iteration: 2912/10000 --- Training Loss:0.000954\n",
      "Epoch: 59/200, Iteration: 2914/10000 --- Training Loss:0.000626\n",
      "Epoch: 59/200, Iteration: 2916/10000 --- Training Loss:0.000418\n",
      "Epoch: 59/200, Iteration: 2918/10000 --- Training Loss:0.000467\n",
      "Epoch: 59/200, Iteration: 2920/10000 --- Training Loss:0.000791\n",
      "Epoch: 59/200, Iteration: 2922/10000 --- Training Loss:0.000883\n",
      "Epoch: 59/200, Iteration: 2924/10000 --- Training Loss:0.000706\n",
      "Epoch: 59/200, Iteration: 2926/10000 --- Training Loss:0.000719\n",
      "Epoch: 59/200, Iteration: 2928/10000 --- Training Loss:0.002446\n",
      "Epoch: 59/200, Iteration: 2930/10000 --- Training Loss:0.001023\n",
      "Epoch: 59/200, Iteration: 2932/10000 --- Training Loss:0.001162\n",
      "Epoch: 59/200, Iteration: 2934/10000 --- Training Loss:0.001061\n",
      "Epoch: 59/200, Iteration: 2936/10000 --- Training Loss:0.000612\n",
      "Epoch: 59/200, Iteration: 2938/10000 --- Training Loss:0.000580\n",
      "Epoch: 59/200, Iteration: 2940/10000 --- Training Loss:0.000771\n",
      "Epoch: 59/200, Iteration: 2942/10000 --- Training Loss:0.001456\n",
      "Epoch: 59/200, Iteration: 2944/10000 --- Training Loss:0.001149\n",
      "Epoch: 59/200, Iteration: 2946/10000 --- Training Loss:0.005351\n",
      "Epoch: 59/200, Iteration: 2948/10000 --- Training Loss:0.000767\n",
      "Epoch: 59/200, Iteration: 2950/10000 --- Training Loss:0.000747\n",
      "Epoch: 59 finished ! Train Loss: 0.00094, Test Loss: 0.00166\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 29 percent completed\n",
      "Epoch: 60/200, Iteration: 2952/10000 --- Training Loss:0.001098\n",
      "Epoch: 60/200, Iteration: 2954/10000 --- Training Loss:0.000688\n",
      "Epoch: 60/200, Iteration: 2956/10000 --- Training Loss:0.000861\n",
      "Epoch: 60/200, Iteration: 2958/10000 --- Training Loss:0.000621\n",
      "Epoch: 60/200, Iteration: 2960/10000 --- Training Loss:0.000795\n",
      "Epoch: 60/200, Iteration: 2962/10000 --- Training Loss:0.000757\n",
      "Epoch: 60/200, Iteration: 2964/10000 --- Training Loss:0.000644\n",
      "Epoch: 60/200, Iteration: 2966/10000 --- Training Loss:0.000661\n",
      "Epoch: 60/200, Iteration: 2968/10000 --- Training Loss:0.000616\n",
      "Epoch: 60/200, Iteration: 2970/10000 --- Training Loss:0.000936\n",
      "Epoch: 60/200, Iteration: 2972/10000 --- Training Loss:0.001214\n",
      "Epoch: 60/200, Iteration: 2974/10000 --- Training Loss:0.001158\n",
      "Epoch: 60/200, Iteration: 2976/10000 --- Training Loss:0.000920\n",
      "Epoch: 60/200, Iteration: 2978/10000 --- Training Loss:0.000506\n",
      "Epoch: 60/200, Iteration: 2980/10000 --- Training Loss:0.000479\n",
      "Epoch: 60/200, Iteration: 2982/10000 --- Training Loss:0.000778\n",
      "Epoch: 60/200, Iteration: 2984/10000 --- Training Loss:0.000411\n",
      "Epoch: 60/200, Iteration: 2986/10000 --- Training Loss:0.000637\n",
      "Epoch: 60/200, Iteration: 2988/10000 --- Training Loss:0.000714\n",
      "Epoch: 60/200, Iteration: 2990/10000 --- Training Loss:0.000462\n",
      "Epoch: 60/200, Iteration: 2992/10000 --- Training Loss:0.000867\n",
      "Epoch: 60/200, Iteration: 2994/10000 --- Training Loss:0.000733\n",
      "Epoch: 60/200, Iteration: 2996/10000 --- Training Loss:0.000430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60/200, Iteration: 2998/10000 --- Training Loss:0.000691\n",
      "Epoch: 60/200, Iteration: 3000/10000 --- Training Loss:0.001508\n",
      "Epoch: 60 finished ! Train Loss: 0.00088, Test Loss: 0.00304\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 61/200, Iteration: 3002/10000 --- Training Loss:0.000972\n",
      "Epoch: 61/200, Iteration: 3004/10000 --- Training Loss:0.000792\n",
      "Epoch: 61/200, Iteration: 3006/10000 --- Training Loss:0.001131\n",
      "Epoch: 61/200, Iteration: 3008/10000 --- Training Loss:0.000536\n",
      "Epoch: 61/200, Iteration: 3010/10000 --- Training Loss:0.001274\n",
      "Epoch: 61/200, Iteration: 3012/10000 --- Training Loss:0.000650\n",
      "Epoch: 61/200, Iteration: 3014/10000 --- Training Loss:0.000650\n",
      "Epoch: 61/200, Iteration: 3016/10000 --- Training Loss:0.001324\n",
      "Epoch: 61/200, Iteration: 3018/10000 --- Training Loss:0.000435\n",
      "Epoch: 61/200, Iteration: 3020/10000 --- Training Loss:0.000590\n",
      "Epoch: 61/200, Iteration: 3022/10000 --- Training Loss:0.000634\n",
      "Epoch: 61/200, Iteration: 3024/10000 --- Training Loss:0.000765\n",
      "Epoch: 61/200, Iteration: 3026/10000 --- Training Loss:0.000397\n",
      "Epoch: 61/200, Iteration: 3028/10000 --- Training Loss:0.000510\n",
      "Epoch: 61/200, Iteration: 3030/10000 --- Training Loss:0.001849\n",
      "Epoch: 61/200, Iteration: 3032/10000 --- Training Loss:0.001113\n",
      "Epoch: 61/200, Iteration: 3034/10000 --- Training Loss:0.000929\n",
      "Epoch: 61/200, Iteration: 3036/10000 --- Training Loss:0.000570\n",
      "Epoch: 61/200, Iteration: 3038/10000 --- Training Loss:0.001411\n",
      "Epoch: 61/200, Iteration: 3040/10000 --- Training Loss:0.001135\n",
      "Epoch: 61/200, Iteration: 3042/10000 --- Training Loss:0.001269\n",
      "Epoch: 61/200, Iteration: 3044/10000 --- Training Loss:0.000693\n",
      "Epoch: 61/200, Iteration: 3046/10000 --- Training Loss:0.001114\n",
      "Epoch: 61/200, Iteration: 3048/10000 --- Training Loss:0.000352\n",
      "Epoch: 61/200, Iteration: 3050/10000 --- Training Loss:0.001909\n",
      "Epoch: 61 finished ! Train Loss: 0.00083, Test Loss: 0.00202\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 62/200, Iteration: 3052/10000 --- Training Loss:0.000523\n",
      "Epoch: 62/200, Iteration: 3054/10000 --- Training Loss:0.000743\n",
      "Epoch: 62/200, Iteration: 3056/10000 --- Training Loss:0.000665\n",
      "Epoch: 62/200, Iteration: 3058/10000 --- Training Loss:0.000515\n",
      "Epoch: 62/200, Iteration: 3060/10000 --- Training Loss:0.000297\n",
      "Epoch: 62/200, Iteration: 3062/10000 --- Training Loss:0.000493\n",
      "Epoch: 62/200, Iteration: 3064/10000 --- Training Loss:0.007473\n",
      "Epoch: 62/200, Iteration: 3066/10000 --- Training Loss:0.000624\n",
      "Epoch: 62/200, Iteration: 3068/10000 --- Training Loss:0.000316\n",
      "Epoch: 62/200, Iteration: 3070/10000 --- Training Loss:0.000458\n",
      "Epoch: 62/200, Iteration: 3072/10000 --- Training Loss:0.001869\n",
      "Epoch: 62/200, Iteration: 3074/10000 --- Training Loss:0.000519\n",
      "Epoch: 62/200, Iteration: 3076/10000 --- Training Loss:0.000457\n",
      "Epoch: 62/200, Iteration: 3078/10000 --- Training Loss:0.000515\n",
      "Epoch: 62/200, Iteration: 3080/10000 --- Training Loss:0.000718\n",
      "Epoch: 62/200, Iteration: 3082/10000 --- Training Loss:0.000345\n",
      "Epoch: 62/200, Iteration: 3084/10000 --- Training Loss:0.000611\n",
      "Epoch: 62/200, Iteration: 3086/10000 --- Training Loss:0.001207\n",
      "Epoch: 62/200, Iteration: 3088/10000 --- Training Loss:0.000500\n",
      "Epoch: 62/200, Iteration: 3090/10000 --- Training Loss:0.001178\n",
      "Epoch: 62/200, Iteration: 3092/10000 --- Training Loss:0.000688\n",
      "Epoch: 62/200, Iteration: 3094/10000 --- Training Loss:0.000681\n",
      "Epoch: 62/200, Iteration: 3096/10000 --- Training Loss:0.001026\n",
      "Epoch: 62/200, Iteration: 3098/10000 --- Training Loss:0.000662\n",
      "Epoch: 62/200, Iteration: 3100/10000 --- Training Loss:0.000618\n",
      "Epoch: 62 finished ! Train Loss: 0.00092, Test Loss: 0.00216\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 63/200, Iteration: 3102/10000 --- Training Loss:0.001898\n",
      "Epoch: 63/200, Iteration: 3104/10000 --- Training Loss:0.000407\n",
      "Epoch: 63/200, Iteration: 3106/10000 --- Training Loss:0.000703\n",
      "Epoch: 63/200, Iteration: 3108/10000 --- Training Loss:0.000838\n",
      "Epoch: 63/200, Iteration: 3110/10000 --- Training Loss:0.000671\n",
      "Epoch: 63/200, Iteration: 3112/10000 --- Training Loss:0.000873\n",
      "Epoch: 63/200, Iteration: 3114/10000 --- Training Loss:0.000556\n",
      "Epoch: 63/200, Iteration: 3116/10000 --- Training Loss:0.001494\n",
      "Epoch: 63/200, Iteration: 3118/10000 --- Training Loss:0.000462\n",
      "Epoch: 63/200, Iteration: 3120/10000 --- Training Loss:0.001327\n",
      "Epoch: 63/200, Iteration: 3122/10000 --- Training Loss:0.000409\n",
      "Epoch: 63/200, Iteration: 3124/10000 --- Training Loss:0.000490\n",
      "Epoch: 63/200, Iteration: 3126/10000 --- Training Loss:0.000379\n",
      "Epoch: 63/200, Iteration: 3128/10000 --- Training Loss:0.000560\n",
      "Epoch: 63/200, Iteration: 3130/10000 --- Training Loss:0.001399\n",
      "Epoch: 63/200, Iteration: 3132/10000 --- Training Loss:0.001124\n",
      "Epoch: 63/200, Iteration: 3134/10000 --- Training Loss:0.000957\n",
      "Epoch: 63/200, Iteration: 3136/10000 --- Training Loss:0.001037\n",
      "Epoch: 63/200, Iteration: 3138/10000 --- Training Loss:0.000948\n",
      "Epoch: 63/200, Iteration: 3140/10000 --- Training Loss:0.001065\n",
      "Epoch: 63/200, Iteration: 3142/10000 --- Training Loss:0.000689\n",
      "Epoch: 63/200, Iteration: 3144/10000 --- Training Loss:0.000989\n",
      "Epoch: 63/200, Iteration: 3146/10000 --- Training Loss:0.000811\n",
      "Epoch: 63/200, Iteration: 3148/10000 --- Training Loss:0.001016\n",
      "Epoch: 63/200, Iteration: 3150/10000 --- Training Loss:0.000640\n",
      "Epoch: 63 finished ! Train Loss: 0.00109, Test Loss: 0.00414\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 64/200, Iteration: 3152/10000 --- Training Loss:0.000545\n",
      "Epoch: 64/200, Iteration: 3154/10000 --- Training Loss:0.000664\n",
      "Epoch: 64/200, Iteration: 3156/10000 --- Training Loss:0.000681\n",
      "Epoch: 64/200, Iteration: 3158/10000 --- Training Loss:0.000452\n",
      "Epoch: 64/200, Iteration: 3160/10000 --- Training Loss:0.000576\n",
      "Epoch: 64/200, Iteration: 3162/10000 --- Training Loss:0.000296\n",
      "Epoch: 64/200, Iteration: 3164/10000 --- Training Loss:0.000421\n",
      "Epoch: 64/200, Iteration: 3166/10000 --- Training Loss:0.000498\n",
      "Epoch: 64/200, Iteration: 3168/10000 --- Training Loss:0.000621\n",
      "Epoch: 64/200, Iteration: 3170/10000 --- Training Loss:0.000721\n",
      "Epoch: 64/200, Iteration: 3172/10000 --- Training Loss:0.000720\n",
      "Epoch: 64/200, Iteration: 3174/10000 --- Training Loss:0.000365\n",
      "Epoch: 64/200, Iteration: 3176/10000 --- Training Loss:0.002327\n",
      "Epoch: 64/200, Iteration: 3178/10000 --- Training Loss:0.000705\n",
      "Epoch: 64/200, Iteration: 3180/10000 --- Training Loss:0.001680\n",
      "Epoch: 64/200, Iteration: 3182/10000 --- Training Loss:0.003648\n",
      "Epoch: 64/200, Iteration: 3184/10000 --- Training Loss:0.006771\n",
      "Epoch: 64/200, Iteration: 3186/10000 --- Training Loss:0.001779\n",
      "Epoch: 64/200, Iteration: 3188/10000 --- Training Loss:0.003947\n",
      "Epoch: 64/200, Iteration: 3190/10000 --- Training Loss:0.003359\n",
      "Epoch: 64/200, Iteration: 3192/10000 --- Training Loss:0.002247\n",
      "Epoch: 64/200, Iteration: 3194/10000 --- Training Loss:0.003111\n",
      "Epoch: 64/200, Iteration: 3196/10000 --- Training Loss:0.004457\n",
      "Epoch: 64/200, Iteration: 3198/10000 --- Training Loss:0.001849\n",
      "Epoch: 64/200, Iteration: 3200/10000 --- Training Loss:0.002674\n",
      "Epoch: 64 finished ! Train Loss: 0.00268, Test Loss: 0.03393\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 65/200, Iteration: 3202/10000 --- Training Loss:0.002009\n",
      "Epoch: 65/200, Iteration: 3204/10000 --- Training Loss:0.003727\n",
      "Epoch: 65/200, Iteration: 3206/10000 --- Training Loss:0.001081\n",
      "Epoch: 65/200, Iteration: 3208/10000 --- Training Loss:0.002722\n",
      "Epoch: 65/200, Iteration: 3210/10000 --- Training Loss:0.001703\n",
      "Epoch: 65/200, Iteration: 3212/10000 --- Training Loss:0.003047\n",
      "Epoch: 65/200, Iteration: 3214/10000 --- Training Loss:0.000943\n",
      "Epoch: 65/200, Iteration: 3216/10000 --- Training Loss:0.001046\n",
      "Epoch: 65/200, Iteration: 3218/10000 --- Training Loss:0.001889\n",
      "Epoch: 65/200, Iteration: 3220/10000 --- Training Loss:0.001202\n",
      "Epoch: 65/200, Iteration: 3222/10000 --- Training Loss:0.000949\n",
      "Epoch: 65/200, Iteration: 3224/10000 --- Training Loss:0.001006\n",
      "Epoch: 65/200, Iteration: 3226/10000 --- Training Loss:0.001144\n",
      "Epoch: 65/200, Iteration: 3228/10000 --- Training Loss:0.000786\n",
      "Epoch: 65/200, Iteration: 3230/10000 --- Training Loss:0.001346\n",
      "Epoch: 65/200, Iteration: 3232/10000 --- Training Loss:0.000854\n",
      "Epoch: 65/200, Iteration: 3234/10000 --- Training Loss:0.000824\n",
      "Epoch: 65/200, Iteration: 3236/10000 --- Training Loss:0.001090\n",
      "Epoch: 65/200, Iteration: 3238/10000 --- Training Loss:0.000920\n",
      "Epoch: 65/200, Iteration: 3240/10000 --- Training Loss:0.000879\n",
      "Epoch: 65/200, Iteration: 3242/10000 --- Training Loss:0.001046\n",
      "Epoch: 65/200, Iteration: 3244/10000 --- Training Loss:0.000681\n",
      "Epoch: 65/200, Iteration: 3246/10000 --- Training Loss:0.001153\n",
      "Epoch: 65/200, Iteration: 3248/10000 --- Training Loss:0.000649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65/200, Iteration: 3250/10000 --- Training Loss:0.000758\n",
      "Epoch: 65 finished ! Train Loss: 0.00124, Test Loss: 0.00167\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 66/200, Iteration: 3252/10000 --- Training Loss:0.000494\n",
      "Epoch: 66/200, Iteration: 3254/10000 --- Training Loss:0.000425\n",
      "Epoch: 66/200, Iteration: 3256/10000 --- Training Loss:0.000557\n",
      "Epoch: 66/200, Iteration: 3258/10000 --- Training Loss:0.000460\n",
      "Epoch: 66/200, Iteration: 3260/10000 --- Training Loss:0.000643\n",
      "Epoch: 66/200, Iteration: 3262/10000 --- Training Loss:0.001376\n",
      "Epoch: 66/200, Iteration: 3264/10000 --- Training Loss:0.001482\n",
      "Epoch: 66/200, Iteration: 3266/10000 --- Training Loss:0.001064\n",
      "Epoch: 66/200, Iteration: 3268/10000 --- Training Loss:0.000627\n",
      "Epoch: 66/200, Iteration: 3270/10000 --- Training Loss:0.000683\n",
      "Epoch: 66/200, Iteration: 3272/10000 --- Training Loss:0.000701\n",
      "Epoch: 66/200, Iteration: 3274/10000 --- Training Loss:0.000673\n",
      "Epoch: 66/200, Iteration: 3276/10000 --- Training Loss:0.000635\n",
      "Epoch: 66/200, Iteration: 3278/10000 --- Training Loss:0.000689\n",
      "Epoch: 66/200, Iteration: 3280/10000 --- Training Loss:0.000573\n",
      "Epoch: 66/200, Iteration: 3282/10000 --- Training Loss:0.000476\n",
      "Epoch: 66/200, Iteration: 3284/10000 --- Training Loss:0.000331\n",
      "Epoch: 66/200, Iteration: 3286/10000 --- Training Loss:0.000520\n",
      "Epoch: 66/200, Iteration: 3288/10000 --- Training Loss:0.000380\n",
      "Epoch: 66/200, Iteration: 3290/10000 --- Training Loss:0.000434\n",
      "Epoch: 66/200, Iteration: 3292/10000 --- Training Loss:0.000636\n",
      "Epoch: 66/200, Iteration: 3294/10000 --- Training Loss:0.001093\n",
      "Epoch: 66/200, Iteration: 3296/10000 --- Training Loss:0.000371\n",
      "Epoch: 66/200, Iteration: 3298/10000 --- Training Loss:0.000324\n",
      "Epoch: 66/200, Iteration: 3300/10000 --- Training Loss:0.001345\n",
      "Epoch: 66 finished ! Train Loss: 0.00069, Test Loss: 0.00169\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 67/200, Iteration: 3302/10000 --- Training Loss:0.000388\n",
      "Epoch: 67/200, Iteration: 3304/10000 --- Training Loss:0.000355\n",
      "Epoch: 67/200, Iteration: 3306/10000 --- Training Loss:0.000475\n",
      "Epoch: 67/200, Iteration: 3308/10000 --- Training Loss:0.000520\n",
      "Epoch: 67/200, Iteration: 3310/10000 --- Training Loss:0.000717\n",
      "Epoch: 67/200, Iteration: 3312/10000 --- Training Loss:0.000645\n",
      "Epoch: 67/200, Iteration: 3314/10000 --- Training Loss:0.000410\n",
      "Epoch: 67/200, Iteration: 3316/10000 --- Training Loss:0.000465\n",
      "Epoch: 67/200, Iteration: 3318/10000 --- Training Loss:0.000636\n",
      "Epoch: 67/200, Iteration: 3320/10000 --- Training Loss:0.000756\n",
      "Epoch: 67/200, Iteration: 3322/10000 --- Training Loss:0.000399\n",
      "Epoch: 67/200, Iteration: 3324/10000 --- Training Loss:0.000287\n",
      "Epoch: 67/200, Iteration: 3326/10000 --- Training Loss:0.000267\n",
      "Epoch: 67/200, Iteration: 3328/10000 --- Training Loss:0.000590\n",
      "Epoch: 67/200, Iteration: 3330/10000 --- Training Loss:0.000480\n",
      "Epoch: 67/200, Iteration: 3332/10000 --- Training Loss:0.000350\n",
      "Epoch: 67/200, Iteration: 3334/10000 --- Training Loss:0.000706\n",
      "Epoch: 67/200, Iteration: 3336/10000 --- Training Loss:0.000354\n",
      "Epoch: 67/200, Iteration: 3338/10000 --- Training Loss:0.000463\n",
      "Epoch: 67/200, Iteration: 3340/10000 --- Training Loss:0.000335\n",
      "Epoch: 67/200, Iteration: 3342/10000 --- Training Loss:0.000404\n",
      "Epoch: 67/200, Iteration: 3344/10000 --- Training Loss:0.000342\n",
      "Epoch: 67/200, Iteration: 3346/10000 --- Training Loss:0.000786\n",
      "Epoch: 67/200, Iteration: 3348/10000 --- Training Loss:0.000935\n",
      "Epoch: 67/200, Iteration: 3350/10000 --- Training Loss:0.000728\n",
      "Epoch: 67 finished ! Train Loss: 0.00053, Test Loss: 0.00207\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 68/200, Iteration: 3352/10000 --- Training Loss:0.000481\n",
      "Epoch: 68/200, Iteration: 3354/10000 --- Training Loss:0.000358\n",
      "Epoch: 68/200, Iteration: 3356/10000 --- Training Loss:0.000325\n",
      "Epoch: 68/200, Iteration: 3358/10000 --- Training Loss:0.000334\n",
      "Epoch: 68/200, Iteration: 3360/10000 --- Training Loss:0.000450\n",
      "Epoch: 68/200, Iteration: 3362/10000 --- Training Loss:0.000361\n",
      "Epoch: 68/200, Iteration: 3364/10000 --- Training Loss:0.000469\n",
      "Epoch: 68/200, Iteration: 3366/10000 --- Training Loss:0.000714\n",
      "Epoch: 68/200, Iteration: 3368/10000 --- Training Loss:0.000661\n",
      "Epoch: 68/200, Iteration: 3370/10000 --- Training Loss:0.000222\n",
      "Epoch: 68/200, Iteration: 3372/10000 --- Training Loss:0.000352\n",
      "Epoch: 68/200, Iteration: 3374/10000 --- Training Loss:0.000607\n",
      "Epoch: 68/200, Iteration: 3376/10000 --- Training Loss:0.000801\n",
      "Epoch: 68/200, Iteration: 3378/10000 --- Training Loss:0.000287\n",
      "Epoch: 68/200, Iteration: 3380/10000 --- Training Loss:0.000494\n",
      "Epoch: 68/200, Iteration: 3382/10000 --- Training Loss:0.000314\n",
      "Epoch: 68/200, Iteration: 3384/10000 --- Training Loss:0.000243\n",
      "Epoch: 68/200, Iteration: 3386/10000 --- Training Loss:0.000385\n",
      "Epoch: 68/200, Iteration: 3388/10000 --- Training Loss:0.000815\n",
      "Epoch: 68/200, Iteration: 3390/10000 --- Training Loss:0.000495\n",
      "Epoch: 68/200, Iteration: 3392/10000 --- Training Loss:0.000536\n",
      "Epoch: 68/200, Iteration: 3394/10000 --- Training Loss:0.000685\n",
      "Epoch: 68/200, Iteration: 3396/10000 --- Training Loss:0.000428\n",
      "Epoch: 68/200, Iteration: 3398/10000 --- Training Loss:0.000715\n",
      "Epoch: 68/200, Iteration: 3400/10000 --- Training Loss:0.000267\n",
      "Epoch: 68 finished ! Train Loss: 0.00053, Test Loss: 0.00202\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 69/200, Iteration: 3402/10000 --- Training Loss:0.000388\n",
      "Epoch: 69/200, Iteration: 3404/10000 --- Training Loss:0.000561\n",
      "Epoch: 69/200, Iteration: 3406/10000 --- Training Loss:0.000452\n",
      "Epoch: 69/200, Iteration: 3408/10000 --- Training Loss:0.000571\n",
      "Epoch: 69/200, Iteration: 3410/10000 --- Training Loss:0.000412\n",
      "Epoch: 69/200, Iteration: 3412/10000 --- Training Loss:0.000315\n",
      "Epoch: 69/200, Iteration: 3414/10000 --- Training Loss:0.000326\n",
      "Epoch: 69/200, Iteration: 3416/10000 --- Training Loss:0.000646\n",
      "Epoch: 69/200, Iteration: 3418/10000 --- Training Loss:0.000425\n",
      "Epoch: 69/200, Iteration: 3420/10000 --- Training Loss:0.000257\n",
      "Epoch: 69/200, Iteration: 3422/10000 --- Training Loss:0.000415\n",
      "Epoch: 69/200, Iteration: 3424/10000 --- Training Loss:0.000376\n",
      "Epoch: 69/200, Iteration: 3426/10000 --- Training Loss:0.000489\n",
      "Epoch: 69/200, Iteration: 3428/10000 --- Training Loss:0.000641\n",
      "Epoch: 69/200, Iteration: 3430/10000 --- Training Loss:0.000635\n",
      "Epoch: 69/200, Iteration: 3432/10000 --- Training Loss:0.000339\n",
      "Epoch: 69/200, Iteration: 3434/10000 --- Training Loss:0.000474\n",
      "Epoch: 69/200, Iteration: 3436/10000 --- Training Loss:0.000613\n",
      "Epoch: 69/200, Iteration: 3438/10000 --- Training Loss:0.001010\n",
      "Epoch: 69/200, Iteration: 3440/10000 --- Training Loss:0.000281\n",
      "Epoch: 69/200, Iteration: 3442/10000 --- Training Loss:0.000581\n",
      "Epoch: 69/200, Iteration: 3444/10000 --- Training Loss:0.000456\n",
      "Epoch: 69/200, Iteration: 3446/10000 --- Training Loss:0.000359\n",
      "Epoch: 69/200, Iteration: 3448/10000 --- Training Loss:0.000402\n",
      "Epoch: 69/200, Iteration: 3450/10000 --- Training Loss:0.000428\n",
      "Epoch: 69 finished ! Train Loss: 0.00054, Test Loss: 0.00234\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 70/200, Iteration: 3452/10000 --- Training Loss:0.000343\n",
      "Epoch: 70/200, Iteration: 3454/10000 --- Training Loss:0.000353\n",
      "Epoch: 70/200, Iteration: 3456/10000 --- Training Loss:0.000393\n",
      "Epoch: 70/200, Iteration: 3458/10000 --- Training Loss:0.000321\n",
      "Epoch: 70/200, Iteration: 3460/10000 --- Training Loss:0.000333\n",
      "Epoch: 70/200, Iteration: 3462/10000 --- Training Loss:0.000570\n",
      "Epoch: 70/200, Iteration: 3464/10000 --- Training Loss:0.000325\n",
      "Epoch: 70/200, Iteration: 3466/10000 --- Training Loss:0.000505\n",
      "Epoch: 70/200, Iteration: 3468/10000 --- Training Loss:0.000405\n",
      "Epoch: 70/200, Iteration: 3470/10000 --- Training Loss:0.000852\n",
      "Epoch: 70/200, Iteration: 3472/10000 --- Training Loss:0.000493\n",
      "Epoch: 70/200, Iteration: 3474/10000 --- Training Loss:0.000579\n",
      "Epoch: 70/200, Iteration: 3476/10000 --- Training Loss:0.000308\n",
      "Epoch: 70/200, Iteration: 3478/10000 --- Training Loss:0.000382\n",
      "Epoch: 70/200, Iteration: 3480/10000 --- Training Loss:0.001133\n",
      "Epoch: 70/200, Iteration: 3482/10000 --- Training Loss:0.000521\n",
      "Epoch: 70/200, Iteration: 3484/10000 --- Training Loss:0.000616\n",
      "Epoch: 70/200, Iteration: 3486/10000 --- Training Loss:0.000369\n",
      "Epoch: 70/200, Iteration: 3488/10000 --- Training Loss:0.000634\n",
      "Epoch: 70/200, Iteration: 3490/10000 --- Training Loss:0.000327\n",
      "Epoch: 70/200, Iteration: 3492/10000 --- Training Loss:0.000355\n",
      "Epoch: 70/200, Iteration: 3494/10000 --- Training Loss:0.000583\n",
      "Epoch: 70/200, Iteration: 3496/10000 --- Training Loss:0.000511\n",
      "Epoch: 70/200, Iteration: 3498/10000 --- Training Loss:0.000420\n",
      "Epoch: 70/200, Iteration: 3500/10000 --- Training Loss:0.000579\n",
      "Epoch: 70 finished ! Train Loss: 0.00048, Test Loss: 0.00173\n",
      "Epoch consuming time: 0m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71/200, Iteration: 3502/10000 --- Training Loss:0.000342\n",
      "Epoch: 71/200, Iteration: 3504/10000 --- Training Loss:0.000327\n",
      "Epoch: 71/200, Iteration: 3506/10000 --- Training Loss:0.000171\n",
      "Epoch: 71/200, Iteration: 3508/10000 --- Training Loss:0.000959\n",
      "Epoch: 71/200, Iteration: 3510/10000 --- Training Loss:0.000383\n",
      "Epoch: 71/200, Iteration: 3512/10000 --- Training Loss:0.000483\n",
      "Epoch: 71/200, Iteration: 3514/10000 --- Training Loss:0.000581\n",
      "Epoch: 71/200, Iteration: 3516/10000 --- Training Loss:0.000362\n",
      "Epoch: 71/200, Iteration: 3518/10000 --- Training Loss:0.000630\n",
      "Epoch: 71/200, Iteration: 3520/10000 --- Training Loss:0.000476\n",
      "Epoch: 71/200, Iteration: 3522/10000 --- Training Loss:0.000421\n",
      "Epoch: 71/200, Iteration: 3524/10000 --- Training Loss:0.000398\n",
      "Epoch: 71/200, Iteration: 3526/10000 --- Training Loss:0.000487\n",
      "Epoch: 71/200, Iteration: 3528/10000 --- Training Loss:0.000536\n",
      "Epoch: 71/200, Iteration: 3530/10000 --- Training Loss:0.000630\n",
      "Epoch: 71/200, Iteration: 3532/10000 --- Training Loss:0.000456\n",
      "Epoch: 71/200, Iteration: 3534/10000 --- Training Loss:0.000623\n",
      "Epoch: 71/200, Iteration: 3536/10000 --- Training Loss:0.000387\n",
      "Epoch: 71/200, Iteration: 3538/10000 --- Training Loss:0.000678\n",
      "Epoch: 71/200, Iteration: 3540/10000 --- Training Loss:0.000350\n",
      "Epoch: 71/200, Iteration: 3542/10000 --- Training Loss:0.000677\n",
      "Epoch: 71/200, Iteration: 3544/10000 --- Training Loss:0.000975\n",
      "Epoch: 71/200, Iteration: 3546/10000 --- Training Loss:0.000616\n",
      "Epoch: 71/200, Iteration: 3548/10000 --- Training Loss:0.000415\n",
      "Epoch: 71/200, Iteration: 3550/10000 --- Training Loss:0.000395\n",
      "Epoch: 71 finished ! Train Loss: 0.00053, Test Loss: 0.00177\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 72/200, Iteration: 3552/10000 --- Training Loss:0.000930\n",
      "Epoch: 72/200, Iteration: 3554/10000 --- Training Loss:0.000691\n",
      "Epoch: 72/200, Iteration: 3556/10000 --- Training Loss:0.000428\n",
      "Epoch: 72/200, Iteration: 3558/10000 --- Training Loss:0.000703\n",
      "Epoch: 72/200, Iteration: 3560/10000 --- Training Loss:0.000385\n",
      "Epoch: 72/200, Iteration: 3562/10000 --- Training Loss:0.000476\n",
      "Epoch: 72/200, Iteration: 3564/10000 --- Training Loss:0.000507\n",
      "Epoch: 72/200, Iteration: 3566/10000 --- Training Loss:0.000555\n",
      "Epoch: 72/200, Iteration: 3568/10000 --- Training Loss:0.000965\n",
      "Epoch: 72/200, Iteration: 3570/10000 --- Training Loss:0.000585\n",
      "Epoch: 72/200, Iteration: 3572/10000 --- Training Loss:0.000379\n",
      "Epoch: 72/200, Iteration: 3574/10000 --- Training Loss:0.000313\n",
      "Epoch: 72/200, Iteration: 3576/10000 --- Training Loss:0.000227\n",
      "Epoch: 72/200, Iteration: 3578/10000 --- Training Loss:0.000571\n",
      "Epoch: 72/200, Iteration: 3580/10000 --- Training Loss:0.000265\n",
      "Epoch: 72/200, Iteration: 3582/10000 --- Training Loss:0.000385\n",
      "Epoch: 72/200, Iteration: 3584/10000 --- Training Loss:0.000296\n",
      "Epoch: 72/200, Iteration: 3586/10000 --- Training Loss:0.000353\n",
      "Epoch: 72/200, Iteration: 3588/10000 --- Training Loss:0.000467\n",
      "Epoch: 72/200, Iteration: 3590/10000 --- Training Loss:0.000679\n",
      "Epoch: 72/200, Iteration: 3592/10000 --- Training Loss:0.000351\n",
      "Epoch: 72/200, Iteration: 3594/10000 --- Training Loss:0.000384\n",
      "Epoch: 72/200, Iteration: 3596/10000 --- Training Loss:0.000389\n",
      "Epoch: 72/200, Iteration: 3598/10000 --- Training Loss:0.000535\n",
      "Epoch: 72/200, Iteration: 3600/10000 --- Training Loss:0.000310\n",
      "Epoch: 72 finished ! Train Loss: 0.00051, Test Loss: 0.00160\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 36 percent completed\n",
      "Epoch: 73/200, Iteration: 3602/10000 --- Training Loss:0.000395\n",
      "Epoch: 73/200, Iteration: 3604/10000 --- Training Loss:0.000237\n",
      "Epoch: 73/200, Iteration: 3606/10000 --- Training Loss:0.000554\n",
      "Epoch: 73/200, Iteration: 3608/10000 --- Training Loss:0.000434\n",
      "Epoch: 73/200, Iteration: 3610/10000 --- Training Loss:0.000286\n",
      "Epoch: 73/200, Iteration: 3612/10000 --- Training Loss:0.000384\n",
      "Epoch: 73/200, Iteration: 3614/10000 --- Training Loss:0.000353\n",
      "Epoch: 73/200, Iteration: 3616/10000 --- Training Loss:0.000456\n",
      "Epoch: 73/200, Iteration: 3618/10000 --- Training Loss:0.000368\n",
      "Epoch: 73/200, Iteration: 3620/10000 --- Training Loss:0.000412\n",
      "Epoch: 73/200, Iteration: 3622/10000 --- Training Loss:0.000224\n",
      "Epoch: 73/200, Iteration: 3624/10000 --- Training Loss:0.000552\n",
      "Epoch: 73/200, Iteration: 3626/10000 --- Training Loss:0.000415\n",
      "Epoch: 73/200, Iteration: 3628/10000 --- Training Loss:0.000511\n",
      "Epoch: 73/200, Iteration: 3630/10000 --- Training Loss:0.000326\n",
      "Epoch: 73/200, Iteration: 3632/10000 --- Training Loss:0.000559\n",
      "Epoch: 73/200, Iteration: 3634/10000 --- Training Loss:0.000231\n",
      "Epoch: 73/200, Iteration: 3636/10000 --- Training Loss:0.000443\n",
      "Epoch: 73/200, Iteration: 3638/10000 --- Training Loss:0.000521\n",
      "Epoch: 73/200, Iteration: 3640/10000 --- Training Loss:0.001421\n",
      "Epoch: 73/200, Iteration: 3642/10000 --- Training Loss:0.000852\n",
      "Epoch: 73/200, Iteration: 3644/10000 --- Training Loss:0.001021\n",
      "Epoch: 73/200, Iteration: 3646/10000 --- Training Loss:0.000555\n",
      "Epoch: 73/200, Iteration: 3648/10000 --- Training Loss:0.000313\n",
      "Epoch: 73/200, Iteration: 3650/10000 --- Training Loss:0.000229\n",
      "Epoch: 73 finished ! Train Loss: 0.00048, Test Loss: 0.00167\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 74/200, Iteration: 3652/10000 --- Training Loss:0.000695\n",
      "Epoch: 74/200, Iteration: 3654/10000 --- Training Loss:0.000294\n",
      "Epoch: 74/200, Iteration: 3656/10000 --- Training Loss:0.000248\n",
      "Epoch: 74/200, Iteration: 3658/10000 --- Training Loss:0.000302\n",
      "Epoch: 74/200, Iteration: 3660/10000 --- Training Loss:0.000456\n",
      "Epoch: 74/200, Iteration: 3662/10000 --- Training Loss:0.000313\n",
      "Epoch: 74/200, Iteration: 3664/10000 --- Training Loss:0.000346\n",
      "Epoch: 74/200, Iteration: 3666/10000 --- Training Loss:0.000403\n",
      "Epoch: 74/200, Iteration: 3668/10000 --- Training Loss:0.000290\n",
      "Epoch: 74/200, Iteration: 3670/10000 --- Training Loss:0.000292\n",
      "Epoch: 74/200, Iteration: 3672/10000 --- Training Loss:0.000370\n",
      "Epoch: 74/200, Iteration: 3674/10000 --- Training Loss:0.000259\n",
      "Epoch: 74/200, Iteration: 3676/10000 --- Training Loss:0.000241\n",
      "Epoch: 74/200, Iteration: 3678/10000 --- Training Loss:0.000486\n",
      "Epoch: 74/200, Iteration: 3680/10000 --- Training Loss:0.000205\n",
      "Epoch: 74/200, Iteration: 3682/10000 --- Training Loss:0.000583\n",
      "Epoch: 74/200, Iteration: 3684/10000 --- Training Loss:0.000717\n",
      "Epoch: 74/200, Iteration: 3686/10000 --- Training Loss:0.001711\n",
      "Epoch: 74/200, Iteration: 3688/10000 --- Training Loss:0.000363\n",
      "Epoch: 74/200, Iteration: 3690/10000 --- Training Loss:0.001205\n",
      "Epoch: 74/200, Iteration: 3692/10000 --- Training Loss:0.000550\n",
      "Epoch: 74/200, Iteration: 3694/10000 --- Training Loss:0.000602\n",
      "Epoch: 74/200, Iteration: 3696/10000 --- Training Loss:0.000976\n",
      "Epoch: 74/200, Iteration: 3698/10000 --- Training Loss:0.000950\n",
      "Epoch: 74/200, Iteration: 3700/10000 --- Training Loss:0.000446\n",
      "Epoch: 74 finished ! Train Loss: 0.00052, Test Loss: 0.00270\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 75/200, Iteration: 3702/10000 --- Training Loss:0.000484\n",
      "Epoch: 75/200, Iteration: 3704/10000 --- Training Loss:0.000551\n",
      "Epoch: 75/200, Iteration: 3706/10000 --- Training Loss:0.000399\n",
      "Epoch: 75/200, Iteration: 3708/10000 --- Training Loss:0.000579\n",
      "Epoch: 75/200, Iteration: 3710/10000 --- Training Loss:0.000403\n",
      "Epoch: 75/200, Iteration: 3712/10000 --- Training Loss:0.000349\n",
      "Epoch: 75/200, Iteration: 3714/10000 --- Training Loss:0.000267\n",
      "Epoch: 75/200, Iteration: 3716/10000 --- Training Loss:0.000334\n",
      "Epoch: 75/200, Iteration: 3718/10000 --- Training Loss:0.000435\n",
      "Epoch: 75/200, Iteration: 3720/10000 --- Training Loss:0.000288\n",
      "Epoch: 75/200, Iteration: 3722/10000 --- Training Loss:0.000306\n",
      "Epoch: 75/200, Iteration: 3724/10000 --- Training Loss:0.000258\n",
      "Epoch: 75/200, Iteration: 3726/10000 --- Training Loss:0.000437\n",
      "Epoch: 75/200, Iteration: 3728/10000 --- Training Loss:0.000284\n",
      "Epoch: 75/200, Iteration: 3730/10000 --- Training Loss:0.000426\n",
      "Epoch: 75/200, Iteration: 3732/10000 --- Training Loss:0.000243\n",
      "Epoch: 75/200, Iteration: 3734/10000 --- Training Loss:0.000292\n",
      "Epoch: 75/200, Iteration: 3736/10000 --- Training Loss:0.000247\n",
      "Epoch: 75/200, Iteration: 3738/10000 --- Training Loss:0.000300\n",
      "Epoch: 75/200, Iteration: 3740/10000 --- Training Loss:0.000593\n",
      "Epoch: 75/200, Iteration: 3742/10000 --- Training Loss:0.000515\n",
      "Epoch: 75/200, Iteration: 3744/10000 --- Training Loss:0.000334\n",
      "Epoch: 75/200, Iteration: 3746/10000 --- Training Loss:0.000466\n",
      "Epoch: 75/200, Iteration: 3748/10000 --- Training Loss:0.000302\n",
      "Epoch: 75/200, Iteration: 3750/10000 --- Training Loss:0.001047\n",
      "Epoch: 75 finished ! Train Loss: 0.00041, Test Loss: 0.00140\n",
      "Epoch consuming time: 0m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved: 37 percent completed\n",
      "Epoch: 76/200, Iteration: 3752/10000 --- Training Loss:0.000284\n",
      "Epoch: 76/200, Iteration: 3754/10000 --- Training Loss:0.000395\n",
      "Epoch: 76/200, Iteration: 3756/10000 --- Training Loss:0.000468\n",
      "Epoch: 76/200, Iteration: 3758/10000 --- Training Loss:0.000703\n",
      "Epoch: 76/200, Iteration: 3760/10000 --- Training Loss:0.000376\n",
      "Epoch: 76/200, Iteration: 3762/10000 --- Training Loss:0.000372\n",
      "Epoch: 76/200, Iteration: 3764/10000 --- Training Loss:0.000230\n",
      "Epoch: 76/200, Iteration: 3766/10000 --- Training Loss:0.000355\n",
      "Epoch: 76/200, Iteration: 3768/10000 --- Training Loss:0.000356\n",
      "Epoch: 76/200, Iteration: 3770/10000 --- Training Loss:0.000817\n",
      "Epoch: 76/200, Iteration: 3772/10000 --- Training Loss:0.000539\n",
      "Epoch: 76/200, Iteration: 3774/10000 --- Training Loss:0.000243\n",
      "Epoch: 76/200, Iteration: 3776/10000 --- Training Loss:0.000233\n",
      "Epoch: 76/200, Iteration: 3778/10000 --- Training Loss:0.000305\n",
      "Epoch: 76/200, Iteration: 3780/10000 --- Training Loss:0.000480\n",
      "Epoch: 76/200, Iteration: 3782/10000 --- Training Loss:0.000428\n",
      "Epoch: 76/200, Iteration: 3784/10000 --- Training Loss:0.000558\n",
      "Epoch: 76/200, Iteration: 3786/10000 --- Training Loss:0.000420\n",
      "Epoch: 76/200, Iteration: 3788/10000 --- Training Loss:0.000233\n",
      "Epoch: 76/200, Iteration: 3790/10000 --- Training Loss:0.000380\n",
      "Epoch: 76/200, Iteration: 3792/10000 --- Training Loss:0.000243\n",
      "Epoch: 76/200, Iteration: 3794/10000 --- Training Loss:0.000785\n",
      "Epoch: 76/200, Iteration: 3796/10000 --- Training Loss:0.000273\n",
      "Epoch: 76/200, Iteration: 3798/10000 --- Training Loss:0.000274\n",
      "Epoch: 76/200, Iteration: 3800/10000 --- Training Loss:0.000348\n",
      "Epoch: 76 finished ! Train Loss: 0.00044, Test Loss: 0.00150\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 77/200, Iteration: 3802/10000 --- Training Loss:0.000493\n",
      "Epoch: 77/200, Iteration: 3804/10000 --- Training Loss:0.000236\n",
      "Epoch: 77/200, Iteration: 3806/10000 --- Training Loss:0.000426\n",
      "Epoch: 77/200, Iteration: 3808/10000 --- Training Loss:0.000257\n",
      "Epoch: 77/200, Iteration: 3810/10000 --- Training Loss:0.000323\n",
      "Epoch: 77/200, Iteration: 3812/10000 --- Training Loss:0.000957\n",
      "Epoch: 77/200, Iteration: 3814/10000 --- Training Loss:0.000354\n",
      "Epoch: 77/200, Iteration: 3816/10000 --- Training Loss:0.000385\n",
      "Epoch: 77/200, Iteration: 3818/10000 --- Training Loss:0.000489\n",
      "Epoch: 77/200, Iteration: 3820/10000 --- Training Loss:0.000456\n",
      "Epoch: 77/200, Iteration: 3822/10000 --- Training Loss:0.000270\n",
      "Epoch: 77/200, Iteration: 3824/10000 --- Training Loss:0.000363\n",
      "Epoch: 77/200, Iteration: 3826/10000 --- Training Loss:0.000317\n",
      "Epoch: 77/200, Iteration: 3828/10000 --- Training Loss:0.000321\n",
      "Epoch: 77/200, Iteration: 3830/10000 --- Training Loss:0.000323\n",
      "Epoch: 77/200, Iteration: 3832/10000 --- Training Loss:0.000487\n",
      "Epoch: 77/200, Iteration: 3834/10000 --- Training Loss:0.000410\n",
      "Epoch: 77/200, Iteration: 3836/10000 --- Training Loss:0.000278\n",
      "Epoch: 77/200, Iteration: 3838/10000 --- Training Loss:0.000486\n",
      "Epoch: 77/200, Iteration: 3840/10000 --- Training Loss:0.000394\n",
      "Epoch: 77/200, Iteration: 3842/10000 --- Training Loss:0.000245\n",
      "Epoch: 77/200, Iteration: 3844/10000 --- Training Loss:0.000436\n",
      "Epoch: 77/200, Iteration: 3846/10000 --- Training Loss:0.000267\n",
      "Epoch: 77/200, Iteration: 3848/10000 --- Training Loss:0.000225\n",
      "Epoch: 77/200, Iteration: 3850/10000 --- Training Loss:0.000514\n",
      "Epoch: 77 finished ! Train Loss: 0.00038, Test Loss: 0.00201\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 78/200, Iteration: 3852/10000 --- Training Loss:0.000399\n",
      "Epoch: 78/200, Iteration: 3854/10000 --- Training Loss:0.000229\n",
      "Epoch: 78/200, Iteration: 3856/10000 --- Training Loss:0.000270\n",
      "Epoch: 78/200, Iteration: 3858/10000 --- Training Loss:0.000293\n",
      "Epoch: 78/200, Iteration: 3860/10000 --- Training Loss:0.000244\n",
      "Epoch: 78/200, Iteration: 3862/10000 --- Training Loss:0.001038\n",
      "Epoch: 78/200, Iteration: 3864/10000 --- Training Loss:0.000377\n",
      "Epoch: 78/200, Iteration: 3866/10000 --- Training Loss:0.000370\n",
      "Epoch: 78/200, Iteration: 3868/10000 --- Training Loss:0.000254\n",
      "Epoch: 78/200, Iteration: 3870/10000 --- Training Loss:0.000163\n",
      "Epoch: 78/200, Iteration: 3872/10000 --- Training Loss:0.000442\n",
      "Epoch: 78/200, Iteration: 3874/10000 --- Training Loss:0.000418\n",
      "Epoch: 78/200, Iteration: 3876/10000 --- Training Loss:0.000233\n",
      "Epoch: 78/200, Iteration: 3878/10000 --- Training Loss:0.000466\n",
      "Epoch: 78/200, Iteration: 3880/10000 --- Training Loss:0.000167\n",
      "Epoch: 78/200, Iteration: 3882/10000 --- Training Loss:0.000311\n",
      "Epoch: 78/200, Iteration: 3884/10000 --- Training Loss:0.000427\n",
      "Epoch: 78/200, Iteration: 3886/10000 --- Training Loss:0.000233\n",
      "Epoch: 78/200, Iteration: 3888/10000 --- Training Loss:0.000334\n",
      "Epoch: 78/200, Iteration: 3890/10000 --- Training Loss:0.000307\n",
      "Epoch: 78/200, Iteration: 3892/10000 --- Training Loss:0.000546\n",
      "Epoch: 78/200, Iteration: 3894/10000 --- Training Loss:0.001021\n",
      "Epoch: 78/200, Iteration: 3896/10000 --- Training Loss:0.000349\n",
      "Epoch: 78/200, Iteration: 3898/10000 --- Training Loss:0.000396\n",
      "Epoch: 78/200, Iteration: 3900/10000 --- Training Loss:0.000527\n",
      "Epoch: 78 finished ! Train Loss: 0.00038, Test Loss: 0.00157\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 79/200, Iteration: 3902/10000 --- Training Loss:0.000481\n",
      "Epoch: 79/200, Iteration: 3904/10000 --- Training Loss:0.000401\n",
      "Epoch: 79/200, Iteration: 3906/10000 --- Training Loss:0.000247\n",
      "Epoch: 79/200, Iteration: 3908/10000 --- Training Loss:0.000223\n",
      "Epoch: 79/200, Iteration: 3910/10000 --- Training Loss:0.000720\n",
      "Epoch: 79/200, Iteration: 3912/10000 --- Training Loss:0.000292\n",
      "Epoch: 79/200, Iteration: 3914/10000 --- Training Loss:0.000247\n",
      "Epoch: 79/200, Iteration: 3916/10000 --- Training Loss:0.000255\n",
      "Epoch: 79/200, Iteration: 3918/10000 --- Training Loss:0.000216\n",
      "Epoch: 79/200, Iteration: 3920/10000 --- Training Loss:0.000428\n",
      "Epoch: 79/200, Iteration: 3922/10000 --- Training Loss:0.000477\n",
      "Epoch: 79/200, Iteration: 3924/10000 --- Training Loss:0.000317\n",
      "Epoch: 79/200, Iteration: 3926/10000 --- Training Loss:0.000314\n",
      "Epoch: 79/200, Iteration: 3928/10000 --- Training Loss:0.000201\n",
      "Epoch: 79/200, Iteration: 3930/10000 --- Training Loss:0.000310\n",
      "Epoch: 79/200, Iteration: 3932/10000 --- Training Loss:0.000437\n",
      "Epoch: 79/200, Iteration: 3934/10000 --- Training Loss:0.000400\n",
      "Epoch: 79/200, Iteration: 3936/10000 --- Training Loss:0.000296\n",
      "Epoch: 79/200, Iteration: 3938/10000 --- Training Loss:0.000319\n",
      "Epoch: 79/200, Iteration: 3940/10000 --- Training Loss:0.000760\n",
      "Epoch: 79/200, Iteration: 3942/10000 --- Training Loss:0.000277\n",
      "Epoch: 79/200, Iteration: 3944/10000 --- Training Loss:0.000466\n",
      "Epoch: 79/200, Iteration: 3946/10000 --- Training Loss:0.000346\n",
      "Epoch: 79/200, Iteration: 3948/10000 --- Training Loss:0.000277\n",
      "Epoch: 79/200, Iteration: 3950/10000 --- Training Loss:0.000308\n",
      "Epoch: 79 finished ! Train Loss: 0.00037, Test Loss: 0.00150\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 80/200, Iteration: 3952/10000 --- Training Loss:0.000310\n",
      "Epoch: 80/200, Iteration: 3954/10000 --- Training Loss:0.000380\n",
      "Epoch: 80/200, Iteration: 3956/10000 --- Training Loss:0.000200\n",
      "Epoch: 80/200, Iteration: 3958/10000 --- Training Loss:0.000245\n",
      "Epoch: 80/200, Iteration: 3960/10000 --- Training Loss:0.000411\n",
      "Epoch: 80/200, Iteration: 3962/10000 --- Training Loss:0.000263\n",
      "Epoch: 80/200, Iteration: 3964/10000 --- Training Loss:0.000289\n",
      "Epoch: 80/200, Iteration: 3966/10000 --- Training Loss:0.000301\n",
      "Epoch: 80/200, Iteration: 3968/10000 --- Training Loss:0.000527\n",
      "Epoch: 80/200, Iteration: 3970/10000 --- Training Loss:0.000289\n",
      "Epoch: 80/200, Iteration: 3972/10000 --- Training Loss:0.000185\n",
      "Epoch: 80/200, Iteration: 3974/10000 --- Training Loss:0.000221\n",
      "Epoch: 80/200, Iteration: 3976/10000 --- Training Loss:0.000513\n",
      "Epoch: 80/200, Iteration: 3978/10000 --- Training Loss:0.000423\n",
      "Epoch: 80/200, Iteration: 3980/10000 --- Training Loss:0.001144\n",
      "Epoch: 80/200, Iteration: 3982/10000 --- Training Loss:0.000353\n",
      "Epoch: 80/200, Iteration: 3984/10000 --- Training Loss:0.000350\n",
      "Epoch: 80/200, Iteration: 3986/10000 --- Training Loss:0.000257\n",
      "Epoch: 80/200, Iteration: 3988/10000 --- Training Loss:0.000253\n",
      "Epoch: 80/200, Iteration: 3990/10000 --- Training Loss:0.000508\n",
      "Epoch: 80/200, Iteration: 3992/10000 --- Training Loss:0.000309\n",
      "Epoch: 80/200, Iteration: 3994/10000 --- Training Loss:0.000348\n",
      "Epoch: 80/200, Iteration: 3996/10000 --- Training Loss:0.000404\n",
      "Epoch: 80/200, Iteration: 3998/10000 --- Training Loss:0.000269\n",
      "Epoch: 80/200, Iteration: 4000/10000 --- Training Loss:0.000289\n",
      "Epoch: 80 finished ! Train Loss: 0.00037, Test Loss: 0.00159\n",
      "Epoch consuming time: 0m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81/200, Iteration: 4002/10000 --- Training Loss:0.000615\n",
      "Epoch: 81/200, Iteration: 4004/10000 --- Training Loss:0.000360\n",
      "Epoch: 81/200, Iteration: 4006/10000 --- Training Loss:0.000257\n",
      "Epoch: 81/200, Iteration: 4008/10000 --- Training Loss:0.000422\n",
      "Epoch: 81/200, Iteration: 4010/10000 --- Training Loss:0.000385\n",
      "Epoch: 81/200, Iteration: 4012/10000 --- Training Loss:0.000250\n",
      "Epoch: 81/200, Iteration: 4014/10000 --- Training Loss:0.000337\n",
      "Epoch: 81/200, Iteration: 4016/10000 --- Training Loss:0.000252\n",
      "Epoch: 81/200, Iteration: 4018/10000 --- Training Loss:0.000246\n",
      "Epoch: 81/200, Iteration: 4020/10000 --- Training Loss:0.000208\n",
      "Epoch: 81/200, Iteration: 4022/10000 --- Training Loss:0.000108\n",
      "Epoch: 81/200, Iteration: 4024/10000 --- Training Loss:0.000153\n",
      "Epoch: 81/200, Iteration: 4026/10000 --- Training Loss:0.000183\n",
      "Epoch: 81/200, Iteration: 4028/10000 --- Training Loss:0.000248\n",
      "Epoch: 81/200, Iteration: 4030/10000 --- Training Loss:0.000248\n",
      "Epoch: 81/200, Iteration: 4032/10000 --- Training Loss:0.000269\n",
      "Epoch: 81/200, Iteration: 4034/10000 --- Training Loss:0.000192\n",
      "Epoch: 81/200, Iteration: 4036/10000 --- Training Loss:0.000343\n",
      "Epoch: 81/200, Iteration: 4038/10000 --- Training Loss:0.000410\n",
      "Epoch: 81/200, Iteration: 4040/10000 --- Training Loss:0.000362\n",
      "Epoch: 81/200, Iteration: 4042/10000 --- Training Loss:0.000242\n",
      "Epoch: 81/200, Iteration: 4044/10000 --- Training Loss:0.000261\n",
      "Epoch: 81/200, Iteration: 4046/10000 --- Training Loss:0.000291\n",
      "Epoch: 81/200, Iteration: 4048/10000 --- Training Loss:0.000386\n",
      "Epoch: 81/200, Iteration: 4050/10000 --- Training Loss:0.000293\n",
      "Epoch: 81 finished ! Train Loss: 0.00035, Test Loss: 0.00186\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 82/200, Iteration: 4052/10000 --- Training Loss:0.000613\n",
      "Epoch: 82/200, Iteration: 4054/10000 --- Training Loss:0.000341\n",
      "Epoch: 82/200, Iteration: 4056/10000 --- Training Loss:0.000275\n",
      "Epoch: 82/200, Iteration: 4058/10000 --- Training Loss:0.000271\n",
      "Epoch: 82/200, Iteration: 4060/10000 --- Training Loss:0.000231\n",
      "Epoch: 82/200, Iteration: 4062/10000 --- Training Loss:0.000265\n",
      "Epoch: 82/200, Iteration: 4064/10000 --- Training Loss:0.000299\n",
      "Epoch: 82/200, Iteration: 4066/10000 --- Training Loss:0.000319\n",
      "Epoch: 82/200, Iteration: 4068/10000 --- Training Loss:0.000278\n",
      "Epoch: 82/200, Iteration: 4070/10000 --- Training Loss:0.000370\n",
      "Epoch: 82/200, Iteration: 4072/10000 --- Training Loss:0.000421\n",
      "Epoch: 82/200, Iteration: 4074/10000 --- Training Loss:0.000230\n",
      "Epoch: 82/200, Iteration: 4076/10000 --- Training Loss:0.000239\n",
      "Epoch: 82/200, Iteration: 4078/10000 --- Training Loss:0.000267\n",
      "Epoch: 82/200, Iteration: 4080/10000 --- Training Loss:0.000512\n",
      "Epoch: 82/200, Iteration: 4082/10000 --- Training Loss:0.000454\n",
      "Epoch: 82/200, Iteration: 4084/10000 --- Training Loss:0.000271\n",
      "Epoch: 82/200, Iteration: 4086/10000 --- Training Loss:0.000257\n",
      "Epoch: 82/200, Iteration: 4088/10000 --- Training Loss:0.000341\n",
      "Epoch: 82/200, Iteration: 4090/10000 --- Training Loss:0.000314\n",
      "Epoch: 82/200, Iteration: 4092/10000 --- Training Loss:0.000272\n",
      "Epoch: 82/200, Iteration: 4094/10000 --- Training Loss:0.000406\n",
      "Epoch: 82/200, Iteration: 4096/10000 --- Training Loss:0.000167\n",
      "Epoch: 82/200, Iteration: 4098/10000 --- Training Loss:0.000440\n",
      "Epoch: 82/200, Iteration: 4100/10000 --- Training Loss:0.000576\n",
      "Epoch: 82 finished ! Train Loss: 0.00034, Test Loss: 0.00176\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 83/200, Iteration: 4102/10000 --- Training Loss:0.000338\n",
      "Epoch: 83/200, Iteration: 4104/10000 --- Training Loss:0.000213\n",
      "Epoch: 83/200, Iteration: 4106/10000 --- Training Loss:0.001751\n",
      "Epoch: 83/200, Iteration: 4108/10000 --- Training Loss:0.000245\n",
      "Epoch: 83/200, Iteration: 4110/10000 --- Training Loss:0.000381\n",
      "Epoch: 83/200, Iteration: 4112/10000 --- Training Loss:0.000254\n",
      "Epoch: 83/200, Iteration: 4114/10000 --- Training Loss:0.000349\n",
      "Epoch: 83/200, Iteration: 4116/10000 --- Training Loss:0.000201\n",
      "Epoch: 83/200, Iteration: 4118/10000 --- Training Loss:0.000252\n",
      "Epoch: 83/200, Iteration: 4120/10000 --- Training Loss:0.000353\n",
      "Epoch: 83/200, Iteration: 4122/10000 --- Training Loss:0.000613\n",
      "Epoch: 83/200, Iteration: 4124/10000 --- Training Loss:0.003854\n",
      "Epoch: 83/200, Iteration: 4126/10000 --- Training Loss:0.000920\n",
      "Epoch: 83/200, Iteration: 4128/10000 --- Training Loss:0.000526\n",
      "Epoch: 83/200, Iteration: 4130/10000 --- Training Loss:0.000420\n",
      "Epoch: 83/200, Iteration: 4132/10000 --- Training Loss:0.000362\n",
      "Epoch: 83/200, Iteration: 4134/10000 --- Training Loss:0.000281\n",
      "Epoch: 83/200, Iteration: 4136/10000 --- Training Loss:0.000636\n",
      "Epoch: 83/200, Iteration: 4138/10000 --- Training Loss:0.000324\n",
      "Epoch: 83/200, Iteration: 4140/10000 --- Training Loss:0.000357\n",
      "Epoch: 83/200, Iteration: 4142/10000 --- Training Loss:0.000399\n",
      "Epoch: 83/200, Iteration: 4144/10000 --- Training Loss:0.000353\n",
      "Epoch: 83/200, Iteration: 4146/10000 --- Training Loss:0.000456\n",
      "Epoch: 83/200, Iteration: 4148/10000 --- Training Loss:0.000514\n",
      "Epoch: 83/200, Iteration: 4150/10000 --- Training Loss:0.000274\n",
      "Epoch: 83 finished ! Train Loss: 0.00054, Test Loss: 0.00229\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 84/200, Iteration: 4152/10000 --- Training Loss:0.000277\n",
      "Epoch: 84/200, Iteration: 4154/10000 --- Training Loss:0.000346\n",
      "Epoch: 84/200, Iteration: 4156/10000 --- Training Loss:0.000240\n",
      "Epoch: 84/200, Iteration: 4158/10000 --- Training Loss:0.000297\n",
      "Epoch: 84/200, Iteration: 4160/10000 --- Training Loss:0.000451\n",
      "Epoch: 84/200, Iteration: 4162/10000 --- Training Loss:0.000376\n",
      "Epoch: 84/200, Iteration: 4164/10000 --- Training Loss:0.000302\n",
      "Epoch: 84/200, Iteration: 4166/10000 --- Training Loss:0.000439\n",
      "Epoch: 84/200, Iteration: 4168/10000 --- Training Loss:0.000253\n",
      "Epoch: 84/200, Iteration: 4170/10000 --- Training Loss:0.000543\n",
      "Epoch: 84/200, Iteration: 4172/10000 --- Training Loss:0.000711\n",
      "Epoch: 84/200, Iteration: 4174/10000 --- Training Loss:0.000382\n",
      "Epoch: 84/200, Iteration: 4176/10000 --- Training Loss:0.001081\n",
      "Epoch: 84/200, Iteration: 4178/10000 --- Training Loss:0.000449\n",
      "Epoch: 84/200, Iteration: 4180/10000 --- Training Loss:0.000358\n",
      "Epoch: 84/200, Iteration: 4182/10000 --- Training Loss:0.000297\n",
      "Epoch: 84/200, Iteration: 4184/10000 --- Training Loss:0.000342\n",
      "Epoch: 84/200, Iteration: 4186/10000 --- Training Loss:0.000286\n",
      "Epoch: 84/200, Iteration: 4188/10000 --- Training Loss:0.000453\n",
      "Epoch: 84/200, Iteration: 4190/10000 --- Training Loss:0.000273\n",
      "Epoch: 84/200, Iteration: 4192/10000 --- Training Loss:0.000204\n",
      "Epoch: 84/200, Iteration: 4194/10000 --- Training Loss:0.000457\n",
      "Epoch: 84/200, Iteration: 4196/10000 --- Training Loss:0.000186\n",
      "Epoch: 84/200, Iteration: 4198/10000 --- Training Loss:0.000494\n",
      "Epoch: 84/200, Iteration: 4200/10000 --- Training Loss:0.000428\n",
      "Epoch: 84 finished ! Train Loss: 0.00043, Test Loss: 0.00222\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 85/200, Iteration: 4202/10000 --- Training Loss:0.000339\n",
      "Epoch: 85/200, Iteration: 4204/10000 --- Training Loss:0.000298\n",
      "Epoch: 85/200, Iteration: 4206/10000 --- Training Loss:0.000506\n",
      "Epoch: 85/200, Iteration: 4208/10000 --- Training Loss:0.000564\n",
      "Epoch: 85/200, Iteration: 4210/10000 --- Training Loss:0.000672\n",
      "Epoch: 85/200, Iteration: 4212/10000 --- Training Loss:0.000212\n",
      "Epoch: 85/200, Iteration: 4214/10000 --- Training Loss:0.000199\n",
      "Epoch: 85/200, Iteration: 4216/10000 --- Training Loss:0.000452\n",
      "Epoch: 85/200, Iteration: 4218/10000 --- Training Loss:0.000248\n",
      "Epoch: 85/200, Iteration: 4220/10000 --- Training Loss:0.000327\n",
      "Epoch: 85/200, Iteration: 4222/10000 --- Training Loss:0.000215\n",
      "Epoch: 85/200, Iteration: 4224/10000 --- Training Loss:0.000426\n",
      "Epoch: 85/200, Iteration: 4226/10000 --- Training Loss:0.000293\n",
      "Epoch: 85/200, Iteration: 4228/10000 --- Training Loss:0.000177\n",
      "Epoch: 85/200, Iteration: 4230/10000 --- Training Loss:0.000242\n",
      "Epoch: 85/200, Iteration: 4232/10000 --- Training Loss:0.000146\n",
      "Epoch: 85/200, Iteration: 4234/10000 --- Training Loss:0.000306\n",
      "Epoch: 85/200, Iteration: 4236/10000 --- Training Loss:0.000292\n",
      "Epoch: 85/200, Iteration: 4238/10000 --- Training Loss:0.000372\n",
      "Epoch: 85/200, Iteration: 4240/10000 --- Training Loss:0.000368\n",
      "Epoch: 85/200, Iteration: 4242/10000 --- Training Loss:0.000254\n",
      "Epoch: 85/200, Iteration: 4244/10000 --- Training Loss:0.000265\n",
      "Epoch: 85/200, Iteration: 4246/10000 --- Training Loss:0.000144\n",
      "Epoch: 85/200, Iteration: 4248/10000 --- Training Loss:0.000457\n",
      "Epoch: 85/200, Iteration: 4250/10000 --- Training Loss:0.000615\n",
      "Epoch: 85 finished ! Train Loss: 0.00037, Test Loss: 0.00185\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 86/200, Iteration: 4252/10000 --- Training Loss:0.000546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86/200, Iteration: 4254/10000 --- Training Loss:0.000211\n",
      "Epoch: 86/200, Iteration: 4256/10000 --- Training Loss:0.000447\n",
      "Epoch: 86/200, Iteration: 4258/10000 --- Training Loss:0.000286\n",
      "Epoch: 86/200, Iteration: 4260/10000 --- Training Loss:0.000566\n",
      "Epoch: 86/200, Iteration: 4262/10000 --- Training Loss:0.000447\n",
      "Epoch: 86/200, Iteration: 4264/10000 --- Training Loss:0.000391\n",
      "Epoch: 86/200, Iteration: 4266/10000 --- Training Loss:0.000420\n",
      "Epoch: 86/200, Iteration: 4268/10000 --- Training Loss:0.000302\n",
      "Epoch: 86/200, Iteration: 4270/10000 --- Training Loss:0.000358\n",
      "Epoch: 86/200, Iteration: 4272/10000 --- Training Loss:0.001083\n",
      "Epoch: 86/200, Iteration: 4274/10000 --- Training Loss:0.000302\n",
      "Epoch: 86/200, Iteration: 4276/10000 --- Training Loss:0.000377\n",
      "Epoch: 86/200, Iteration: 4278/10000 --- Training Loss:0.000208\n",
      "Epoch: 86/200, Iteration: 4280/10000 --- Training Loss:0.000292\n",
      "Epoch: 86/200, Iteration: 4282/10000 --- Training Loss:0.000240\n",
      "Epoch: 86/200, Iteration: 4284/10000 --- Training Loss:0.000280\n",
      "Epoch: 86/200, Iteration: 4286/10000 --- Training Loss:0.000388\n",
      "Epoch: 86/200, Iteration: 4288/10000 --- Training Loss:0.000235\n",
      "Epoch: 86/200, Iteration: 4290/10000 --- Training Loss:0.000273\n",
      "Epoch: 86/200, Iteration: 4292/10000 --- Training Loss:0.000270\n",
      "Epoch: 86/200, Iteration: 4294/10000 --- Training Loss:0.000381\n",
      "Epoch: 86/200, Iteration: 4296/10000 --- Training Loss:0.000226\n",
      "Epoch: 86/200, Iteration: 4298/10000 --- Training Loss:0.000328\n",
      "Epoch: 86/200, Iteration: 4300/10000 --- Training Loss:0.000242\n",
      "Epoch: 86 finished ! Train Loss: 0.00034, Test Loss: 0.00201\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 87/200, Iteration: 4302/10000 --- Training Loss:0.000208\n",
      "Epoch: 87/200, Iteration: 4304/10000 --- Training Loss:0.000277\n",
      "Epoch: 87/200, Iteration: 4306/10000 --- Training Loss:0.000234\n",
      "Epoch: 87/200, Iteration: 4308/10000 --- Training Loss:0.000485\n",
      "Epoch: 87/200, Iteration: 4310/10000 --- Training Loss:0.000415\n",
      "Epoch: 87/200, Iteration: 4312/10000 --- Training Loss:0.000295\n",
      "Epoch: 87/200, Iteration: 4314/10000 --- Training Loss:0.000247\n",
      "Epoch: 87/200, Iteration: 4316/10000 --- Training Loss:0.000452\n",
      "Epoch: 87/200, Iteration: 4318/10000 --- Training Loss:0.000188\n",
      "Epoch: 87/200, Iteration: 4320/10000 --- Training Loss:0.000329\n",
      "Epoch: 87/200, Iteration: 4322/10000 --- Training Loss:0.000318\n",
      "Epoch: 87/200, Iteration: 4324/10000 --- Training Loss:0.000181\n",
      "Epoch: 87/200, Iteration: 4326/10000 --- Training Loss:0.000262\n",
      "Epoch: 87/200, Iteration: 4328/10000 --- Training Loss:0.000496\n",
      "Epoch: 87/200, Iteration: 4330/10000 --- Training Loss:0.000244\n",
      "Epoch: 87/200, Iteration: 4332/10000 --- Training Loss:0.000187\n",
      "Epoch: 87/200, Iteration: 4334/10000 --- Training Loss:0.000182\n",
      "Epoch: 87/200, Iteration: 4336/10000 --- Training Loss:0.000248\n",
      "Epoch: 87/200, Iteration: 4338/10000 --- Training Loss:0.000287\n",
      "Epoch: 87/200, Iteration: 4340/10000 --- Training Loss:0.000262\n",
      "Epoch: 87/200, Iteration: 4342/10000 --- Training Loss:0.000323\n",
      "Epoch: 87/200, Iteration: 4344/10000 --- Training Loss:0.000312\n",
      "Epoch: 87/200, Iteration: 4346/10000 --- Training Loss:0.000196\n",
      "Epoch: 87/200, Iteration: 4348/10000 --- Training Loss:0.000267\n",
      "Epoch: 87/200, Iteration: 4350/10000 --- Training Loss:0.000330\n",
      "Epoch: 87 finished ! Train Loss: 0.00029, Test Loss: 0.00126\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 43 percent completed\n",
      "Epoch: 88/200, Iteration: 4352/10000 --- Training Loss:0.000262\n",
      "Epoch: 88/200, Iteration: 4354/10000 --- Training Loss:0.000293\n",
      "Epoch: 88/200, Iteration: 4356/10000 --- Training Loss:0.000287\n",
      "Epoch: 88/200, Iteration: 4358/10000 --- Training Loss:0.000175\n",
      "Epoch: 88/200, Iteration: 4360/10000 --- Training Loss:0.000439\n",
      "Epoch: 88/200, Iteration: 4362/10000 --- Training Loss:0.000266\n",
      "Epoch: 88/200, Iteration: 4364/10000 --- Training Loss:0.000367\n",
      "Epoch: 88/200, Iteration: 4366/10000 --- Training Loss:0.000181\n",
      "Epoch: 88/200, Iteration: 4368/10000 --- Training Loss:0.000192\n",
      "Epoch: 88/200, Iteration: 4370/10000 --- Training Loss:0.000268\n",
      "Epoch: 88/200, Iteration: 4372/10000 --- Training Loss:0.000479\n",
      "Epoch: 88/200, Iteration: 4374/10000 --- Training Loss:0.000199\n",
      "Epoch: 88/200, Iteration: 4376/10000 --- Training Loss:0.000276\n",
      "Epoch: 88/200, Iteration: 4378/10000 --- Training Loss:0.000244\n",
      "Epoch: 88/200, Iteration: 4380/10000 --- Training Loss:0.000302\n",
      "Epoch: 88/200, Iteration: 4382/10000 --- Training Loss:0.000319\n",
      "Epoch: 88/200, Iteration: 4384/10000 --- Training Loss:0.000255\n",
      "Epoch: 88/200, Iteration: 4386/10000 --- Training Loss:0.000306\n",
      "Epoch: 88/200, Iteration: 4388/10000 --- Training Loss:0.000985\n",
      "Epoch: 88/200, Iteration: 4390/10000 --- Training Loss:0.000279\n",
      "Epoch: 88/200, Iteration: 4392/10000 --- Training Loss:0.000237\n",
      "Epoch: 88/200, Iteration: 4394/10000 --- Training Loss:0.000264\n",
      "Epoch: 88/200, Iteration: 4396/10000 --- Training Loss:0.000162\n",
      "Epoch: 88/200, Iteration: 4398/10000 --- Training Loss:0.000410\n",
      "Epoch: 88/200, Iteration: 4400/10000 --- Training Loss:0.000378\n",
      "Epoch: 88 finished ! Train Loss: 0.00031, Test Loss: 0.00148\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 89/200, Iteration: 4402/10000 --- Training Loss:0.000182\n",
      "Epoch: 89/200, Iteration: 4404/10000 --- Training Loss:0.001875\n",
      "Epoch: 89/200, Iteration: 4406/10000 --- Training Loss:0.000384\n",
      "Epoch: 89/200, Iteration: 4408/10000 --- Training Loss:0.000360\n",
      "Epoch: 89/200, Iteration: 4410/10000 --- Training Loss:0.000524\n",
      "Epoch: 89/200, Iteration: 4412/10000 --- Training Loss:0.000639\n",
      "Epoch: 89/200, Iteration: 4414/10000 --- Training Loss:0.000395\n",
      "Epoch: 89/200, Iteration: 4416/10000 --- Training Loss:0.000530\n",
      "Epoch: 89/200, Iteration: 4418/10000 --- Training Loss:0.000406\n",
      "Epoch: 89/200, Iteration: 4420/10000 --- Training Loss:0.000374\n",
      "Epoch: 89/200, Iteration: 4422/10000 --- Training Loss:0.000670\n",
      "Epoch: 89/200, Iteration: 4424/10000 --- Training Loss:0.000467\n",
      "Epoch: 89/200, Iteration: 4426/10000 --- Training Loss:0.000680\n",
      "Epoch: 89/200, Iteration: 4428/10000 --- Training Loss:0.000497\n",
      "Epoch: 89/200, Iteration: 4430/10000 --- Training Loss:0.000423\n",
      "Epoch: 89/200, Iteration: 4432/10000 --- Training Loss:0.000619\n",
      "Epoch: 89/200, Iteration: 4434/10000 --- Training Loss:0.000904\n",
      "Epoch: 89/200, Iteration: 4436/10000 --- Training Loss:0.000252\n",
      "Epoch: 89/200, Iteration: 4438/10000 --- Training Loss:0.000404\n",
      "Epoch: 89/200, Iteration: 4440/10000 --- Training Loss:0.000295\n",
      "Epoch: 89/200, Iteration: 4442/10000 --- Training Loss:0.000248\n",
      "Epoch: 89/200, Iteration: 4444/10000 --- Training Loss:0.000187\n",
      "Epoch: 89/200, Iteration: 4446/10000 --- Training Loss:0.000338\n",
      "Epoch: 89/200, Iteration: 4448/10000 --- Training Loss:0.000256\n",
      "Epoch: 89/200, Iteration: 4450/10000 --- Training Loss:0.000179\n",
      "Epoch: 89 finished ! Train Loss: 0.00049, Test Loss: 0.00174\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 90/200, Iteration: 4452/10000 --- Training Loss:0.000277\n",
      "Epoch: 90/200, Iteration: 4454/10000 --- Training Loss:0.000275\n",
      "Epoch: 90/200, Iteration: 4456/10000 --- Training Loss:0.000236\n",
      "Epoch: 90/200, Iteration: 4458/10000 --- Training Loss:0.000284\n",
      "Epoch: 90/200, Iteration: 4460/10000 --- Training Loss:0.000248\n",
      "Epoch: 90/200, Iteration: 4462/10000 --- Training Loss:0.000206\n",
      "Epoch: 90/200, Iteration: 4464/10000 --- Training Loss:0.000191\n",
      "Epoch: 90/200, Iteration: 4466/10000 --- Training Loss:0.000275\n",
      "Epoch: 90/200, Iteration: 4468/10000 --- Training Loss:0.000170\n",
      "Epoch: 90/200, Iteration: 4470/10000 --- Training Loss:0.000298\n",
      "Epoch: 90/200, Iteration: 4472/10000 --- Training Loss:0.000157\n",
      "Epoch: 90/200, Iteration: 4474/10000 --- Training Loss:0.000202\n",
      "Epoch: 90/200, Iteration: 4476/10000 --- Training Loss:0.000428\n",
      "Epoch: 90/200, Iteration: 4478/10000 --- Training Loss:0.000286\n",
      "Epoch: 90/200, Iteration: 4480/10000 --- Training Loss:0.000283\n",
      "Epoch: 90/200, Iteration: 4482/10000 --- Training Loss:0.000329\n",
      "Epoch: 90/200, Iteration: 4484/10000 --- Training Loss:0.000289\n",
      "Epoch: 90/200, Iteration: 4486/10000 --- Training Loss:0.000166\n",
      "Epoch: 90/200, Iteration: 4488/10000 --- Training Loss:0.000212\n",
      "Epoch: 90/200, Iteration: 4490/10000 --- Training Loss:0.000885\n",
      "Epoch: 90/200, Iteration: 4492/10000 --- Training Loss:0.000255\n",
      "Epoch: 90/200, Iteration: 4494/10000 --- Training Loss:0.000301\n",
      "Epoch: 90/200, Iteration: 4496/10000 --- Training Loss:0.000231\n",
      "Epoch: 90/200, Iteration: 4498/10000 --- Training Loss:0.000243\n",
      "Epoch: 90/200, Iteration: 4500/10000 --- Training Loss:0.000271\n",
      "Epoch: 90 finished ! Train Loss: 0.00029, Test Loss: 0.00139\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 91/200, Iteration: 4502/10000 --- Training Loss:0.000278\n",
      "Epoch: 91/200, Iteration: 4504/10000 --- Training Loss:0.000274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91/200, Iteration: 4506/10000 --- Training Loss:0.000306\n",
      "Epoch: 91/200, Iteration: 4508/10000 --- Training Loss:0.000197\n",
      "Epoch: 91/200, Iteration: 4510/10000 --- Training Loss:0.000249\n",
      "Epoch: 91/200, Iteration: 4512/10000 --- Training Loss:0.000212\n",
      "Epoch: 91/200, Iteration: 4514/10000 --- Training Loss:0.000238\n",
      "Epoch: 91/200, Iteration: 4516/10000 --- Training Loss:0.000254\n",
      "Epoch: 91/200, Iteration: 4518/10000 --- Training Loss:0.000213\n",
      "Epoch: 91/200, Iteration: 4520/10000 --- Training Loss:0.000245\n",
      "Epoch: 91/200, Iteration: 4522/10000 --- Training Loss:0.000244\n",
      "Epoch: 91/200, Iteration: 4524/10000 --- Training Loss:0.000193\n",
      "Epoch: 91/200, Iteration: 4526/10000 --- Training Loss:0.000184\n",
      "Epoch: 91/200, Iteration: 4528/10000 --- Training Loss:0.000402\n",
      "Epoch: 91/200, Iteration: 4530/10000 --- Training Loss:0.000222\n",
      "Epoch: 91/200, Iteration: 4532/10000 --- Training Loss:0.000203\n",
      "Epoch: 91/200, Iteration: 4534/10000 --- Training Loss:0.000266\n",
      "Epoch: 91/200, Iteration: 4536/10000 --- Training Loss:0.000680\n",
      "Epoch: 91/200, Iteration: 4538/10000 --- Training Loss:0.000391\n",
      "Epoch: 91/200, Iteration: 4540/10000 --- Training Loss:0.000230\n",
      "Epoch: 91/200, Iteration: 4542/10000 --- Training Loss:0.000270\n",
      "Epoch: 91/200, Iteration: 4544/10000 --- Training Loss:0.000209\n",
      "Epoch: 91/200, Iteration: 4546/10000 --- Training Loss:0.000173\n",
      "Epoch: 91/200, Iteration: 4548/10000 --- Training Loss:0.000235\n",
      "Epoch: 91/200, Iteration: 4550/10000 --- Training Loss:0.000352\n",
      "Epoch: 91 finished ! Train Loss: 0.00030, Test Loss: 0.00252\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 92/200, Iteration: 4552/10000 --- Training Loss:0.000445\n",
      "Epoch: 92/200, Iteration: 4554/10000 --- Training Loss:0.000616\n",
      "Epoch: 92/200, Iteration: 4556/10000 --- Training Loss:0.000410\n",
      "Epoch: 92/200, Iteration: 4558/10000 --- Training Loss:0.000232\n",
      "Epoch: 92/200, Iteration: 4560/10000 --- Training Loss:0.000235\n",
      "Epoch: 92/200, Iteration: 4562/10000 --- Training Loss:0.000655\n",
      "Epoch: 92/200, Iteration: 4564/10000 --- Training Loss:0.000319\n",
      "Epoch: 92/200, Iteration: 4566/10000 --- Training Loss:0.000161\n",
      "Epoch: 92/200, Iteration: 4568/10000 --- Training Loss:0.000277\n",
      "Epoch: 92/200, Iteration: 4570/10000 --- Training Loss:0.000191\n",
      "Epoch: 92/200, Iteration: 4572/10000 --- Training Loss:0.000210\n",
      "Epoch: 92/200, Iteration: 4574/10000 --- Training Loss:0.000256\n",
      "Epoch: 92/200, Iteration: 4576/10000 --- Training Loss:0.000368\n",
      "Epoch: 92/200, Iteration: 4578/10000 --- Training Loss:0.000336\n",
      "Epoch: 92/200, Iteration: 4580/10000 --- Training Loss:0.000215\n",
      "Epoch: 92/200, Iteration: 4582/10000 --- Training Loss:0.000552\n",
      "Epoch: 92/200, Iteration: 4584/10000 --- Training Loss:0.000212\n",
      "Epoch: 92/200, Iteration: 4586/10000 --- Training Loss:0.000216\n",
      "Epoch: 92/200, Iteration: 4588/10000 --- Training Loss:0.000458\n",
      "Epoch: 92/200, Iteration: 4590/10000 --- Training Loss:0.000347\n",
      "Epoch: 92/200, Iteration: 4592/10000 --- Training Loss:0.000257\n",
      "Epoch: 92/200, Iteration: 4594/10000 --- Training Loss:0.000416\n",
      "Epoch: 92/200, Iteration: 4596/10000 --- Training Loss:0.000168\n",
      "Epoch: 92/200, Iteration: 4598/10000 --- Training Loss:0.000471\n",
      "Epoch: 92/200, Iteration: 4600/10000 --- Training Loss:0.000399\n",
      "Epoch: 92 finished ! Train Loss: 0.00033, Test Loss: 0.00251\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 93/200, Iteration: 4602/10000 --- Training Loss:0.000400\n",
      "Epoch: 93/200, Iteration: 4604/10000 --- Training Loss:0.000277\n",
      "Epoch: 93/200, Iteration: 4606/10000 --- Training Loss:0.000304\n",
      "Epoch: 93/200, Iteration: 4608/10000 --- Training Loss:0.000386\n",
      "Epoch: 93/200, Iteration: 4610/10000 --- Training Loss:0.000357\n",
      "Epoch: 93/200, Iteration: 4612/10000 --- Training Loss:0.000241\n",
      "Epoch: 93/200, Iteration: 4614/10000 --- Training Loss:0.000391\n",
      "Epoch: 93/200, Iteration: 4616/10000 --- Training Loss:0.000257\n",
      "Epoch: 93/200, Iteration: 4618/10000 --- Training Loss:0.000229\n",
      "Epoch: 93/200, Iteration: 4620/10000 --- Training Loss:0.000824\n",
      "Epoch: 93/200, Iteration: 4622/10000 --- Training Loss:0.000239\n",
      "Epoch: 93/200, Iteration: 4624/10000 --- Training Loss:0.000291\n",
      "Epoch: 93/200, Iteration: 4626/10000 --- Training Loss:0.000252\n",
      "Epoch: 93/200, Iteration: 4628/10000 --- Training Loss:0.000378\n",
      "Epoch: 93/200, Iteration: 4630/10000 --- Training Loss:0.000221\n",
      "Epoch: 93/200, Iteration: 4632/10000 --- Training Loss:0.000149\n",
      "Epoch: 93/200, Iteration: 4634/10000 --- Training Loss:0.000262\n",
      "Epoch: 93/200, Iteration: 4636/10000 --- Training Loss:0.000185\n",
      "Epoch: 93/200, Iteration: 4638/10000 --- Training Loss:0.000126\n",
      "Epoch: 93/200, Iteration: 4640/10000 --- Training Loss:0.000137\n",
      "Epoch: 93/200, Iteration: 4642/10000 --- Training Loss:0.000237\n",
      "Epoch: 93/200, Iteration: 4644/10000 --- Training Loss:0.000316\n",
      "Epoch: 93/200, Iteration: 4646/10000 --- Training Loss:0.000222\n",
      "Epoch: 93/200, Iteration: 4648/10000 --- Training Loss:0.000198\n",
      "Epoch: 93/200, Iteration: 4650/10000 --- Training Loss:0.000162\n",
      "Epoch: 93 finished ! Train Loss: 0.00028, Test Loss: 0.00136\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 94/200, Iteration: 4652/10000 --- Training Loss:0.000255\n",
      "Epoch: 94/200, Iteration: 4654/10000 --- Training Loss:0.000316\n",
      "Epoch: 94/200, Iteration: 4656/10000 --- Training Loss:0.000212\n",
      "Epoch: 94/200, Iteration: 4658/10000 --- Training Loss:0.000221\n",
      "Epoch: 94/200, Iteration: 4660/10000 --- Training Loss:0.000207\n",
      "Epoch: 94/200, Iteration: 4662/10000 --- Training Loss:0.000158\n",
      "Epoch: 94/200, Iteration: 4664/10000 --- Training Loss:0.000120\n",
      "Epoch: 94/200, Iteration: 4666/10000 --- Training Loss:0.000175\n",
      "Epoch: 94/200, Iteration: 4668/10000 --- Training Loss:0.000293\n",
      "Epoch: 94/200, Iteration: 4670/10000 --- Training Loss:0.000201\n",
      "Epoch: 94/200, Iteration: 4672/10000 --- Training Loss:0.000273\n",
      "Epoch: 94/200, Iteration: 4674/10000 --- Training Loss:0.000203\n",
      "Epoch: 94/200, Iteration: 4676/10000 --- Training Loss:0.000183\n",
      "Epoch: 94/200, Iteration: 4678/10000 --- Training Loss:0.000319\n",
      "Epoch: 94/200, Iteration: 4680/10000 --- Training Loss:0.000206\n",
      "Epoch: 94/200, Iteration: 4682/10000 --- Training Loss:0.000257\n",
      "Epoch: 94/200, Iteration: 4684/10000 --- Training Loss:0.000144\n",
      "Epoch: 94/200, Iteration: 4686/10000 --- Training Loss:0.000276\n",
      "Epoch: 94/200, Iteration: 4688/10000 --- Training Loss:0.000175\n",
      "Epoch: 94/200, Iteration: 4690/10000 --- Training Loss:0.000502\n",
      "Epoch: 94/200, Iteration: 4692/10000 --- Training Loss:0.000401\n",
      "Epoch: 94/200, Iteration: 4694/10000 --- Training Loss:0.000189\n",
      "Epoch: 94/200, Iteration: 4696/10000 --- Training Loss:0.000197\n",
      "Epoch: 94/200, Iteration: 4698/10000 --- Training Loss:0.000324\n",
      "Epoch: 94/200, Iteration: 4700/10000 --- Training Loss:0.000345\n",
      "Epoch: 94 finished ! Train Loss: 0.00025, Test Loss: 0.00152\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 95/200, Iteration: 4702/10000 --- Training Loss:0.000262\n",
      "Epoch: 95/200, Iteration: 4704/10000 --- Training Loss:0.000182\n",
      "Epoch: 95/200, Iteration: 4706/10000 --- Training Loss:0.000162\n",
      "Epoch: 95/200, Iteration: 4708/10000 --- Training Loss:0.000277\n",
      "Epoch: 95/200, Iteration: 4710/10000 --- Training Loss:0.000369\n",
      "Epoch: 95/200, Iteration: 4712/10000 --- Training Loss:0.000226\n",
      "Epoch: 95/200, Iteration: 4714/10000 --- Training Loss:0.000444\n",
      "Epoch: 95/200, Iteration: 4716/10000 --- Training Loss:0.000163\n",
      "Epoch: 95/200, Iteration: 4718/10000 --- Training Loss:0.000276\n",
      "Epoch: 95/200, Iteration: 4720/10000 --- Training Loss:0.000396\n",
      "Epoch: 95/200, Iteration: 4722/10000 --- Training Loss:0.000148\n",
      "Epoch: 95/200, Iteration: 4724/10000 --- Training Loss:0.000194\n",
      "Epoch: 95/200, Iteration: 4726/10000 --- Training Loss:0.000227\n",
      "Epoch: 95/200, Iteration: 4728/10000 --- Training Loss:0.000262\n",
      "Epoch: 95/200, Iteration: 4730/10000 --- Training Loss:0.000226\n",
      "Epoch: 95/200, Iteration: 4732/10000 --- Training Loss:0.000203\n",
      "Epoch: 95/200, Iteration: 4734/10000 --- Training Loss:0.000330\n",
      "Epoch: 95/200, Iteration: 4736/10000 --- Training Loss:0.000340\n",
      "Epoch: 95/200, Iteration: 4738/10000 --- Training Loss:0.000297\n",
      "Epoch: 95/200, Iteration: 4740/10000 --- Training Loss:0.000348\n",
      "Epoch: 95/200, Iteration: 4742/10000 --- Training Loss:0.000153\n",
      "Epoch: 95/200, Iteration: 4744/10000 --- Training Loss:0.000469\n",
      "Epoch: 95/200, Iteration: 4746/10000 --- Training Loss:0.000239\n",
      "Epoch: 95/200, Iteration: 4748/10000 --- Training Loss:0.000216\n",
      "Epoch: 95/200, Iteration: 4750/10000 --- Training Loss:0.000264\n",
      "Epoch: 95 finished ! Train Loss: 0.00028, Test Loss: 0.00181\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 96/200, Iteration: 4752/10000 --- Training Loss:0.000198\n",
      "Epoch: 96/200, Iteration: 4754/10000 --- Training Loss:0.000426\n",
      "Epoch: 96/200, Iteration: 4756/10000 --- Training Loss:0.000195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96/200, Iteration: 4758/10000 --- Training Loss:0.000165\n",
      "Epoch: 96/200, Iteration: 4760/10000 --- Training Loss:0.000190\n",
      "Epoch: 96/200, Iteration: 4762/10000 --- Training Loss:0.000320\n",
      "Epoch: 96/200, Iteration: 4764/10000 --- Training Loss:0.000190\n",
      "Epoch: 96/200, Iteration: 4766/10000 --- Training Loss:0.000196\n",
      "Epoch: 96/200, Iteration: 4768/10000 --- Training Loss:0.000190\n",
      "Epoch: 96/200, Iteration: 4770/10000 --- Training Loss:0.000235\n",
      "Epoch: 96/200, Iteration: 4772/10000 --- Training Loss:0.000174\n",
      "Epoch: 96/200, Iteration: 4774/10000 --- Training Loss:0.000252\n",
      "Epoch: 96/200, Iteration: 4776/10000 --- Training Loss:0.000493\n",
      "Epoch: 96/200, Iteration: 4778/10000 --- Training Loss:0.000306\n",
      "Epoch: 96/200, Iteration: 4780/10000 --- Training Loss:0.000473\n",
      "Epoch: 96/200, Iteration: 4782/10000 --- Training Loss:0.000185\n",
      "Epoch: 96/200, Iteration: 4784/10000 --- Training Loss:0.000199\n",
      "Epoch: 96/200, Iteration: 4786/10000 --- Training Loss:0.000422\n",
      "Epoch: 96/200, Iteration: 4788/10000 --- Training Loss:0.000157\n",
      "Epoch: 96/200, Iteration: 4790/10000 --- Training Loss:0.000342\n",
      "Epoch: 96/200, Iteration: 4792/10000 --- Training Loss:0.000226\n",
      "Epoch: 96/200, Iteration: 4794/10000 --- Training Loss:0.000220\n",
      "Epoch: 96/200, Iteration: 4796/10000 --- Training Loss:0.000184\n",
      "Epoch: 96/200, Iteration: 4798/10000 --- Training Loss:0.000156\n",
      "Epoch: 96/200, Iteration: 4800/10000 --- Training Loss:0.001072\n",
      "Epoch: 96 finished ! Train Loss: 0.00028, Test Loss: 0.00341\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 97/200, Iteration: 4802/10000 --- Training Loss:0.000413\n",
      "Epoch: 97/200, Iteration: 4804/10000 --- Training Loss:0.001280\n",
      "Epoch: 97/200, Iteration: 4806/10000 --- Training Loss:0.000307\n",
      "Epoch: 97/200, Iteration: 4808/10000 --- Training Loss:0.000397\n",
      "Epoch: 97/200, Iteration: 4810/10000 --- Training Loss:0.000683\n",
      "Epoch: 97/200, Iteration: 4812/10000 --- Training Loss:0.000192\n",
      "Epoch: 97/200, Iteration: 4814/10000 --- Training Loss:0.000300\n",
      "Epoch: 97/200, Iteration: 4816/10000 --- Training Loss:0.000576\n",
      "Epoch: 97/200, Iteration: 4818/10000 --- Training Loss:0.000377\n",
      "Epoch: 97/200, Iteration: 4820/10000 --- Training Loss:0.000252\n",
      "Epoch: 97/200, Iteration: 4822/10000 --- Training Loss:0.000467\n",
      "Epoch: 97/200, Iteration: 4824/10000 --- Training Loss:0.000325\n",
      "Epoch: 97/200, Iteration: 4826/10000 --- Training Loss:0.000161\n",
      "Epoch: 97/200, Iteration: 4828/10000 --- Training Loss:0.000193\n",
      "Epoch: 97/200, Iteration: 4830/10000 --- Training Loss:0.000460\n",
      "Epoch: 97/200, Iteration: 4832/10000 --- Training Loss:0.000226\n",
      "Epoch: 97/200, Iteration: 4834/10000 --- Training Loss:0.000302\n",
      "Epoch: 97/200, Iteration: 4836/10000 --- Training Loss:0.000369\n",
      "Epoch: 97/200, Iteration: 4838/10000 --- Training Loss:0.000221\n",
      "Epoch: 97/200, Iteration: 4840/10000 --- Training Loss:0.000205\n",
      "Epoch: 97/200, Iteration: 4842/10000 --- Training Loss:0.000234\n",
      "Epoch: 97/200, Iteration: 4844/10000 --- Training Loss:0.000251\n",
      "Epoch: 97/200, Iteration: 4846/10000 --- Training Loss:0.000281\n",
      "Epoch: 97/200, Iteration: 4848/10000 --- Training Loss:0.000255\n",
      "Epoch: 97/200, Iteration: 4850/10000 --- Training Loss:0.000230\n",
      "Epoch: 97 finished ! Train Loss: 0.00036, Test Loss: 0.00164\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 98/200, Iteration: 4852/10000 --- Training Loss:0.000226\n",
      "Epoch: 98/200, Iteration: 4854/10000 --- Training Loss:0.000210\n",
      "Epoch: 98/200, Iteration: 4856/10000 --- Training Loss:0.000184\n",
      "Epoch: 98/200, Iteration: 4858/10000 --- Training Loss:0.000263\n",
      "Epoch: 98/200, Iteration: 4860/10000 --- Training Loss:0.000151\n",
      "Epoch: 98/200, Iteration: 4862/10000 --- Training Loss:0.000199\n",
      "Epoch: 98/200, Iteration: 4864/10000 --- Training Loss:0.000303\n",
      "Epoch: 98/200, Iteration: 4866/10000 --- Training Loss:0.000456\n",
      "Epoch: 98/200, Iteration: 4868/10000 --- Training Loss:0.000239\n",
      "Epoch: 98/200, Iteration: 4870/10000 --- Training Loss:0.000289\n",
      "Epoch: 98/200, Iteration: 4872/10000 --- Training Loss:0.000236\n",
      "Epoch: 98/200, Iteration: 4874/10000 --- Training Loss:0.000164\n",
      "Epoch: 98/200, Iteration: 4876/10000 --- Training Loss:0.000231\n",
      "Epoch: 98/200, Iteration: 4878/10000 --- Training Loss:0.000137\n",
      "Epoch: 98/200, Iteration: 4880/10000 --- Training Loss:0.000376\n",
      "Epoch: 98/200, Iteration: 4882/10000 --- Training Loss:0.000171\n",
      "Epoch: 98/200, Iteration: 4884/10000 --- Training Loss:0.000331\n",
      "Epoch: 98/200, Iteration: 4886/10000 --- Training Loss:0.000628\n",
      "Epoch: 98/200, Iteration: 4888/10000 --- Training Loss:0.000148\n",
      "Epoch: 98/200, Iteration: 4890/10000 --- Training Loss:0.000379\n",
      "Epoch: 98/200, Iteration: 4892/10000 --- Training Loss:0.000419\n",
      "Epoch: 98/200, Iteration: 4894/10000 --- Training Loss:0.000178\n",
      "Epoch: 98/200, Iteration: 4896/10000 --- Training Loss:0.000222\n",
      "Epoch: 98/200, Iteration: 4898/10000 --- Training Loss:0.000443\n",
      "Epoch: 98/200, Iteration: 4900/10000 --- Training Loss:0.000445\n",
      "Epoch: 98 finished ! Train Loss: 0.00026, Test Loss: 0.00217\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 99/200, Iteration: 4902/10000 --- Training Loss:0.000175\n",
      "Epoch: 99/200, Iteration: 4904/10000 --- Training Loss:0.000318\n",
      "Epoch: 99/200, Iteration: 4906/10000 --- Training Loss:0.000277\n",
      "Epoch: 99/200, Iteration: 4908/10000 --- Training Loss:0.000145\n",
      "Epoch: 99/200, Iteration: 4910/10000 --- Training Loss:0.000389\n",
      "Epoch: 99/200, Iteration: 4912/10000 --- Training Loss:0.000237\n",
      "Epoch: 99/200, Iteration: 4914/10000 --- Training Loss:0.000247\n",
      "Epoch: 99/200, Iteration: 4916/10000 --- Training Loss:0.000578\n",
      "Epoch: 99/200, Iteration: 4918/10000 --- Training Loss:0.000308\n",
      "Epoch: 99/200, Iteration: 4920/10000 --- Training Loss:0.000264\n",
      "Epoch: 99/200, Iteration: 4922/10000 --- Training Loss:0.000345\n",
      "Epoch: 99/200, Iteration: 4924/10000 --- Training Loss:0.000310\n",
      "Epoch: 99/200, Iteration: 4926/10000 --- Training Loss:0.000272\n",
      "Epoch: 99/200, Iteration: 4928/10000 --- Training Loss:0.000216\n",
      "Epoch: 99/200, Iteration: 4930/10000 --- Training Loss:0.000186\n",
      "Epoch: 99/200, Iteration: 4932/10000 --- Training Loss:0.000295\n",
      "Epoch: 99/200, Iteration: 4934/10000 --- Training Loss:0.000113\n",
      "Epoch: 99/200, Iteration: 4936/10000 --- Training Loss:0.000192\n",
      "Epoch: 99/200, Iteration: 4938/10000 --- Training Loss:0.000154\n",
      "Epoch: 99/200, Iteration: 4940/10000 --- Training Loss:0.000211\n",
      "Epoch: 99/200, Iteration: 4942/10000 --- Training Loss:0.000210\n",
      "Epoch: 99/200, Iteration: 4944/10000 --- Training Loss:0.000356\n",
      "Epoch: 99/200, Iteration: 4946/10000 --- Training Loss:0.000317\n",
      "Epoch: 99/200, Iteration: 4948/10000 --- Training Loss:0.000151\n",
      "Epoch: 99/200, Iteration: 4950/10000 --- Training Loss:0.000146\n",
      "Epoch: 99 finished ! Train Loss: 0.00028, Test Loss: 0.00141\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 100/200, Iteration: 4952/10000 --- Training Loss:0.000370\n",
      "Epoch: 100/200, Iteration: 4954/10000 --- Training Loss:0.000215\n",
      "Epoch: 100/200, Iteration: 4956/10000 --- Training Loss:0.000132\n",
      "Epoch: 100/200, Iteration: 4958/10000 --- Training Loss:0.000167\n",
      "Epoch: 100/200, Iteration: 4960/10000 --- Training Loss:0.000184\n",
      "Epoch: 100/200, Iteration: 4962/10000 --- Training Loss:0.000267\n",
      "Epoch: 100/200, Iteration: 4964/10000 --- Training Loss:0.000317\n",
      "Epoch: 100/200, Iteration: 4966/10000 --- Training Loss:0.000207\n",
      "Epoch: 100/200, Iteration: 4968/10000 --- Training Loss:0.000235\n",
      "Epoch: 100/200, Iteration: 4970/10000 --- Training Loss:0.000239\n",
      "Epoch: 100/200, Iteration: 4972/10000 --- Training Loss:0.000173\n",
      "Epoch: 100/200, Iteration: 4974/10000 --- Training Loss:0.000192\n",
      "Epoch: 100/200, Iteration: 4976/10000 --- Training Loss:0.000192\n",
      "Epoch: 100/200, Iteration: 4978/10000 --- Training Loss:0.000123\n",
      "Epoch: 100/200, Iteration: 4980/10000 --- Training Loss:0.000163\n",
      "Epoch: 100/200, Iteration: 4982/10000 --- Training Loss:0.000386\n",
      "Epoch: 100/200, Iteration: 4984/10000 --- Training Loss:0.000141\n",
      "Epoch: 100/200, Iteration: 4986/10000 --- Training Loss:0.000137\n",
      "Epoch: 100/200, Iteration: 4988/10000 --- Training Loss:0.000139\n",
      "Epoch: 100/200, Iteration: 4990/10000 --- Training Loss:0.000420\n",
      "Epoch: 100/200, Iteration: 4992/10000 --- Training Loss:0.000190\n",
      "Epoch: 100/200, Iteration: 4994/10000 --- Training Loss:0.000242\n",
      "Epoch: 100/200, Iteration: 4996/10000 --- Training Loss:0.000234\n",
      "Epoch: 100/200, Iteration: 4998/10000 --- Training Loss:0.000197\n",
      "Epoch: 100/200, Iteration: 5000/10000 --- Training Loss:0.000569\n",
      "Epoch: 100 finished ! Train Loss: 0.00025, Test Loss: 0.00187\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 101/200, Iteration: 5002/10000 --- Training Loss:0.000123\n",
      "Epoch: 101/200, Iteration: 5004/10000 --- Training Loss:0.000162\n",
      "Epoch: 101/200, Iteration: 5006/10000 --- Training Loss:0.000203\n",
      "Epoch: 101/200, Iteration: 5008/10000 --- Training Loss:0.000141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101/200, Iteration: 5010/10000 --- Training Loss:0.000116\n",
      "Epoch: 101/200, Iteration: 5012/10000 --- Training Loss:0.000231\n",
      "Epoch: 101/200, Iteration: 5014/10000 --- Training Loss:0.000255\n",
      "Epoch: 101/200, Iteration: 5016/10000 --- Training Loss:0.000228\n",
      "Epoch: 101/200, Iteration: 5018/10000 --- Training Loss:0.000194\n",
      "Epoch: 101/200, Iteration: 5020/10000 --- Training Loss:0.000224\n",
      "Epoch: 101/200, Iteration: 5022/10000 --- Training Loss:0.000164\n",
      "Epoch: 101/200, Iteration: 5024/10000 --- Training Loss:0.000404\n",
      "Epoch: 101/200, Iteration: 5026/10000 --- Training Loss:0.000218\n",
      "Epoch: 101/200, Iteration: 5028/10000 --- Training Loss:0.000270\n",
      "Epoch: 101/200, Iteration: 5030/10000 --- Training Loss:0.000303\n",
      "Epoch: 101/200, Iteration: 5032/10000 --- Training Loss:0.000163\n",
      "Epoch: 101/200, Iteration: 5034/10000 --- Training Loss:0.000167\n",
      "Epoch: 101/200, Iteration: 5036/10000 --- Training Loss:0.000205\n",
      "Epoch: 101/200, Iteration: 5038/10000 --- Training Loss:0.000282\n",
      "Epoch: 101/200, Iteration: 5040/10000 --- Training Loss:0.000260\n",
      "Epoch: 101/200, Iteration: 5042/10000 --- Training Loss:0.000201\n",
      "Epoch: 101/200, Iteration: 5044/10000 --- Training Loss:0.000133\n",
      "Epoch: 101/200, Iteration: 5046/10000 --- Training Loss:0.000346\n",
      "Epoch: 101/200, Iteration: 5048/10000 --- Training Loss:0.000732\n",
      "Epoch: 101/200, Iteration: 5050/10000 --- Training Loss:0.000154\n",
      "Epoch: 101 finished ! Train Loss: 0.00023, Test Loss: 0.00209\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 102/200, Iteration: 5052/10000 --- Training Loss:0.000180\n",
      "Epoch: 102/200, Iteration: 5054/10000 --- Training Loss:0.000288\n",
      "Epoch: 102/200, Iteration: 5056/10000 --- Training Loss:0.000328\n",
      "Epoch: 102/200, Iteration: 5058/10000 --- Training Loss:0.000236\n",
      "Epoch: 102/200, Iteration: 5060/10000 --- Training Loss:0.000150\n",
      "Epoch: 102/200, Iteration: 5062/10000 --- Training Loss:0.000133\n",
      "Epoch: 102/200, Iteration: 5064/10000 --- Training Loss:0.000205\n",
      "Epoch: 102/200, Iteration: 5066/10000 --- Training Loss:0.000183\n",
      "Epoch: 102/200, Iteration: 5068/10000 --- Training Loss:0.000458\n",
      "Epoch: 102/200, Iteration: 5070/10000 --- Training Loss:0.000240\n",
      "Epoch: 102/200, Iteration: 5072/10000 --- Training Loss:0.000362\n",
      "Epoch: 102/200, Iteration: 5074/10000 --- Training Loss:0.000211\n",
      "Epoch: 102/200, Iteration: 5076/10000 --- Training Loss:0.000338\n",
      "Epoch: 102/200, Iteration: 5078/10000 --- Training Loss:0.000341\n",
      "Epoch: 102/200, Iteration: 5080/10000 --- Training Loss:0.000154\n",
      "Epoch: 102/200, Iteration: 5082/10000 --- Training Loss:0.000359\n",
      "Epoch: 102/200, Iteration: 5084/10000 --- Training Loss:0.000238\n",
      "Epoch: 102/200, Iteration: 5086/10000 --- Training Loss:0.000340\n",
      "Epoch: 102/200, Iteration: 5088/10000 --- Training Loss:0.000220\n",
      "Epoch: 102/200, Iteration: 5090/10000 --- Training Loss:0.000469\n",
      "Epoch: 102/200, Iteration: 5092/10000 --- Training Loss:0.000294\n",
      "Epoch: 102/200, Iteration: 5094/10000 --- Training Loss:0.000181\n",
      "Epoch: 102/200, Iteration: 5096/10000 --- Training Loss:0.000355\n",
      "Epoch: 102/200, Iteration: 5098/10000 --- Training Loss:0.000141\n",
      "Epoch: 102/200, Iteration: 5100/10000 --- Training Loss:0.000488\n",
      "Epoch: 102 finished ! Train Loss: 0.00028, Test Loss: 0.00121\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 51 percent completed\n",
      "Epoch: 103/200, Iteration: 5102/10000 --- Training Loss:0.000267\n",
      "Epoch: 103/200, Iteration: 5104/10000 --- Training Loss:0.000175\n",
      "Epoch: 103/200, Iteration: 5106/10000 --- Training Loss:0.000124\n",
      "Epoch: 103/200, Iteration: 5108/10000 --- Training Loss:0.000240\n",
      "Epoch: 103/200, Iteration: 5110/10000 --- Training Loss:0.000297\n",
      "Epoch: 103/200, Iteration: 5112/10000 --- Training Loss:0.000174\n",
      "Epoch: 103/200, Iteration: 5114/10000 --- Training Loss:0.000217\n",
      "Epoch: 103/200, Iteration: 5116/10000 --- Training Loss:0.000193\n",
      "Epoch: 103/200, Iteration: 5118/10000 --- Training Loss:0.000256\n",
      "Epoch: 103/200, Iteration: 5120/10000 --- Training Loss:0.000144\n",
      "Epoch: 103/200, Iteration: 5122/10000 --- Training Loss:0.000244\n",
      "Epoch: 103/200, Iteration: 5124/10000 --- Training Loss:0.000164\n",
      "Epoch: 103/200, Iteration: 5126/10000 --- Training Loss:0.000227\n",
      "Epoch: 103/200, Iteration: 5128/10000 --- Training Loss:0.000219\n",
      "Epoch: 103/200, Iteration: 5130/10000 --- Training Loss:0.000175\n",
      "Epoch: 103/200, Iteration: 5132/10000 --- Training Loss:0.000169\n",
      "Epoch: 103/200, Iteration: 5134/10000 --- Training Loss:0.000422\n",
      "Epoch: 103/200, Iteration: 5136/10000 --- Training Loss:0.000167\n",
      "Epoch: 103/200, Iteration: 5138/10000 --- Training Loss:0.000150\n",
      "Epoch: 103/200, Iteration: 5140/10000 --- Training Loss:0.000183\n",
      "Epoch: 103/200, Iteration: 5142/10000 --- Training Loss:0.000464\n",
      "Epoch: 103/200, Iteration: 5144/10000 --- Training Loss:0.000239\n",
      "Epoch: 103/200, Iteration: 5146/10000 --- Training Loss:0.000294\n",
      "Epoch: 103/200, Iteration: 5148/10000 --- Training Loss:0.000303\n",
      "Epoch: 103/200, Iteration: 5150/10000 --- Training Loss:0.000167\n",
      "Epoch: 103 finished ! Train Loss: 0.00024, Test Loss: 0.00278\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 104/200, Iteration: 5152/10000 --- Training Loss:0.000177\n",
      "Epoch: 104/200, Iteration: 5154/10000 --- Training Loss:0.000268\n",
      "Epoch: 104/200, Iteration: 5156/10000 --- Training Loss:0.000191\n",
      "Epoch: 104/200, Iteration: 5158/10000 --- Training Loss:0.000167\n",
      "Epoch: 104/200, Iteration: 5160/10000 --- Training Loss:0.000150\n",
      "Epoch: 104/200, Iteration: 5162/10000 --- Training Loss:0.000585\n",
      "Epoch: 104/200, Iteration: 5164/10000 --- Training Loss:0.000354\n",
      "Epoch: 104/200, Iteration: 5166/10000 --- Training Loss:0.000199\n",
      "Epoch: 104/200, Iteration: 5168/10000 --- Training Loss:0.000240\n",
      "Epoch: 104/200, Iteration: 5170/10000 --- Training Loss:0.000234\n",
      "Epoch: 104/200, Iteration: 5172/10000 --- Training Loss:0.000424\n",
      "Epoch: 104/200, Iteration: 5174/10000 --- Training Loss:0.000247\n",
      "Epoch: 104/200, Iteration: 5176/10000 --- Training Loss:0.000198\n",
      "Epoch: 104/200, Iteration: 5178/10000 --- Training Loss:0.000315\n",
      "Epoch: 104/200, Iteration: 5180/10000 --- Training Loss:0.000428\n",
      "Epoch: 104/200, Iteration: 5182/10000 --- Training Loss:0.000333\n",
      "Epoch: 104/200, Iteration: 5184/10000 --- Training Loss:0.000398\n",
      "Epoch: 104/200, Iteration: 5186/10000 --- Training Loss:0.000238\n",
      "Epoch: 104/200, Iteration: 5188/10000 --- Training Loss:0.000137\n",
      "Epoch: 104/200, Iteration: 5190/10000 --- Training Loss:0.000245\n",
      "Epoch: 104/200, Iteration: 5192/10000 --- Training Loss:0.000181\n",
      "Epoch: 104/200, Iteration: 5194/10000 --- Training Loss:0.000185\n",
      "Epoch: 104/200, Iteration: 5196/10000 --- Training Loss:0.000230\n",
      "Epoch: 104/200, Iteration: 5198/10000 --- Training Loss:0.000449\n",
      "Epoch: 104/200, Iteration: 5200/10000 --- Training Loss:0.000205\n",
      "Epoch: 104 finished ! Train Loss: 0.00026, Test Loss: 0.00238\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 105/200, Iteration: 5202/10000 --- Training Loss:0.000244\n",
      "Epoch: 105/200, Iteration: 5204/10000 --- Training Loss:0.000255\n",
      "Epoch: 105/200, Iteration: 5206/10000 --- Training Loss:0.000221\n",
      "Epoch: 105/200, Iteration: 5208/10000 --- Training Loss:0.000278\n",
      "Epoch: 105/200, Iteration: 5210/10000 --- Training Loss:0.000627\n",
      "Epoch: 105/200, Iteration: 5212/10000 --- Training Loss:0.000208\n",
      "Epoch: 105/200, Iteration: 5214/10000 --- Training Loss:0.000201\n",
      "Epoch: 105/200, Iteration: 5216/10000 --- Training Loss:0.000162\n",
      "Epoch: 105/200, Iteration: 5218/10000 --- Training Loss:0.000445\n",
      "Epoch: 105/200, Iteration: 5220/10000 --- Training Loss:0.000247\n",
      "Epoch: 105/200, Iteration: 5222/10000 --- Training Loss:0.000364\n",
      "Epoch: 105/200, Iteration: 5224/10000 --- Training Loss:0.000255\n",
      "Epoch: 105/200, Iteration: 5226/10000 --- Training Loss:0.000135\n",
      "Epoch: 105/200, Iteration: 5228/10000 --- Training Loss:0.000538\n",
      "Epoch: 105/200, Iteration: 5230/10000 --- Training Loss:0.000375\n",
      "Epoch: 105/200, Iteration: 5232/10000 --- Training Loss:0.000128\n",
      "Epoch: 105/200, Iteration: 5234/10000 --- Training Loss:0.000288\n",
      "Epoch: 105/200, Iteration: 5236/10000 --- Training Loss:0.000210\n",
      "Epoch: 105/200, Iteration: 5238/10000 --- Training Loss:0.000283\n",
      "Epoch: 105/200, Iteration: 5240/10000 --- Training Loss:0.000119\n",
      "Epoch: 105/200, Iteration: 5242/10000 --- Training Loss:0.000167\n",
      "Epoch: 105/200, Iteration: 5244/10000 --- Training Loss:0.000161\n",
      "Epoch: 105/200, Iteration: 5246/10000 --- Training Loss:0.000168\n",
      "Epoch: 105/200, Iteration: 5248/10000 --- Training Loss:0.000206\n",
      "Epoch: 105/200, Iteration: 5250/10000 --- Training Loss:0.000360\n",
      "Epoch: 105 finished ! Train Loss: 0.00027, Test Loss: 0.00236\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 106/200, Iteration: 5252/10000 --- Training Loss:0.000236\n",
      "Epoch: 106/200, Iteration: 5254/10000 --- Training Loss:0.000601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 106/200, Iteration: 5256/10000 --- Training Loss:0.000220\n",
      "Epoch: 106/200, Iteration: 5258/10000 --- Training Loss:0.000305\n",
      "Epoch: 106/200, Iteration: 5260/10000 --- Training Loss:0.000194\n",
      "Epoch: 106/200, Iteration: 5262/10000 --- Training Loss:0.000328\n",
      "Epoch: 106/200, Iteration: 5264/10000 --- Training Loss:0.000130\n",
      "Epoch: 106/200, Iteration: 5266/10000 --- Training Loss:0.000183\n",
      "Epoch: 106/200, Iteration: 5268/10000 --- Training Loss:0.000170\n",
      "Epoch: 106/200, Iteration: 5270/10000 --- Training Loss:0.000178\n",
      "Epoch: 106/200, Iteration: 5272/10000 --- Training Loss:0.000155\n",
      "Epoch: 106/200, Iteration: 5274/10000 --- Training Loss:0.000148\n",
      "Epoch: 106/200, Iteration: 5276/10000 --- Training Loss:0.000252\n",
      "Epoch: 106/200, Iteration: 5278/10000 --- Training Loss:0.000236\n",
      "Epoch: 106/200, Iteration: 5280/10000 --- Training Loss:0.000109\n",
      "Epoch: 106/200, Iteration: 5282/10000 --- Training Loss:0.000161\n",
      "Epoch: 106/200, Iteration: 5284/10000 --- Training Loss:0.000200\n",
      "Epoch: 106/200, Iteration: 5286/10000 --- Training Loss:0.000202\n",
      "Epoch: 106/200, Iteration: 5288/10000 --- Training Loss:0.000162\n",
      "Epoch: 106/200, Iteration: 5290/10000 --- Training Loss:0.000170\n",
      "Epoch: 106/200, Iteration: 5292/10000 --- Training Loss:0.000165\n",
      "Epoch: 106/200, Iteration: 5294/10000 --- Training Loss:0.000212\n",
      "Epoch: 106/200, Iteration: 5296/10000 --- Training Loss:0.000416\n",
      "Epoch: 106/200, Iteration: 5298/10000 --- Training Loss:0.000209\n",
      "Epoch: 106/200, Iteration: 5300/10000 --- Training Loss:0.000262\n",
      "Epoch: 106 finished ! Train Loss: 0.00021, Test Loss: 0.00194\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 107/200, Iteration: 5302/10000 --- Training Loss:0.000214\n",
      "Epoch: 107/200, Iteration: 5304/10000 --- Training Loss:0.000211\n",
      "Epoch: 107/200, Iteration: 5306/10000 --- Training Loss:0.000168\n",
      "Epoch: 107/200, Iteration: 5308/10000 --- Training Loss:0.000487\n",
      "Epoch: 107/200, Iteration: 5310/10000 --- Training Loss:0.000512\n",
      "Epoch: 107/200, Iteration: 5312/10000 --- Training Loss:0.000446\n",
      "Epoch: 107/200, Iteration: 5314/10000 --- Training Loss:0.000395\n",
      "Epoch: 107/200, Iteration: 5316/10000 --- Training Loss:0.000392\n",
      "Epoch: 107/200, Iteration: 5318/10000 --- Training Loss:0.000236\n",
      "Epoch: 107/200, Iteration: 5320/10000 --- Training Loss:0.000246\n",
      "Epoch: 107/200, Iteration: 5322/10000 --- Training Loss:0.000272\n",
      "Epoch: 107/200, Iteration: 5324/10000 --- Training Loss:0.000193\n",
      "Epoch: 107/200, Iteration: 5326/10000 --- Training Loss:0.000151\n",
      "Epoch: 107/200, Iteration: 5328/10000 --- Training Loss:0.000177\n",
      "Epoch: 107/200, Iteration: 5330/10000 --- Training Loss:0.000172\n",
      "Epoch: 107/200, Iteration: 5332/10000 --- Training Loss:0.000247\n",
      "Epoch: 107/200, Iteration: 5334/10000 --- Training Loss:0.000217\n",
      "Epoch: 107/200, Iteration: 5336/10000 --- Training Loss:0.000157\n",
      "Epoch: 107/200, Iteration: 5338/10000 --- Training Loss:0.000167\n",
      "Epoch: 107/200, Iteration: 5340/10000 --- Training Loss:0.000210\n",
      "Epoch: 107/200, Iteration: 5342/10000 --- Training Loss:0.000129\n",
      "Epoch: 107/200, Iteration: 5344/10000 --- Training Loss:0.000286\n",
      "Epoch: 107/200, Iteration: 5346/10000 --- Training Loss:0.000199\n",
      "Epoch: 107/200, Iteration: 5348/10000 --- Training Loss:0.000277\n",
      "Epoch: 107/200, Iteration: 5350/10000 --- Training Loss:0.000123\n",
      "Epoch: 107 finished ! Train Loss: 0.00027, Test Loss: 0.00215\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 108/200, Iteration: 5352/10000 --- Training Loss:0.000136\n",
      "Epoch: 108/200, Iteration: 5354/10000 --- Training Loss:0.000254\n",
      "Epoch: 108/200, Iteration: 5356/10000 --- Training Loss:0.000202\n",
      "Epoch: 108/200, Iteration: 5358/10000 --- Training Loss:0.000618\n",
      "Epoch: 108/200, Iteration: 5360/10000 --- Training Loss:0.000648\n",
      "Epoch: 108/200, Iteration: 5362/10000 --- Training Loss:0.000558\n",
      "Epoch: 108/200, Iteration: 5364/10000 --- Training Loss:0.000343\n",
      "Epoch: 108/200, Iteration: 5366/10000 --- Training Loss:0.000286\n",
      "Epoch: 108/200, Iteration: 5368/10000 --- Training Loss:0.000489\n",
      "Epoch: 108/200, Iteration: 5370/10000 --- Training Loss:0.000319\n",
      "Epoch: 108/200, Iteration: 5372/10000 --- Training Loss:0.000289\n",
      "Epoch: 108/200, Iteration: 5374/10000 --- Training Loss:0.000281\n",
      "Epoch: 108/200, Iteration: 5376/10000 --- Training Loss:0.000196\n",
      "Epoch: 108/200, Iteration: 5378/10000 --- Training Loss:0.000444\n",
      "Epoch: 108/200, Iteration: 5380/10000 --- Training Loss:0.000276\n",
      "Epoch: 108/200, Iteration: 5382/10000 --- Training Loss:0.000506\n",
      "Epoch: 108/200, Iteration: 5384/10000 --- Training Loss:0.000266\n",
      "Epoch: 108/200, Iteration: 5386/10000 --- Training Loss:0.000947\n",
      "Epoch: 108/200, Iteration: 5388/10000 --- Training Loss:0.000338\n",
      "Epoch: 108/200, Iteration: 5390/10000 --- Training Loss:0.000387\n",
      "Epoch: 108/200, Iteration: 5392/10000 --- Training Loss:0.000365\n",
      "Epoch: 108/200, Iteration: 5394/10000 --- Training Loss:0.000309\n",
      "Epoch: 108/200, Iteration: 5396/10000 --- Training Loss:0.000249\n",
      "Epoch: 108/200, Iteration: 5398/10000 --- Training Loss:0.000351\n",
      "Epoch: 108/200, Iteration: 5400/10000 --- Training Loss:0.000221\n",
      "Epoch: 108 finished ! Train Loss: 0.00037, Test Loss: 0.00273\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 109/200, Iteration: 5402/10000 --- Training Loss:0.000382\n",
      "Epoch: 109/200, Iteration: 5404/10000 --- Training Loss:0.000312\n",
      "Epoch: 109/200, Iteration: 5406/10000 --- Training Loss:0.000406\n",
      "Epoch: 109/200, Iteration: 5408/10000 --- Training Loss:0.000216\n",
      "Epoch: 109/200, Iteration: 5410/10000 --- Training Loss:0.000452\n",
      "Epoch: 109/200, Iteration: 5412/10000 --- Training Loss:0.000229\n",
      "Epoch: 109/200, Iteration: 5414/10000 --- Training Loss:0.000218\n",
      "Epoch: 109/200, Iteration: 5416/10000 --- Training Loss:0.000228\n",
      "Epoch: 109/200, Iteration: 5418/10000 --- Training Loss:0.000236\n",
      "Epoch: 109/200, Iteration: 5420/10000 --- Training Loss:0.000266\n",
      "Epoch: 109/200, Iteration: 5422/10000 --- Training Loss:0.000433\n",
      "Epoch: 109/200, Iteration: 5424/10000 --- Training Loss:0.000277\n",
      "Epoch: 109/200, Iteration: 5426/10000 --- Training Loss:0.000352\n",
      "Epoch: 109/200, Iteration: 5428/10000 --- Training Loss:0.000165\n",
      "Epoch: 109/200, Iteration: 5430/10000 --- Training Loss:0.000229\n",
      "Epoch: 109/200, Iteration: 5432/10000 --- Training Loss:0.000135\n",
      "Epoch: 109/200, Iteration: 5434/10000 --- Training Loss:0.000318\n",
      "Epoch: 109/200, Iteration: 5436/10000 --- Training Loss:0.000152\n",
      "Epoch: 109/200, Iteration: 5438/10000 --- Training Loss:0.000277\n",
      "Epoch: 109/200, Iteration: 5440/10000 --- Training Loss:0.000175\n",
      "Epoch: 109/200, Iteration: 5442/10000 --- Training Loss:0.000134\n",
      "Epoch: 109/200, Iteration: 5444/10000 --- Training Loss:0.000186\n",
      "Epoch: 109/200, Iteration: 5446/10000 --- Training Loss:0.000296\n",
      "Epoch: 109/200, Iteration: 5448/10000 --- Training Loss:0.000334\n",
      "Epoch: 109/200, Iteration: 5450/10000 --- Training Loss:0.000259\n",
      "Epoch: 109 finished ! Train Loss: 0.00028, Test Loss: 0.00220\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 110/200, Iteration: 5452/10000 --- Training Loss:0.000155\n",
      "Epoch: 110/200, Iteration: 5454/10000 --- Training Loss:0.000482\n",
      "Epoch: 110/200, Iteration: 5456/10000 --- Training Loss:0.000233\n",
      "Epoch: 110/200, Iteration: 5458/10000 --- Training Loss:0.000139\n",
      "Epoch: 110/200, Iteration: 5460/10000 --- Training Loss:0.000192\n",
      "Epoch: 110/200, Iteration: 5462/10000 --- Training Loss:0.000186\n",
      "Epoch: 110/200, Iteration: 5464/10000 --- Training Loss:0.000314\n",
      "Epoch: 110/200, Iteration: 5466/10000 --- Training Loss:0.000283\n",
      "Epoch: 110/200, Iteration: 5468/10000 --- Training Loss:0.000139\n",
      "Epoch: 110/200, Iteration: 5470/10000 --- Training Loss:0.000370\n",
      "Epoch: 110/200, Iteration: 5472/10000 --- Training Loss:0.000258\n",
      "Epoch: 110/200, Iteration: 5474/10000 --- Training Loss:0.000202\n",
      "Epoch: 110/200, Iteration: 5476/10000 --- Training Loss:0.000453\n",
      "Epoch: 110/200, Iteration: 5478/10000 --- Training Loss:0.000260\n",
      "Epoch: 110/200, Iteration: 5480/10000 --- Training Loss:0.000240\n",
      "Epoch: 110/200, Iteration: 5482/10000 --- Training Loss:0.000230\n",
      "Epoch: 110/200, Iteration: 5484/10000 --- Training Loss:0.000145\n",
      "Epoch: 110/200, Iteration: 5486/10000 --- Training Loss:0.000218\n",
      "Epoch: 110/200, Iteration: 5488/10000 --- Training Loss:0.000243\n",
      "Epoch: 110/200, Iteration: 5490/10000 --- Training Loss:0.000124\n",
      "Epoch: 110/200, Iteration: 5492/10000 --- Training Loss:0.000150\n",
      "Epoch: 110/200, Iteration: 5494/10000 --- Training Loss:0.000143\n",
      "Epoch: 110/200, Iteration: 5496/10000 --- Training Loss:0.000119\n",
      "Epoch: 110/200, Iteration: 5498/10000 --- Training Loss:0.000237\n",
      "Epoch: 110/200, Iteration: 5500/10000 --- Training Loss:0.000166\n",
      "Epoch: 110 finished ! Train Loss: 0.00024, Test Loss: 0.00121\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 111/200, Iteration: 5502/10000 --- Training Loss:0.000214\n",
      "Epoch: 111/200, Iteration: 5504/10000 --- Training Loss:0.000192\n",
      "Epoch: 111/200, Iteration: 5506/10000 --- Training Loss:0.000143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 111/200, Iteration: 5508/10000 --- Training Loss:0.000228\n",
      "Epoch: 111/200, Iteration: 5510/10000 --- Training Loss:0.000122\n",
      "Epoch: 111/200, Iteration: 5512/10000 --- Training Loss:0.000114\n",
      "Epoch: 111/200, Iteration: 5514/10000 --- Training Loss:0.000173\n",
      "Epoch: 111/200, Iteration: 5516/10000 --- Training Loss:0.000283\n",
      "Epoch: 111/200, Iteration: 5518/10000 --- Training Loss:0.000120\n",
      "Epoch: 111/200, Iteration: 5520/10000 --- Training Loss:0.000180\n",
      "Epoch: 111/200, Iteration: 5522/10000 --- Training Loss:0.000246\n",
      "Epoch: 111/200, Iteration: 5524/10000 --- Training Loss:0.000099\n",
      "Epoch: 111/200, Iteration: 5526/10000 --- Training Loss:0.000193\n",
      "Epoch: 111/200, Iteration: 5528/10000 --- Training Loss:0.000399\n",
      "Epoch: 111/200, Iteration: 5530/10000 --- Training Loss:0.000411\n",
      "Epoch: 111/200, Iteration: 5532/10000 --- Training Loss:0.000254\n",
      "Epoch: 111/200, Iteration: 5534/10000 --- Training Loss:0.000247\n",
      "Epoch: 111/200, Iteration: 5536/10000 --- Training Loss:0.000265\n",
      "Epoch: 111/200, Iteration: 5538/10000 --- Training Loss:0.000249\n",
      "Epoch: 111/200, Iteration: 5540/10000 --- Training Loss:0.000179\n",
      "Epoch: 111/200, Iteration: 5542/10000 --- Training Loss:0.000378\n",
      "Epoch: 111/200, Iteration: 5544/10000 --- Training Loss:0.000212\n",
      "Epoch: 111/200, Iteration: 5546/10000 --- Training Loss:0.000300\n",
      "Epoch: 111/200, Iteration: 5548/10000 --- Training Loss:0.000306\n",
      "Epoch: 111/200, Iteration: 5550/10000 --- Training Loss:0.000284\n",
      "Epoch: 111 finished ! Train Loss: 0.00024, Test Loss: 0.00265\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 112/200, Iteration: 5552/10000 --- Training Loss:0.000178\n",
      "Epoch: 112/200, Iteration: 5554/10000 --- Training Loss:0.000240\n",
      "Epoch: 112/200, Iteration: 5556/10000 --- Training Loss:0.000355\n",
      "Epoch: 112/200, Iteration: 5558/10000 --- Training Loss:0.000178\n",
      "Epoch: 112/200, Iteration: 5560/10000 --- Training Loss:0.000192\n",
      "Epoch: 112/200, Iteration: 5562/10000 --- Training Loss:0.000444\n",
      "Epoch: 112/200, Iteration: 5564/10000 --- Training Loss:0.000272\n",
      "Epoch: 112/200, Iteration: 5566/10000 --- Training Loss:0.000363\n",
      "Epoch: 112/200, Iteration: 5568/10000 --- Training Loss:0.000186\n",
      "Epoch: 112/200, Iteration: 5570/10000 --- Training Loss:0.000202\n",
      "Epoch: 112/200, Iteration: 5572/10000 --- Training Loss:0.000242\n",
      "Epoch: 112/200, Iteration: 5574/10000 --- Training Loss:0.000172\n",
      "Epoch: 112/200, Iteration: 5576/10000 --- Training Loss:0.000327\n",
      "Epoch: 112/200, Iteration: 5578/10000 --- Training Loss:0.000302\n",
      "Epoch: 112/200, Iteration: 5580/10000 --- Training Loss:0.000245\n",
      "Epoch: 112/200, Iteration: 5582/10000 --- Training Loss:0.000128\n",
      "Epoch: 112/200, Iteration: 5584/10000 --- Training Loss:0.000140\n",
      "Epoch: 112/200, Iteration: 5586/10000 --- Training Loss:0.000358\n",
      "Epoch: 112/200, Iteration: 5588/10000 --- Training Loss:0.000466\n",
      "Epoch: 112/200, Iteration: 5590/10000 --- Training Loss:0.000298\n",
      "Epoch: 112/200, Iteration: 5592/10000 --- Training Loss:0.000199\n",
      "Epoch: 112/200, Iteration: 5594/10000 --- Training Loss:0.000309\n",
      "Epoch: 112/200, Iteration: 5596/10000 --- Training Loss:0.000221\n",
      "Epoch: 112/200, Iteration: 5598/10000 --- Training Loss:0.000137\n",
      "Epoch: 112/200, Iteration: 5600/10000 --- Training Loss:0.000240\n",
      "Epoch: 112 finished ! Train Loss: 0.00026, Test Loss: 0.00200\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 113/200, Iteration: 5602/10000 --- Training Loss:0.000134\n",
      "Epoch: 113/200, Iteration: 5604/10000 --- Training Loss:0.000188\n",
      "Epoch: 113/200, Iteration: 5606/10000 --- Training Loss:0.000231\n",
      "Epoch: 113/200, Iteration: 5608/10000 --- Training Loss:0.000315\n",
      "Epoch: 113/200, Iteration: 5610/10000 --- Training Loss:0.000119\n",
      "Epoch: 113/200, Iteration: 5612/10000 --- Training Loss:0.000164\n",
      "Epoch: 113/200, Iteration: 5614/10000 --- Training Loss:0.000229\n",
      "Epoch: 113/200, Iteration: 5616/10000 --- Training Loss:0.000156\n",
      "Epoch: 113/200, Iteration: 5618/10000 --- Training Loss:0.000257\n",
      "Epoch: 113/200, Iteration: 5620/10000 --- Training Loss:0.000149\n",
      "Epoch: 113/200, Iteration: 5622/10000 --- Training Loss:0.000361\n",
      "Epoch: 113/200, Iteration: 5624/10000 --- Training Loss:0.000345\n",
      "Epoch: 113/200, Iteration: 5626/10000 --- Training Loss:0.000175\n",
      "Epoch: 113/200, Iteration: 5628/10000 --- Training Loss:0.000199\n",
      "Epoch: 113/200, Iteration: 5630/10000 --- Training Loss:0.000173\n",
      "Epoch: 113/200, Iteration: 5632/10000 --- Training Loss:0.000506\n",
      "Epoch: 113/200, Iteration: 5634/10000 --- Training Loss:0.000194\n",
      "Epoch: 113/200, Iteration: 5636/10000 --- Training Loss:0.000194\n",
      "Epoch: 113/200, Iteration: 5638/10000 --- Training Loss:0.000105\n",
      "Epoch: 113/200, Iteration: 5640/10000 --- Training Loss:0.000188\n",
      "Epoch: 113/200, Iteration: 5642/10000 --- Training Loss:0.000149\n",
      "Epoch: 113/200, Iteration: 5644/10000 --- Training Loss:0.000197\n",
      "Epoch: 113/200, Iteration: 5646/10000 --- Training Loss:0.000215\n",
      "Epoch: 113/200, Iteration: 5648/10000 --- Training Loss:0.000218\n",
      "Epoch: 113/200, Iteration: 5650/10000 --- Training Loss:0.000398\n",
      "Epoch: 113 finished ! Train Loss: 0.00023, Test Loss: 0.00221\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 114/200, Iteration: 5652/10000 --- Training Loss:0.000142\n",
      "Epoch: 114/200, Iteration: 5654/10000 --- Training Loss:0.000158\n",
      "Epoch: 114/200, Iteration: 5656/10000 --- Training Loss:0.000189\n",
      "Epoch: 114/200, Iteration: 5658/10000 --- Training Loss:0.000204\n",
      "Epoch: 114/200, Iteration: 5660/10000 --- Training Loss:0.000116\n",
      "Epoch: 114/200, Iteration: 5662/10000 --- Training Loss:0.000320\n",
      "Epoch: 114/200, Iteration: 5664/10000 --- Training Loss:0.001221\n",
      "Epoch: 114/200, Iteration: 5666/10000 --- Training Loss:0.000469\n",
      "Epoch: 114/200, Iteration: 5668/10000 --- Training Loss:0.000359\n",
      "Epoch: 114/200, Iteration: 5670/10000 --- Training Loss:0.000316\n",
      "Epoch: 114/200, Iteration: 5672/10000 --- Training Loss:0.000452\n",
      "Epoch: 114/200, Iteration: 5674/10000 --- Training Loss:0.000254\n",
      "Epoch: 114/200, Iteration: 5676/10000 --- Training Loss:0.000339\n",
      "Epoch: 114/200, Iteration: 5678/10000 --- Training Loss:0.000208\n",
      "Epoch: 114/200, Iteration: 5680/10000 --- Training Loss:0.000286\n",
      "Epoch: 114/200, Iteration: 5682/10000 --- Training Loss:0.000306\n",
      "Epoch: 114/200, Iteration: 5684/10000 --- Training Loss:0.000236\n",
      "Epoch: 114/200, Iteration: 5686/10000 --- Training Loss:0.000165\n",
      "Epoch: 114/200, Iteration: 5688/10000 --- Training Loss:0.000217\n",
      "Epoch: 114/200, Iteration: 5690/10000 --- Training Loss:0.000195\n",
      "Epoch: 114/200, Iteration: 5692/10000 --- Training Loss:0.000280\n",
      "Epoch: 114/200, Iteration: 5694/10000 --- Training Loss:0.000363\n",
      "Epoch: 114/200, Iteration: 5696/10000 --- Training Loss:0.000245\n",
      "Epoch: 114/200, Iteration: 5698/10000 --- Training Loss:0.000184\n",
      "Epoch: 114/200, Iteration: 5700/10000 --- Training Loss:0.000177\n",
      "Epoch: 114 finished ! Train Loss: 0.00029, Test Loss: 0.00164\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 115/200, Iteration: 5702/10000 --- Training Loss:0.000138\n",
      "Epoch: 115/200, Iteration: 5704/10000 --- Training Loss:0.000402\n",
      "Epoch: 115/200, Iteration: 5706/10000 --- Training Loss:0.000402\n",
      "Epoch: 115/200, Iteration: 5708/10000 --- Training Loss:0.000256\n",
      "Epoch: 115/200, Iteration: 5710/10000 --- Training Loss:0.000152\n",
      "Epoch: 115/200, Iteration: 5712/10000 --- Training Loss:0.000160\n",
      "Epoch: 115/200, Iteration: 5714/10000 --- Training Loss:0.000151\n",
      "Epoch: 115/200, Iteration: 5716/10000 --- Training Loss:0.000168\n",
      "Epoch: 115/200, Iteration: 5718/10000 --- Training Loss:0.000216\n",
      "Epoch: 115/200, Iteration: 5720/10000 --- Training Loss:0.000272\n",
      "Epoch: 115/200, Iteration: 5722/10000 --- Training Loss:0.000215\n",
      "Epoch: 115/200, Iteration: 5724/10000 --- Training Loss:0.000148\n",
      "Epoch: 115/200, Iteration: 5726/10000 --- Training Loss:0.000178\n",
      "Epoch: 115/200, Iteration: 5728/10000 --- Training Loss:0.000103\n",
      "Epoch: 115/200, Iteration: 5730/10000 --- Training Loss:0.000135\n",
      "Epoch: 115/200, Iteration: 5732/10000 --- Training Loss:0.000243\n",
      "Epoch: 115/200, Iteration: 5734/10000 --- Training Loss:0.000234\n",
      "Epoch: 115/200, Iteration: 5736/10000 --- Training Loss:0.000437\n",
      "Epoch: 115/200, Iteration: 5738/10000 --- Training Loss:0.000160\n",
      "Epoch: 115/200, Iteration: 5740/10000 --- Training Loss:0.000142\n",
      "Epoch: 115/200, Iteration: 5742/10000 --- Training Loss:0.000080\n",
      "Epoch: 115/200, Iteration: 5744/10000 --- Training Loss:0.000218\n",
      "Epoch: 115/200, Iteration: 5746/10000 --- Training Loss:0.000129\n",
      "Epoch: 115/200, Iteration: 5748/10000 --- Training Loss:0.000151\n",
      "Epoch: 115/200, Iteration: 5750/10000 --- Training Loss:0.000161\n",
      "Epoch: 115 finished ! Train Loss: 0.00021, Test Loss: 0.00160\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 116/200, Iteration: 5752/10000 --- Training Loss:0.000096\n",
      "Epoch: 116/200, Iteration: 5754/10000 --- Training Loss:0.000176\n",
      "Epoch: 116/200, Iteration: 5756/10000 --- Training Loss:0.000401\n",
      "Epoch: 116/200, Iteration: 5758/10000 --- Training Loss:0.001092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116/200, Iteration: 5760/10000 --- Training Loss:0.000213\n",
      "Epoch: 116/200, Iteration: 5762/10000 --- Training Loss:0.000151\n",
      "Epoch: 116/200, Iteration: 5764/10000 --- Training Loss:0.000365\n",
      "Epoch: 116/200, Iteration: 5766/10000 --- Training Loss:0.000231\n",
      "Epoch: 116/200, Iteration: 5768/10000 --- Training Loss:0.000263\n",
      "Epoch: 116/200, Iteration: 5770/10000 --- Training Loss:0.000235\n",
      "Epoch: 116/200, Iteration: 5772/10000 --- Training Loss:0.000143\n",
      "Epoch: 116/200, Iteration: 5774/10000 --- Training Loss:0.000202\n",
      "Epoch: 116/200, Iteration: 5776/10000 --- Training Loss:0.000245\n",
      "Epoch: 116/200, Iteration: 5778/10000 --- Training Loss:0.000145\n",
      "Epoch: 116/200, Iteration: 5780/10000 --- Training Loss:0.000210\n",
      "Epoch: 116/200, Iteration: 5782/10000 --- Training Loss:0.000148\n",
      "Epoch: 116/200, Iteration: 5784/10000 --- Training Loss:0.000105\n",
      "Epoch: 116/200, Iteration: 5786/10000 --- Training Loss:0.000224\n",
      "Epoch: 116/200, Iteration: 5788/10000 --- Training Loss:0.000209\n",
      "Epoch: 116/200, Iteration: 5790/10000 --- Training Loss:0.000322\n",
      "Epoch: 116/200, Iteration: 5792/10000 --- Training Loss:0.000163\n",
      "Epoch: 116/200, Iteration: 5794/10000 --- Training Loss:0.000161\n",
      "Epoch: 116/200, Iteration: 5796/10000 --- Training Loss:0.000162\n",
      "Epoch: 116/200, Iteration: 5798/10000 --- Training Loss:0.000130\n",
      "Epoch: 116/200, Iteration: 5800/10000 --- Training Loss:0.000218\n",
      "Epoch: 116 finished ! Train Loss: 0.00021, Test Loss: 0.00165\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 117/200, Iteration: 5802/10000 --- Training Loss:0.000179\n",
      "Epoch: 117/200, Iteration: 5804/10000 --- Training Loss:0.000154\n",
      "Epoch: 117/200, Iteration: 5806/10000 --- Training Loss:0.000147\n",
      "Epoch: 117/200, Iteration: 5808/10000 --- Training Loss:0.002015\n",
      "Epoch: 117/200, Iteration: 5810/10000 --- Training Loss:0.000186\n",
      "Epoch: 117/200, Iteration: 5812/10000 --- Training Loss:0.000244\n",
      "Epoch: 117/200, Iteration: 5814/10000 --- Training Loss:0.000302\n",
      "Epoch: 117/200, Iteration: 5816/10000 --- Training Loss:0.000215\n",
      "Epoch: 117/200, Iteration: 5818/10000 --- Training Loss:0.000157\n",
      "Epoch: 117/200, Iteration: 5820/10000 --- Training Loss:0.000309\n",
      "Epoch: 117/200, Iteration: 5822/10000 --- Training Loss:0.000153\n",
      "Epoch: 117/200, Iteration: 5824/10000 --- Training Loss:0.000120\n",
      "Epoch: 117/200, Iteration: 5826/10000 --- Training Loss:0.000385\n",
      "Epoch: 117/200, Iteration: 5828/10000 --- Training Loss:0.000178\n",
      "Epoch: 117/200, Iteration: 5830/10000 --- Training Loss:0.000202\n",
      "Epoch: 117/200, Iteration: 5832/10000 --- Training Loss:0.000195\n",
      "Epoch: 117/200, Iteration: 5834/10000 --- Training Loss:0.000636\n",
      "Epoch: 117/200, Iteration: 5836/10000 --- Training Loss:0.000272\n",
      "Epoch: 117/200, Iteration: 5838/10000 --- Training Loss:0.000371\n",
      "Epoch: 117/200, Iteration: 5840/10000 --- Training Loss:0.000242\n",
      "Epoch: 117/200, Iteration: 5842/10000 --- Training Loss:0.000117\n",
      "Epoch: 117/200, Iteration: 5844/10000 --- Training Loss:0.000276\n",
      "Epoch: 117/200, Iteration: 5846/10000 --- Training Loss:0.000196\n",
      "Epoch: 117/200, Iteration: 5848/10000 --- Training Loss:0.000195\n",
      "Epoch: 117/200, Iteration: 5850/10000 --- Training Loss:0.000180\n",
      "Epoch: 117 finished ! Train Loss: 0.00025, Test Loss: 0.00170\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 118/200, Iteration: 5852/10000 --- Training Loss:0.000237\n",
      "Epoch: 118/200, Iteration: 5854/10000 --- Training Loss:0.000157\n",
      "Epoch: 118/200, Iteration: 5856/10000 --- Training Loss:0.000341\n",
      "Epoch: 118/200, Iteration: 5858/10000 --- Training Loss:0.000176\n",
      "Epoch: 118/200, Iteration: 5860/10000 --- Training Loss:0.000168\n",
      "Epoch: 118/200, Iteration: 5862/10000 --- Training Loss:0.000163\n",
      "Epoch: 118/200, Iteration: 5864/10000 --- Training Loss:0.000147\n",
      "Epoch: 118/200, Iteration: 5866/10000 --- Training Loss:0.000230\n",
      "Epoch: 118/200, Iteration: 5868/10000 --- Training Loss:0.000300\n",
      "Epoch: 118/200, Iteration: 5870/10000 --- Training Loss:0.000136\n",
      "Epoch: 118/200, Iteration: 5872/10000 --- Training Loss:0.000091\n",
      "Epoch: 118/200, Iteration: 5874/10000 --- Training Loss:0.000192\n",
      "Epoch: 118/200, Iteration: 5876/10000 --- Training Loss:0.000131\n",
      "Epoch: 118/200, Iteration: 5878/10000 --- Training Loss:0.000126\n",
      "Epoch: 118/200, Iteration: 5880/10000 --- Training Loss:0.000125\n",
      "Epoch: 118/200, Iteration: 5882/10000 --- Training Loss:0.000164\n",
      "Epoch: 118/200, Iteration: 5884/10000 --- Training Loss:0.000207\n",
      "Epoch: 118/200, Iteration: 5886/10000 --- Training Loss:0.000136\n",
      "Epoch: 118/200, Iteration: 5888/10000 --- Training Loss:0.000137\n",
      "Epoch: 118/200, Iteration: 5890/10000 --- Training Loss:0.000141\n",
      "Epoch: 118/200, Iteration: 5892/10000 --- Training Loss:0.000190\n",
      "Epoch: 118/200, Iteration: 5894/10000 --- Training Loss:0.000175\n",
      "Epoch: 118/200, Iteration: 5896/10000 --- Training Loss:0.000148\n",
      "Epoch: 118/200, Iteration: 5898/10000 --- Training Loss:0.000140\n",
      "Epoch: 118/200, Iteration: 5900/10000 --- Training Loss:0.000207\n",
      "Epoch: 118 finished ! Train Loss: 0.00018, Test Loss: 0.00181\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 119/200, Iteration: 5902/10000 --- Training Loss:0.000142\n",
      "Epoch: 119/200, Iteration: 5904/10000 --- Training Loss:0.000180\n",
      "Epoch: 119/200, Iteration: 5906/10000 --- Training Loss:0.000119\n",
      "Epoch: 119/200, Iteration: 5908/10000 --- Training Loss:0.000223\n",
      "Epoch: 119/200, Iteration: 5910/10000 --- Training Loss:0.000178\n",
      "Epoch: 119/200, Iteration: 5912/10000 --- Training Loss:0.000153\n",
      "Epoch: 119/200, Iteration: 5914/10000 --- Training Loss:0.000105\n",
      "Epoch: 119/200, Iteration: 5916/10000 --- Training Loss:0.000190\n",
      "Epoch: 119/200, Iteration: 5918/10000 --- Training Loss:0.000188\n",
      "Epoch: 119/200, Iteration: 5920/10000 --- Training Loss:0.000234\n",
      "Epoch: 119/200, Iteration: 5922/10000 --- Training Loss:0.000179\n",
      "Epoch: 119/200, Iteration: 5924/10000 --- Training Loss:0.000104\n",
      "Epoch: 119/200, Iteration: 5926/10000 --- Training Loss:0.000150\n",
      "Epoch: 119/200, Iteration: 5928/10000 --- Training Loss:0.000123\n",
      "Epoch: 119/200, Iteration: 5930/10000 --- Training Loss:0.000146\n",
      "Epoch: 119/200, Iteration: 5932/10000 --- Training Loss:0.000094\n",
      "Epoch: 119/200, Iteration: 5934/10000 --- Training Loss:0.000179\n",
      "Epoch: 119/200, Iteration: 5936/10000 --- Training Loss:0.000116\n",
      "Epoch: 119/200, Iteration: 5938/10000 --- Training Loss:0.000130\n",
      "Epoch: 119/200, Iteration: 5940/10000 --- Training Loss:0.000142\n",
      "Epoch: 119/200, Iteration: 5942/10000 --- Training Loss:0.000144\n",
      "Epoch: 119/200, Iteration: 5944/10000 --- Training Loss:0.000274\n",
      "Epoch: 119/200, Iteration: 5946/10000 --- Training Loss:0.000127\n",
      "Epoch: 119/200, Iteration: 5948/10000 --- Training Loss:0.000423\n",
      "Epoch: 119/200, Iteration: 5950/10000 --- Training Loss:0.000120\n",
      "Epoch: 119 finished ! Train Loss: 0.00017, Test Loss: 0.00153\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 120/200, Iteration: 5952/10000 --- Training Loss:0.000173\n",
      "Epoch: 120/200, Iteration: 5954/10000 --- Training Loss:0.000345\n",
      "Epoch: 120/200, Iteration: 5956/10000 --- Training Loss:0.000135\n",
      "Epoch: 120/200, Iteration: 5958/10000 --- Training Loss:0.000209\n",
      "Epoch: 120/200, Iteration: 5960/10000 --- Training Loss:0.000157\n",
      "Epoch: 120/200, Iteration: 5962/10000 --- Training Loss:0.000172\n",
      "Epoch: 120/200, Iteration: 5964/10000 --- Training Loss:0.000199\n",
      "Epoch: 120/200, Iteration: 5966/10000 --- Training Loss:0.000101\n",
      "Epoch: 120/200, Iteration: 5968/10000 --- Training Loss:0.000360\n",
      "Epoch: 120/200, Iteration: 5970/10000 --- Training Loss:0.000091\n",
      "Epoch: 120/200, Iteration: 5972/10000 --- Training Loss:0.000162\n",
      "Epoch: 120/200, Iteration: 5974/10000 --- Training Loss:0.000125\n",
      "Epoch: 120/200, Iteration: 5976/10000 --- Training Loss:0.000108\n",
      "Epoch: 120/200, Iteration: 5978/10000 --- Training Loss:0.000171\n",
      "Epoch: 120/200, Iteration: 5980/10000 --- Training Loss:0.000265\n",
      "Epoch: 120/200, Iteration: 5982/10000 --- Training Loss:0.000431\n",
      "Epoch: 120/200, Iteration: 5984/10000 --- Training Loss:0.000290\n",
      "Epoch: 120/200, Iteration: 5986/10000 --- Training Loss:0.000151\n",
      "Epoch: 120/200, Iteration: 5988/10000 --- Training Loss:0.000229\n",
      "Epoch: 120/200, Iteration: 5990/10000 --- Training Loss:0.000524\n",
      "Epoch: 120/200, Iteration: 5992/10000 --- Training Loss:0.000368\n",
      "Epoch: 120/200, Iteration: 5994/10000 --- Training Loss:0.000671\n",
      "Epoch: 120/200, Iteration: 5996/10000 --- Training Loss:0.000475\n",
      "Epoch: 120/200, Iteration: 5998/10000 --- Training Loss:0.000445\n",
      "Epoch: 120/200, Iteration: 6000/10000 --- Training Loss:0.000360\n",
      "Epoch: 120 finished ! Train Loss: 0.00026, Test Loss: 0.00319\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 121/200, Iteration: 6002/10000 --- Training Loss:0.000261\n",
      "Epoch: 121/200, Iteration: 6004/10000 --- Training Loss:0.000311\n",
      "Epoch: 121/200, Iteration: 6006/10000 --- Training Loss:0.000352\n",
      "Epoch: 121/200, Iteration: 6008/10000 --- Training Loss:0.000240\n",
      "Epoch: 121/200, Iteration: 6010/10000 --- Training Loss:0.000176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121/200, Iteration: 6012/10000 --- Training Loss:0.000585\n",
      "Epoch: 121/200, Iteration: 6014/10000 --- Training Loss:0.000401\n",
      "Epoch: 121/200, Iteration: 6016/10000 --- Training Loss:0.000597\n",
      "Epoch: 121/200, Iteration: 6018/10000 --- Training Loss:0.000225\n",
      "Epoch: 121/200, Iteration: 6020/10000 --- Training Loss:0.000316\n",
      "Epoch: 121/200, Iteration: 6022/10000 --- Training Loss:0.000349\n",
      "Epoch: 121/200, Iteration: 6024/10000 --- Training Loss:0.000129\n",
      "Epoch: 121/200, Iteration: 6026/10000 --- Training Loss:0.000320\n",
      "Epoch: 121/200, Iteration: 6028/10000 --- Training Loss:0.000337\n",
      "Epoch: 121/200, Iteration: 6030/10000 --- Training Loss:0.000296\n",
      "Epoch: 121/200, Iteration: 6032/10000 --- Training Loss:0.000398\n",
      "Epoch: 121/200, Iteration: 6034/10000 --- Training Loss:0.000439\n",
      "Epoch: 121/200, Iteration: 6036/10000 --- Training Loss:0.000375\n",
      "Epoch: 121/200, Iteration: 6038/10000 --- Training Loss:0.000333\n",
      "Epoch: 121/200, Iteration: 6040/10000 --- Training Loss:0.000239\n",
      "Epoch: 121/200, Iteration: 6042/10000 --- Training Loss:0.000296\n",
      "Epoch: 121/200, Iteration: 6044/10000 --- Training Loss:0.000202\n",
      "Epoch: 121/200, Iteration: 6046/10000 --- Training Loss:0.000224\n",
      "Epoch: 121/200, Iteration: 6048/10000 --- Training Loss:0.000210\n",
      "Epoch: 121/200, Iteration: 6050/10000 --- Training Loss:0.000292\n",
      "Epoch: 121 finished ! Train Loss: 0.00037, Test Loss: 0.00154\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 122/200, Iteration: 6052/10000 --- Training Loss:0.000266\n",
      "Epoch: 122/200, Iteration: 6054/10000 --- Training Loss:0.000194\n",
      "Epoch: 122/200, Iteration: 6056/10000 --- Training Loss:0.000196\n",
      "Epoch: 122/200, Iteration: 6058/10000 --- Training Loss:0.000244\n",
      "Epoch: 122/200, Iteration: 6060/10000 --- Training Loss:0.000178\n",
      "Epoch: 122/200, Iteration: 6062/10000 --- Training Loss:0.000243\n",
      "Epoch: 122/200, Iteration: 6064/10000 --- Training Loss:0.000185\n",
      "Epoch: 122/200, Iteration: 6066/10000 --- Training Loss:0.000129\n",
      "Epoch: 122/200, Iteration: 6068/10000 --- Training Loss:0.000301\n",
      "Epoch: 122/200, Iteration: 6070/10000 --- Training Loss:0.000249\n",
      "Epoch: 122/200, Iteration: 6072/10000 --- Training Loss:0.000252\n",
      "Epoch: 122/200, Iteration: 6074/10000 --- Training Loss:0.000227\n",
      "Epoch: 122/200, Iteration: 6076/10000 --- Training Loss:0.000423\n",
      "Epoch: 122/200, Iteration: 6078/10000 --- Training Loss:0.000173\n",
      "Epoch: 122/200, Iteration: 6080/10000 --- Training Loss:0.000164\n",
      "Epoch: 122/200, Iteration: 6082/10000 --- Training Loss:0.000262\n",
      "Epoch: 122/200, Iteration: 6084/10000 --- Training Loss:0.000254\n",
      "Epoch: 122/200, Iteration: 6086/10000 --- Training Loss:0.000177\n",
      "Epoch: 122/200, Iteration: 6088/10000 --- Training Loss:0.000280\n",
      "Epoch: 122/200, Iteration: 6090/10000 --- Training Loss:0.000277\n",
      "Epoch: 122/200, Iteration: 6092/10000 --- Training Loss:0.000346\n",
      "Epoch: 122/200, Iteration: 6094/10000 --- Training Loss:0.000125\n",
      "Epoch: 122/200, Iteration: 6096/10000 --- Training Loss:0.000300\n",
      "Epoch: 122/200, Iteration: 6098/10000 --- Training Loss:0.000366\n",
      "Epoch: 122/200, Iteration: 6100/10000 --- Training Loss:0.000190\n",
      "Epoch: 122 finished ! Train Loss: 0.00024, Test Loss: 0.00235\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 123/200, Iteration: 6102/10000 --- Training Loss:0.000206\n",
      "Epoch: 123/200, Iteration: 6104/10000 --- Training Loss:0.000138\n",
      "Epoch: 123/200, Iteration: 6106/10000 --- Training Loss:0.000339\n",
      "Epoch: 123/200, Iteration: 6108/10000 --- Training Loss:0.000273\n",
      "Epoch: 123/200, Iteration: 6110/10000 --- Training Loss:0.000253\n",
      "Epoch: 123/200, Iteration: 6112/10000 --- Training Loss:0.000198\n",
      "Epoch: 123/200, Iteration: 6114/10000 --- Training Loss:0.000291\n",
      "Epoch: 123/200, Iteration: 6116/10000 --- Training Loss:0.000188\n",
      "Epoch: 123/200, Iteration: 6118/10000 --- Training Loss:0.000155\n",
      "Epoch: 123/200, Iteration: 6120/10000 --- Training Loss:0.000304\n",
      "Epoch: 123/200, Iteration: 6122/10000 --- Training Loss:0.000095\n",
      "Epoch: 123/200, Iteration: 6124/10000 --- Training Loss:0.000123\n",
      "Epoch: 123/200, Iteration: 6126/10000 --- Training Loss:0.000141\n",
      "Epoch: 123/200, Iteration: 6128/10000 --- Training Loss:0.000256\n",
      "Epoch: 123/200, Iteration: 6130/10000 --- Training Loss:0.000102\n",
      "Epoch: 123/200, Iteration: 6132/10000 --- Training Loss:0.000305\n",
      "Epoch: 123/200, Iteration: 6134/10000 --- Training Loss:0.000208\n",
      "Epoch: 123/200, Iteration: 6136/10000 --- Training Loss:0.000146\n",
      "Epoch: 123/200, Iteration: 6138/10000 --- Training Loss:0.000392\n",
      "Epoch: 123/200, Iteration: 6140/10000 --- Training Loss:0.000125\n",
      "Epoch: 123/200, Iteration: 6142/10000 --- Training Loss:0.000368\n",
      "Epoch: 123/200, Iteration: 6144/10000 --- Training Loss:0.000198\n",
      "Epoch: 123/200, Iteration: 6146/10000 --- Training Loss:0.000162\n",
      "Epoch: 123/200, Iteration: 6148/10000 --- Training Loss:0.000267\n",
      "Epoch: 123/200, Iteration: 6150/10000 --- Training Loss:0.000188\n",
      "Epoch: 123 finished ! Train Loss: 0.00020, Test Loss: 0.00242\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 124/200, Iteration: 6152/10000 --- Training Loss:0.000133\n",
      "Epoch: 124/200, Iteration: 6154/10000 --- Training Loss:0.000164\n",
      "Epoch: 124/200, Iteration: 6156/10000 --- Training Loss:0.000286\n",
      "Epoch: 124/200, Iteration: 6158/10000 --- Training Loss:0.000109\n",
      "Epoch: 124/200, Iteration: 6160/10000 --- Training Loss:0.000176\n",
      "Epoch: 124/200, Iteration: 6162/10000 --- Training Loss:0.000105\n",
      "Epoch: 124/200, Iteration: 6164/10000 --- Training Loss:0.000196\n",
      "Epoch: 124/200, Iteration: 6166/10000 --- Training Loss:0.000200\n",
      "Epoch: 124/200, Iteration: 6168/10000 --- Training Loss:0.000106\n",
      "Epoch: 124/200, Iteration: 6170/10000 --- Training Loss:0.000107\n",
      "Epoch: 124/200, Iteration: 6172/10000 --- Training Loss:0.000160\n",
      "Epoch: 124/200, Iteration: 6174/10000 --- Training Loss:0.000430\n",
      "Epoch: 124/200, Iteration: 6176/10000 --- Training Loss:0.000229\n",
      "Epoch: 124/200, Iteration: 6178/10000 --- Training Loss:0.000428\n",
      "Epoch: 124/200, Iteration: 6180/10000 --- Training Loss:0.000237\n",
      "Epoch: 124/200, Iteration: 6182/10000 --- Training Loss:0.000237\n",
      "Epoch: 124/200, Iteration: 6184/10000 --- Training Loss:0.000257\n",
      "Epoch: 124/200, Iteration: 6186/10000 --- Training Loss:0.000323\n",
      "Epoch: 124/200, Iteration: 6188/10000 --- Training Loss:0.000261\n",
      "Epoch: 124/200, Iteration: 6190/10000 --- Training Loss:0.000524\n",
      "Epoch: 124/200, Iteration: 6192/10000 --- Training Loss:0.000347\n",
      "Epoch: 124/200, Iteration: 6194/10000 --- Training Loss:0.000269\n",
      "Epoch: 124/200, Iteration: 6196/10000 --- Training Loss:0.000186\n",
      "Epoch: 124/200, Iteration: 6198/10000 --- Training Loss:0.000648\n",
      "Epoch: 124/200, Iteration: 6200/10000 --- Training Loss:0.000223\n",
      "Epoch: 124 finished ! Train Loss: 0.00022, Test Loss: 0.00116\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 62 percent completed\n",
      "Epoch: 125/200, Iteration: 6202/10000 --- Training Loss:0.000167\n",
      "Epoch: 125/200, Iteration: 6204/10000 --- Training Loss:0.000215\n",
      "Epoch: 125/200, Iteration: 6206/10000 --- Training Loss:0.000149\n",
      "Epoch: 125/200, Iteration: 6208/10000 --- Training Loss:0.000195\n",
      "Epoch: 125/200, Iteration: 6210/10000 --- Training Loss:0.000200\n",
      "Epoch: 125/200, Iteration: 6212/10000 --- Training Loss:0.000114\n",
      "Epoch: 125/200, Iteration: 6214/10000 --- Training Loss:0.000113\n",
      "Epoch: 125/200, Iteration: 6216/10000 --- Training Loss:0.000122\n",
      "Epoch: 125/200, Iteration: 6218/10000 --- Training Loss:0.000194\n",
      "Epoch: 125/200, Iteration: 6220/10000 --- Training Loss:0.000217\n",
      "Epoch: 125/200, Iteration: 6222/10000 --- Training Loss:0.000145\n",
      "Epoch: 125/200, Iteration: 6224/10000 --- Training Loss:0.000229\n",
      "Epoch: 125/200, Iteration: 6226/10000 --- Training Loss:0.000210\n",
      "Epoch: 125/200, Iteration: 6228/10000 --- Training Loss:0.000107\n",
      "Epoch: 125/200, Iteration: 6230/10000 --- Training Loss:0.000165\n",
      "Epoch: 125/200, Iteration: 6232/10000 --- Training Loss:0.000156\n",
      "Epoch: 125/200, Iteration: 6234/10000 --- Training Loss:0.000091\n",
      "Epoch: 125/200, Iteration: 6236/10000 --- Training Loss:0.000321\n",
      "Epoch: 125/200, Iteration: 6238/10000 --- Training Loss:0.000106\n",
      "Epoch: 125/200, Iteration: 6240/10000 --- Training Loss:0.000244\n",
      "Epoch: 125/200, Iteration: 6242/10000 --- Training Loss:0.000088\n",
      "Epoch: 125/200, Iteration: 6244/10000 --- Training Loss:0.000247\n",
      "Epoch: 125/200, Iteration: 6246/10000 --- Training Loss:0.000195\n",
      "Epoch: 125/200, Iteration: 6248/10000 --- Training Loss:0.000094\n",
      "Epoch: 125/200, Iteration: 6250/10000 --- Training Loss:0.000223\n",
      "Epoch: 125 finished ! Train Loss: 0.00017, Test Loss: 0.00275\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 126/200, Iteration: 6252/10000 --- Training Loss:0.000117\n",
      "Epoch: 126/200, Iteration: 6254/10000 --- Training Loss:0.000185\n",
      "Epoch: 126/200, Iteration: 6256/10000 --- Training Loss:0.000147\n",
      "Epoch: 126/200, Iteration: 6258/10000 --- Training Loss:0.000110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 126/200, Iteration: 6260/10000 --- Training Loss:0.000115\n",
      "Epoch: 126/200, Iteration: 6262/10000 --- Training Loss:0.000190\n",
      "Epoch: 126/200, Iteration: 6264/10000 --- Training Loss:0.000109\n",
      "Epoch: 126/200, Iteration: 6266/10000 --- Training Loss:0.000230\n",
      "Epoch: 126/200, Iteration: 6268/10000 --- Training Loss:0.000154\n",
      "Epoch: 126/200, Iteration: 6270/10000 --- Training Loss:0.000136\n",
      "Epoch: 126/200, Iteration: 6272/10000 --- Training Loss:0.000190\n",
      "Epoch: 126/200, Iteration: 6274/10000 --- Training Loss:0.000167\n",
      "Epoch: 126/200, Iteration: 6276/10000 --- Training Loss:0.000194\n",
      "Epoch: 126/200, Iteration: 6278/10000 --- Training Loss:0.000205\n",
      "Epoch: 126/200, Iteration: 6280/10000 --- Training Loss:0.000184\n",
      "Epoch: 126/200, Iteration: 6282/10000 --- Training Loss:0.000139\n",
      "Epoch: 126/200, Iteration: 6284/10000 --- Training Loss:0.000178\n",
      "Epoch: 126/200, Iteration: 6286/10000 --- Training Loss:0.000141\n",
      "Epoch: 126/200, Iteration: 6288/10000 --- Training Loss:0.000098\n",
      "Epoch: 126/200, Iteration: 6290/10000 --- Training Loss:0.000121\n",
      "Epoch: 126/200, Iteration: 6292/10000 --- Training Loss:0.000148\n",
      "Epoch: 126/200, Iteration: 6294/10000 --- Training Loss:0.000156\n",
      "Epoch: 126/200, Iteration: 6296/10000 --- Training Loss:0.000160\n",
      "Epoch: 126/200, Iteration: 6298/10000 --- Training Loss:0.000205\n",
      "Epoch: 126/200, Iteration: 6300/10000 --- Training Loss:0.000159\n",
      "Epoch: 126 finished ! Train Loss: 0.00017, Test Loss: 0.00160\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 127/200, Iteration: 6302/10000 --- Training Loss:0.000136\n",
      "Epoch: 127/200, Iteration: 6304/10000 --- Training Loss:0.000156\n",
      "Epoch: 127/200, Iteration: 6306/10000 --- Training Loss:0.000182\n",
      "Epoch: 127/200, Iteration: 6308/10000 --- Training Loss:0.000130\n",
      "Epoch: 127/200, Iteration: 6310/10000 --- Training Loss:0.000174\n",
      "Epoch: 127/200, Iteration: 6312/10000 --- Training Loss:0.000163\n",
      "Epoch: 127/200, Iteration: 6314/10000 --- Training Loss:0.000196\n",
      "Epoch: 127/200, Iteration: 6316/10000 --- Training Loss:0.000245\n",
      "Epoch: 127/200, Iteration: 6318/10000 --- Training Loss:0.000169\n",
      "Epoch: 127/200, Iteration: 6320/10000 --- Training Loss:0.000353\n",
      "Epoch: 127/200, Iteration: 6322/10000 --- Training Loss:0.000116\n",
      "Epoch: 127/200, Iteration: 6324/10000 --- Training Loss:0.000154\n",
      "Epoch: 127/200, Iteration: 6326/10000 --- Training Loss:0.000097\n",
      "Epoch: 127/200, Iteration: 6328/10000 --- Training Loss:0.000135\n",
      "Epoch: 127/200, Iteration: 6330/10000 --- Training Loss:0.000116\n",
      "Epoch: 127/200, Iteration: 6332/10000 --- Training Loss:0.000173\n",
      "Epoch: 127/200, Iteration: 6334/10000 --- Training Loss:0.000165\n",
      "Epoch: 127/200, Iteration: 6336/10000 --- Training Loss:0.000178\n",
      "Epoch: 127/200, Iteration: 6338/10000 --- Training Loss:0.000199\n",
      "Epoch: 127/200, Iteration: 6340/10000 --- Training Loss:0.000133\n",
      "Epoch: 127/200, Iteration: 6342/10000 --- Training Loss:0.000143\n",
      "Epoch: 127/200, Iteration: 6344/10000 --- Training Loss:0.000131\n",
      "Epoch: 127/200, Iteration: 6346/10000 --- Training Loss:0.000194\n",
      "Epoch: 127/200, Iteration: 6348/10000 --- Training Loss:0.000175\n",
      "Epoch: 127/200, Iteration: 6350/10000 --- Training Loss:0.000183\n",
      "Epoch: 127 finished ! Train Loss: 0.00017, Test Loss: 0.00157\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 128/200, Iteration: 6352/10000 --- Training Loss:0.000163\n",
      "Epoch: 128/200, Iteration: 6354/10000 --- Training Loss:0.000219\n",
      "Epoch: 128/200, Iteration: 6356/10000 --- Training Loss:0.000118\n",
      "Epoch: 128/200, Iteration: 6358/10000 --- Training Loss:0.000141\n",
      "Epoch: 128/200, Iteration: 6360/10000 --- Training Loss:0.000125\n",
      "Epoch: 128/200, Iteration: 6362/10000 --- Training Loss:0.000126\n",
      "Epoch: 128/200, Iteration: 6364/10000 --- Training Loss:0.000182\n",
      "Epoch: 128/200, Iteration: 6366/10000 --- Training Loss:0.000122\n",
      "Epoch: 128/200, Iteration: 6368/10000 --- Training Loss:0.000140\n",
      "Epoch: 128/200, Iteration: 6370/10000 --- Training Loss:0.000086\n",
      "Epoch: 128/200, Iteration: 6372/10000 --- Training Loss:0.000237\n",
      "Epoch: 128/200, Iteration: 6374/10000 --- Training Loss:0.000192\n",
      "Epoch: 128/200, Iteration: 6376/10000 --- Training Loss:0.000187\n",
      "Epoch: 128/200, Iteration: 6378/10000 --- Training Loss:0.000174\n",
      "Epoch: 128/200, Iteration: 6380/10000 --- Training Loss:0.000248\n",
      "Epoch: 128/200, Iteration: 6382/10000 --- Training Loss:0.000121\n",
      "Epoch: 128/200, Iteration: 6384/10000 --- Training Loss:0.000193\n",
      "Epoch: 128/200, Iteration: 6386/10000 --- Training Loss:0.000193\n",
      "Epoch: 128/200, Iteration: 6388/10000 --- Training Loss:0.000276\n",
      "Epoch: 128/200, Iteration: 6390/10000 --- Training Loss:0.000160\n",
      "Epoch: 128/200, Iteration: 6392/10000 --- Training Loss:0.000274\n",
      "Epoch: 128/200, Iteration: 6394/10000 --- Training Loss:0.000193\n",
      "Epoch: 128/200, Iteration: 6396/10000 --- Training Loss:0.000119\n",
      "Epoch: 128/200, Iteration: 6398/10000 --- Training Loss:0.000118\n",
      "Epoch: 128/200, Iteration: 6400/10000 --- Training Loss:0.000078\n",
      "Epoch: 128 finished ! Train Loss: 0.00017, Test Loss: 0.00242\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 129/200, Iteration: 6402/10000 --- Training Loss:0.000182\n",
      "Epoch: 129/200, Iteration: 6404/10000 --- Training Loss:0.000132\n",
      "Epoch: 129/200, Iteration: 6406/10000 --- Training Loss:0.000068\n",
      "Epoch: 129/200, Iteration: 6408/10000 --- Training Loss:0.000175\n",
      "Epoch: 129/200, Iteration: 6410/10000 --- Training Loss:0.000183\n",
      "Epoch: 129/200, Iteration: 6412/10000 --- Training Loss:0.000173\n",
      "Epoch: 129/200, Iteration: 6414/10000 --- Training Loss:0.000121\n",
      "Epoch: 129/200, Iteration: 6416/10000 --- Training Loss:0.000349\n",
      "Epoch: 129/200, Iteration: 6418/10000 --- Training Loss:0.000092\n",
      "Epoch: 129/200, Iteration: 6420/10000 --- Training Loss:0.000247\n",
      "Epoch: 129/200, Iteration: 6422/10000 --- Training Loss:0.000253\n",
      "Epoch: 129/200, Iteration: 6424/10000 --- Training Loss:0.000329\n",
      "Epoch: 129/200, Iteration: 6426/10000 --- Training Loss:0.000146\n",
      "Epoch: 129/200, Iteration: 6428/10000 --- Training Loss:0.000354\n",
      "Epoch: 129/200, Iteration: 6430/10000 --- Training Loss:0.000204\n",
      "Epoch: 129/200, Iteration: 6432/10000 --- Training Loss:0.000220\n",
      "Epoch: 129/200, Iteration: 6434/10000 --- Training Loss:0.000131\n",
      "Epoch: 129/200, Iteration: 6436/10000 --- Training Loss:0.000255\n",
      "Epoch: 129/200, Iteration: 6438/10000 --- Training Loss:0.000136\n",
      "Epoch: 129/200, Iteration: 6440/10000 --- Training Loss:0.000085\n",
      "Epoch: 129/200, Iteration: 6442/10000 --- Training Loss:0.000227\n",
      "Epoch: 129/200, Iteration: 6444/10000 --- Training Loss:0.000247\n",
      "Epoch: 129/200, Iteration: 6446/10000 --- Training Loss:0.000150\n",
      "Epoch: 129/200, Iteration: 6448/10000 --- Training Loss:0.000156\n",
      "Epoch: 129/200, Iteration: 6450/10000 --- Training Loss:0.000202\n",
      "Epoch: 129 finished ! Train Loss: 0.00022, Test Loss: 0.00214\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 130/200, Iteration: 6452/10000 --- Training Loss:0.000109\n",
      "Epoch: 130/200, Iteration: 6454/10000 --- Training Loss:0.000173\n",
      "Epoch: 130/200, Iteration: 6456/10000 --- Training Loss:0.000344\n",
      "Epoch: 130/200, Iteration: 6458/10000 --- Training Loss:0.000119\n",
      "Epoch: 130/200, Iteration: 6460/10000 --- Training Loss:0.000254\n",
      "Epoch: 130/200, Iteration: 6462/10000 --- Training Loss:0.008505\n",
      "Epoch: 130/200, Iteration: 6464/10000 --- Training Loss:0.001545\n",
      "Epoch: 130/200, Iteration: 6466/10000 --- Training Loss:0.001042\n",
      "Epoch: 130/200, Iteration: 6468/10000 --- Training Loss:0.000742\n",
      "Epoch: 130/200, Iteration: 6470/10000 --- Training Loss:0.000626\n",
      "Epoch: 130/200, Iteration: 6472/10000 --- Training Loss:0.002223\n",
      "Epoch: 130/200, Iteration: 6474/10000 --- Training Loss:0.000867\n",
      "Epoch: 130/200, Iteration: 6476/10000 --- Training Loss:0.000617\n",
      "Epoch: 130/200, Iteration: 6478/10000 --- Training Loss:0.000596\n",
      "Epoch: 130/200, Iteration: 6480/10000 --- Training Loss:0.000738\n",
      "Epoch: 130/200, Iteration: 6482/10000 --- Training Loss:0.000432\n",
      "Epoch: 130/200, Iteration: 6484/10000 --- Training Loss:0.000527\n",
      "Epoch: 130/200, Iteration: 6486/10000 --- Training Loss:0.001259\n",
      "Epoch: 130/200, Iteration: 6488/10000 --- Training Loss:0.000764\n",
      "Epoch: 130/200, Iteration: 6490/10000 --- Training Loss:0.000666\n",
      "Epoch: 130/200, Iteration: 6492/10000 --- Training Loss:0.000349\n",
      "Epoch: 130/200, Iteration: 6494/10000 --- Training Loss:0.000507\n",
      "Epoch: 130/200, Iteration: 6496/10000 --- Training Loss:0.000301\n",
      "Epoch: 130/200, Iteration: 6498/10000 --- Training Loss:0.000370\n",
      "Epoch: 130/200, Iteration: 6500/10000 --- Training Loss:0.000478\n",
      "Epoch: 130 finished ! Train Loss: 0.00082, Test Loss: 0.00170\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 131/200, Iteration: 6502/10000 --- Training Loss:0.000341\n",
      "Epoch: 131/200, Iteration: 6504/10000 --- Training Loss:0.000254\n",
      "Epoch: 131/200, Iteration: 6506/10000 --- Training Loss:0.001310\n",
      "Epoch: 131/200, Iteration: 6508/10000 --- Training Loss:0.000440\n",
      "Epoch: 131/200, Iteration: 6510/10000 --- Training Loss:0.000421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 131/200, Iteration: 6512/10000 --- Training Loss:0.000413\n",
      "Epoch: 131/200, Iteration: 6514/10000 --- Training Loss:0.000282\n",
      "Epoch: 131/200, Iteration: 6516/10000 --- Training Loss:0.000266\n",
      "Epoch: 131/200, Iteration: 6518/10000 --- Training Loss:0.000233\n",
      "Epoch: 131/200, Iteration: 6520/10000 --- Training Loss:0.000243\n",
      "Epoch: 131/200, Iteration: 6522/10000 --- Training Loss:0.000369\n",
      "Epoch: 131/200, Iteration: 6524/10000 --- Training Loss:0.000165\n",
      "Epoch: 131/200, Iteration: 6526/10000 --- Training Loss:0.000223\n",
      "Epoch: 131/200, Iteration: 6528/10000 --- Training Loss:0.000214\n",
      "Epoch: 131/200, Iteration: 6530/10000 --- Training Loss:0.000161\n",
      "Epoch: 131/200, Iteration: 6532/10000 --- Training Loss:0.000214\n",
      "Epoch: 131/200, Iteration: 6534/10000 --- Training Loss:0.000232\n",
      "Epoch: 131/200, Iteration: 6536/10000 --- Training Loss:0.000198\n",
      "Epoch: 131/200, Iteration: 6538/10000 --- Training Loss:0.000178\n",
      "Epoch: 131/200, Iteration: 6540/10000 --- Training Loss:0.000214\n",
      "Epoch: 131/200, Iteration: 6542/10000 --- Training Loss:0.000231\n",
      "Epoch: 131/200, Iteration: 6544/10000 --- Training Loss:0.000173\n",
      "Epoch: 131/200, Iteration: 6546/10000 --- Training Loss:0.000165\n",
      "Epoch: 131/200, Iteration: 6548/10000 --- Training Loss:0.000216\n",
      "Epoch: 131/200, Iteration: 6550/10000 --- Training Loss:0.000364\n",
      "Epoch: 131 finished ! Train Loss: 0.00029, Test Loss: 0.00138\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 132/200, Iteration: 6552/10000 --- Training Loss:0.000238\n",
      "Epoch: 132/200, Iteration: 6554/10000 --- Training Loss:0.000166\n",
      "Epoch: 132/200, Iteration: 6556/10000 --- Training Loss:0.000189\n",
      "Epoch: 132/200, Iteration: 6558/10000 --- Training Loss:0.000150\n",
      "Epoch: 132/200, Iteration: 6560/10000 --- Training Loss:0.000234\n",
      "Epoch: 132/200, Iteration: 6562/10000 --- Training Loss:0.000186\n",
      "Epoch: 132/200, Iteration: 6564/10000 --- Training Loss:0.000116\n",
      "Epoch: 132/200, Iteration: 6566/10000 --- Training Loss:0.000381\n",
      "Epoch: 132/200, Iteration: 6568/10000 --- Training Loss:0.000158\n",
      "Epoch: 132/200, Iteration: 6570/10000 --- Training Loss:0.000307\n",
      "Epoch: 132/200, Iteration: 6572/10000 --- Training Loss:0.000119\n",
      "Epoch: 132/200, Iteration: 6574/10000 --- Training Loss:0.000428\n",
      "Epoch: 132/200, Iteration: 6576/10000 --- Training Loss:0.000149\n",
      "Epoch: 132/200, Iteration: 6578/10000 --- Training Loss:0.000217\n",
      "Epoch: 132/200, Iteration: 6580/10000 --- Training Loss:0.000211\n",
      "Epoch: 132/200, Iteration: 6582/10000 --- Training Loss:0.000195\n",
      "Epoch: 132/200, Iteration: 6584/10000 --- Training Loss:0.000146\n",
      "Epoch: 132/200, Iteration: 6586/10000 --- Training Loss:0.000219\n",
      "Epoch: 132/200, Iteration: 6588/10000 --- Training Loss:0.000185\n",
      "Epoch: 132/200, Iteration: 6590/10000 --- Training Loss:0.000211\n",
      "Epoch: 132/200, Iteration: 6592/10000 --- Training Loss:0.000172\n",
      "Epoch: 132/200, Iteration: 6594/10000 --- Training Loss:0.000141\n",
      "Epoch: 132/200, Iteration: 6596/10000 --- Training Loss:0.000294\n",
      "Epoch: 132/200, Iteration: 6598/10000 --- Training Loss:0.000139\n",
      "Epoch: 132/200, Iteration: 6600/10000 --- Training Loss:0.000104\n",
      "Epoch: 132 finished ! Train Loss: 0.00021, Test Loss: 0.00121\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 133/200, Iteration: 6602/10000 --- Training Loss:0.000218\n",
      "Epoch: 133/200, Iteration: 6604/10000 --- Training Loss:0.000267\n",
      "Epoch: 133/200, Iteration: 6606/10000 --- Training Loss:0.000131\n",
      "Epoch: 133/200, Iteration: 6608/10000 --- Training Loss:0.000150\n",
      "Epoch: 133/200, Iteration: 6610/10000 --- Training Loss:0.000432\n",
      "Epoch: 133/200, Iteration: 6612/10000 --- Training Loss:0.000345\n",
      "Epoch: 133/200, Iteration: 6614/10000 --- Training Loss:0.000462\n",
      "Epoch: 133/200, Iteration: 6616/10000 --- Training Loss:0.000224\n",
      "Epoch: 133/200, Iteration: 6618/10000 --- Training Loss:0.000438\n",
      "Epoch: 133/200, Iteration: 6620/10000 --- Training Loss:0.000243\n",
      "Epoch: 133/200, Iteration: 6622/10000 --- Training Loss:0.000298\n",
      "Epoch: 133/200, Iteration: 6624/10000 --- Training Loss:0.000181\n",
      "Epoch: 133/200, Iteration: 6626/10000 --- Training Loss:0.000251\n",
      "Epoch: 133/200, Iteration: 6628/10000 --- Training Loss:0.000257\n",
      "Epoch: 133/200, Iteration: 6630/10000 --- Training Loss:0.000115\n",
      "Epoch: 133/200, Iteration: 6632/10000 --- Training Loss:0.000203\n",
      "Epoch: 133/200, Iteration: 6634/10000 --- Training Loss:0.000148\n",
      "Epoch: 133/200, Iteration: 6636/10000 --- Training Loss:0.000237\n",
      "Epoch: 133/200, Iteration: 6638/10000 --- Training Loss:0.000257\n",
      "Epoch: 133/200, Iteration: 6640/10000 --- Training Loss:0.000209\n",
      "Epoch: 133/200, Iteration: 6642/10000 --- Training Loss:0.000212\n",
      "Epoch: 133/200, Iteration: 6644/10000 --- Training Loss:0.000145\n",
      "Epoch: 133/200, Iteration: 6646/10000 --- Training Loss:0.000185\n",
      "Epoch: 133/200, Iteration: 6648/10000 --- Training Loss:0.000203\n",
      "Epoch: 133/200, Iteration: 6650/10000 --- Training Loss:0.000194\n",
      "Epoch: 133 finished ! Train Loss: 0.00026, Test Loss: 0.00192\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 134/200, Iteration: 6652/10000 --- Training Loss:0.000343\n",
      "Epoch: 134/200, Iteration: 6654/10000 --- Training Loss:0.000209\n",
      "Epoch: 134/200, Iteration: 6656/10000 --- Training Loss:0.000397\n",
      "Epoch: 134/200, Iteration: 6658/10000 --- Training Loss:0.000104\n",
      "Epoch: 134/200, Iteration: 6660/10000 --- Training Loss:0.000349\n",
      "Epoch: 134/200, Iteration: 6662/10000 --- Training Loss:0.000172\n",
      "Epoch: 134/200, Iteration: 6664/10000 --- Training Loss:0.000199\n",
      "Epoch: 134/200, Iteration: 6666/10000 --- Training Loss:0.000287\n",
      "Epoch: 134/200, Iteration: 6668/10000 --- Training Loss:0.000163\n",
      "Epoch: 134/200, Iteration: 6670/10000 --- Training Loss:0.000164\n",
      "Epoch: 134/200, Iteration: 6672/10000 --- Training Loss:0.000197\n",
      "Epoch: 134/200, Iteration: 6674/10000 --- Training Loss:0.000111\n",
      "Epoch: 134/200, Iteration: 6676/10000 --- Training Loss:0.000820\n",
      "Epoch: 134/200, Iteration: 6678/10000 --- Training Loss:0.000195\n",
      "Epoch: 134/200, Iteration: 6680/10000 --- Training Loss:0.000236\n",
      "Epoch: 134/200, Iteration: 6682/10000 --- Training Loss:0.000220\n",
      "Epoch: 134/200, Iteration: 6684/10000 --- Training Loss:0.000143\n",
      "Epoch: 134/200, Iteration: 6686/10000 --- Training Loss:0.000195\n",
      "Epoch: 134/200, Iteration: 6688/10000 --- Training Loss:0.000150\n",
      "Epoch: 134/200, Iteration: 6690/10000 --- Training Loss:0.000105\n",
      "Epoch: 134/200, Iteration: 6692/10000 --- Training Loss:0.000169\n",
      "Epoch: 134/200, Iteration: 6694/10000 --- Training Loss:0.000418\n",
      "Epoch: 134/200, Iteration: 6696/10000 --- Training Loss:0.000248\n",
      "Epoch: 134/200, Iteration: 6698/10000 --- Training Loss:0.000165\n",
      "Epoch: 134/200, Iteration: 6700/10000 --- Training Loss:0.000225\n",
      "Epoch: 134 finished ! Train Loss: 0.00022, Test Loss: 0.00125\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 135/200, Iteration: 6702/10000 --- Training Loss:0.000127\n",
      "Epoch: 135/200, Iteration: 6704/10000 --- Training Loss:0.000146\n",
      "Epoch: 135/200, Iteration: 6706/10000 --- Training Loss:0.000084\n",
      "Epoch: 135/200, Iteration: 6708/10000 --- Training Loss:0.000135\n",
      "Epoch: 135/200, Iteration: 6710/10000 --- Training Loss:0.000342\n",
      "Epoch: 135/200, Iteration: 6712/10000 --- Training Loss:0.000153\n",
      "Epoch: 135/200, Iteration: 6714/10000 --- Training Loss:0.000240\n",
      "Epoch: 135/200, Iteration: 6716/10000 --- Training Loss:0.000202\n",
      "Epoch: 135/200, Iteration: 6718/10000 --- Training Loss:0.000346\n",
      "Epoch: 135/200, Iteration: 6720/10000 --- Training Loss:0.000184\n",
      "Epoch: 135/200, Iteration: 6722/10000 --- Training Loss:0.000257\n",
      "Epoch: 135/200, Iteration: 6724/10000 --- Training Loss:0.000134\n",
      "Epoch: 135/200, Iteration: 6726/10000 --- Training Loss:0.000168\n",
      "Epoch: 135/200, Iteration: 6728/10000 --- Training Loss:0.000471\n",
      "Epoch: 135/200, Iteration: 6730/10000 --- Training Loss:0.000351\n",
      "Epoch: 135/200, Iteration: 6732/10000 --- Training Loss:0.000225\n",
      "Epoch: 135/200, Iteration: 6734/10000 --- Training Loss:0.000211\n",
      "Epoch: 135/200, Iteration: 6736/10000 --- Training Loss:0.000179\n",
      "Epoch: 135/200, Iteration: 6738/10000 --- Training Loss:0.000213\n",
      "Epoch: 135/200, Iteration: 6740/10000 --- Training Loss:0.000261\n",
      "Epoch: 135/200, Iteration: 6742/10000 --- Training Loss:0.000159\n",
      "Epoch: 135/200, Iteration: 6744/10000 --- Training Loss:0.000207\n",
      "Epoch: 135/200, Iteration: 6746/10000 --- Training Loss:0.000555\n",
      "Epoch: 135/200, Iteration: 6748/10000 --- Training Loss:0.000122\n",
      "Epoch: 135/200, Iteration: 6750/10000 --- Training Loss:0.000199\n",
      "Epoch: 135 finished ! Train Loss: 0.00022, Test Loss: 0.00145\n",
      "Epoch consuming time: 0m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136/200, Iteration: 6752/10000 --- Training Loss:0.000301\n",
      "Epoch: 136/200, Iteration: 6754/10000 --- Training Loss:0.000197\n",
      "Epoch: 136/200, Iteration: 6756/10000 --- Training Loss:0.000153\n",
      "Epoch: 136/200, Iteration: 6758/10000 --- Training Loss:0.000180\n",
      "Epoch: 136/200, Iteration: 6760/10000 --- Training Loss:0.000131\n",
      "Epoch: 136/200, Iteration: 6762/10000 --- Training Loss:0.000145\n",
      "Epoch: 136/200, Iteration: 6764/10000 --- Training Loss:0.000118\n",
      "Epoch: 136/200, Iteration: 6766/10000 --- Training Loss:0.000111\n",
      "Epoch: 136/200, Iteration: 6768/10000 --- Training Loss:0.000454\n",
      "Epoch: 136/200, Iteration: 6770/10000 --- Training Loss:0.000173\n",
      "Epoch: 136/200, Iteration: 6772/10000 --- Training Loss:0.000277\n",
      "Epoch: 136/200, Iteration: 6774/10000 --- Training Loss:0.000233\n",
      "Epoch: 136/200, Iteration: 6776/10000 --- Training Loss:0.000107\n",
      "Epoch: 136/200, Iteration: 6778/10000 --- Training Loss:0.000143\n",
      "Epoch: 136/200, Iteration: 6780/10000 --- Training Loss:0.000162\n",
      "Epoch: 136/200, Iteration: 6782/10000 --- Training Loss:0.000134\n",
      "Epoch: 136/200, Iteration: 6784/10000 --- Training Loss:0.000371\n",
      "Epoch: 136/200, Iteration: 6786/10000 --- Training Loss:0.000721\n",
      "Epoch: 136/200, Iteration: 6788/10000 --- Training Loss:0.000404\n",
      "Epoch: 136/200, Iteration: 6790/10000 --- Training Loss:0.000298\n",
      "Epoch: 136/200, Iteration: 6792/10000 --- Training Loss:0.000289\n",
      "Epoch: 136/200, Iteration: 6794/10000 --- Training Loss:0.000184\n",
      "Epoch: 136/200, Iteration: 6796/10000 --- Training Loss:0.000184\n",
      "Epoch: 136/200, Iteration: 6798/10000 --- Training Loss:0.000234\n",
      "Epoch: 136/200, Iteration: 6800/10000 --- Training Loss:0.000300\n",
      "Epoch: 136 finished ! Train Loss: 0.00024, Test Loss: 0.00163\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 137/200, Iteration: 6802/10000 --- Training Loss:0.000153\n",
      "Epoch: 137/200, Iteration: 6804/10000 --- Training Loss:0.000149\n",
      "Epoch: 137/200, Iteration: 6806/10000 --- Training Loss:0.000563\n",
      "Epoch: 137/200, Iteration: 6808/10000 --- Training Loss:0.000180\n",
      "Epoch: 137/200, Iteration: 6810/10000 --- Training Loss:0.000166\n",
      "Epoch: 137/200, Iteration: 6812/10000 --- Training Loss:0.000132\n",
      "Epoch: 137/200, Iteration: 6814/10000 --- Training Loss:0.000114\n",
      "Epoch: 137/200, Iteration: 6816/10000 --- Training Loss:0.000109\n",
      "Epoch: 137/200, Iteration: 6818/10000 --- Training Loss:0.000186\n",
      "Epoch: 137/200, Iteration: 6820/10000 --- Training Loss:0.000195\n",
      "Epoch: 137/200, Iteration: 6822/10000 --- Training Loss:0.000188\n",
      "Epoch: 137/200, Iteration: 6824/10000 --- Training Loss:0.000201\n",
      "Epoch: 137/200, Iteration: 6826/10000 --- Training Loss:0.000123\n",
      "Epoch: 137/200, Iteration: 6828/10000 --- Training Loss:0.000147\n",
      "Epoch: 137/200, Iteration: 6830/10000 --- Training Loss:0.000622\n",
      "Epoch: 137/200, Iteration: 6832/10000 --- Training Loss:0.000170\n",
      "Epoch: 137/200, Iteration: 6834/10000 --- Training Loss:0.000248\n",
      "Epoch: 137/200, Iteration: 6836/10000 --- Training Loss:0.000388\n",
      "Epoch: 137/200, Iteration: 6838/10000 --- Training Loss:0.000181\n",
      "Epoch: 137/200, Iteration: 6840/10000 --- Training Loss:0.000179\n",
      "Epoch: 137/200, Iteration: 6842/10000 --- Training Loss:0.000128\n",
      "Epoch: 137/200, Iteration: 6844/10000 --- Training Loss:0.000157\n",
      "Epoch: 137/200, Iteration: 6846/10000 --- Training Loss:0.000090\n",
      "Epoch: 137/200, Iteration: 6848/10000 --- Training Loss:0.000134\n",
      "Epoch: 137/200, Iteration: 6850/10000 --- Training Loss:0.000370\n",
      "Epoch: 137 finished ! Train Loss: 0.00020, Test Loss: 0.00266\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 138/200, Iteration: 6852/10000 --- Training Loss:0.000109\n",
      "Epoch: 138/200, Iteration: 6854/10000 --- Training Loss:0.000121\n",
      "Epoch: 138/200, Iteration: 6856/10000 --- Training Loss:0.000219\n",
      "Epoch: 138/200, Iteration: 6858/10000 --- Training Loss:0.000059\n",
      "Epoch: 138/200, Iteration: 6860/10000 --- Training Loss:0.000162\n",
      "Epoch: 138/200, Iteration: 6862/10000 --- Training Loss:0.000223\n",
      "Epoch: 138/200, Iteration: 6864/10000 --- Training Loss:0.000168\n",
      "Epoch: 138/200, Iteration: 6866/10000 --- Training Loss:0.000362\n",
      "Epoch: 138/200, Iteration: 6868/10000 --- Training Loss:0.000165\n",
      "Epoch: 138/200, Iteration: 6870/10000 --- Training Loss:0.000108\n",
      "Epoch: 138/200, Iteration: 6872/10000 --- Training Loss:0.000140\n",
      "Epoch: 138/200, Iteration: 6874/10000 --- Training Loss:0.000111\n",
      "Epoch: 138/200, Iteration: 6876/10000 --- Training Loss:0.000143\n",
      "Epoch: 138/200, Iteration: 6878/10000 --- Training Loss:0.000109\n",
      "Epoch: 138/200, Iteration: 6880/10000 --- Training Loss:0.000120\n",
      "Epoch: 138/200, Iteration: 6882/10000 --- Training Loss:0.000136\n",
      "Epoch: 138/200, Iteration: 6884/10000 --- Training Loss:0.000099\n",
      "Epoch: 138/200, Iteration: 6886/10000 --- Training Loss:0.000191\n",
      "Epoch: 138/200, Iteration: 6888/10000 --- Training Loss:0.000208\n",
      "Epoch: 138/200, Iteration: 6890/10000 --- Training Loss:0.000135\n",
      "Epoch: 138/200, Iteration: 6892/10000 --- Training Loss:0.000147\n",
      "Epoch: 138/200, Iteration: 6894/10000 --- Training Loss:0.000216\n",
      "Epoch: 138/200, Iteration: 6896/10000 --- Training Loss:0.000227\n",
      "Epoch: 138/200, Iteration: 6898/10000 --- Training Loss:0.000117\n",
      "Epoch: 138/200, Iteration: 6900/10000 --- Training Loss:0.000091\n",
      "Epoch: 138 finished ! Train Loss: 0.00017, Test Loss: 0.00261\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 139/200, Iteration: 6902/10000 --- Training Loss:0.000138\n",
      "Epoch: 139/200, Iteration: 6904/10000 --- Training Loss:0.000160\n",
      "Epoch: 139/200, Iteration: 6906/10000 --- Training Loss:0.000139\n",
      "Epoch: 139/200, Iteration: 6908/10000 --- Training Loss:0.000144\n",
      "Epoch: 139/200, Iteration: 6910/10000 --- Training Loss:0.000296\n",
      "Epoch: 139/200, Iteration: 6912/10000 --- Training Loss:0.000192\n",
      "Epoch: 139/200, Iteration: 6914/10000 --- Training Loss:0.000126\n",
      "Epoch: 139/200, Iteration: 6916/10000 --- Training Loss:0.000149\n",
      "Epoch: 139/200, Iteration: 6918/10000 --- Training Loss:0.000931\n",
      "Epoch: 139/200, Iteration: 6920/10000 --- Training Loss:0.000241\n",
      "Epoch: 139/200, Iteration: 6922/10000 --- Training Loss:0.000300\n",
      "Epoch: 139/200, Iteration: 6924/10000 --- Training Loss:0.000306\n",
      "Epoch: 139/200, Iteration: 6926/10000 --- Training Loss:0.000311\n",
      "Epoch: 139/200, Iteration: 6928/10000 --- Training Loss:0.000361\n",
      "Epoch: 139/200, Iteration: 6930/10000 --- Training Loss:0.000198\n",
      "Epoch: 139/200, Iteration: 6932/10000 --- Training Loss:0.000243\n",
      "Epoch: 139/200, Iteration: 6934/10000 --- Training Loss:0.000157\n",
      "Epoch: 139/200, Iteration: 6936/10000 --- Training Loss:0.000144\n",
      "Epoch: 139/200, Iteration: 6938/10000 --- Training Loss:0.000288\n",
      "Epoch: 139/200, Iteration: 6940/10000 --- Training Loss:0.000114\n",
      "Epoch: 139/200, Iteration: 6942/10000 --- Training Loss:0.000180\n",
      "Epoch: 139/200, Iteration: 6944/10000 --- Training Loss:0.000667\n",
      "Epoch: 139/200, Iteration: 6946/10000 --- Training Loss:0.000150\n",
      "Epoch: 139/200, Iteration: 6948/10000 --- Training Loss:0.000212\n",
      "Epoch: 139/200, Iteration: 6950/10000 --- Training Loss:0.000232\n",
      "Epoch: 139 finished ! Train Loss: 0.00024, Test Loss: 0.00160\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 140/200, Iteration: 6952/10000 --- Training Loss:0.000204\n",
      "Epoch: 140/200, Iteration: 6954/10000 --- Training Loss:0.000220\n",
      "Epoch: 140/200, Iteration: 6956/10000 --- Training Loss:0.000176\n",
      "Epoch: 140/200, Iteration: 6958/10000 --- Training Loss:0.000299\n",
      "Epoch: 140/200, Iteration: 6960/10000 --- Training Loss:0.000188\n",
      "Epoch: 140/200, Iteration: 6962/10000 --- Training Loss:0.000117\n",
      "Epoch: 140/200, Iteration: 6964/10000 --- Training Loss:0.000302\n",
      "Epoch: 140/200, Iteration: 6966/10000 --- Training Loss:0.000200\n",
      "Epoch: 140/200, Iteration: 6968/10000 --- Training Loss:0.000330\n",
      "Epoch: 140/200, Iteration: 6970/10000 --- Training Loss:0.000208\n",
      "Epoch: 140/200, Iteration: 6972/10000 --- Training Loss:0.000176\n",
      "Epoch: 140/200, Iteration: 6974/10000 --- Training Loss:0.000138\n",
      "Epoch: 140/200, Iteration: 6976/10000 --- Training Loss:0.000258\n",
      "Epoch: 140/200, Iteration: 6978/10000 --- Training Loss:0.000149\n",
      "Epoch: 140/200, Iteration: 6980/10000 --- Training Loss:0.000266\n",
      "Epoch: 140/200, Iteration: 6982/10000 --- Training Loss:0.000139\n",
      "Epoch: 140/200, Iteration: 6984/10000 --- Training Loss:0.000150\n",
      "Epoch: 140/200, Iteration: 6986/10000 --- Training Loss:0.000149\n",
      "Epoch: 140/200, Iteration: 6988/10000 --- Training Loss:0.000443\n",
      "Epoch: 140/200, Iteration: 6990/10000 --- Training Loss:0.001167\n",
      "Epoch: 140/200, Iteration: 6992/10000 --- Training Loss:0.000333\n",
      "Epoch: 140/200, Iteration: 6994/10000 --- Training Loss:0.000246\n",
      "Epoch: 140/200, Iteration: 6996/10000 --- Training Loss:0.000362\n",
      "Epoch: 140/200, Iteration: 6998/10000 --- Training Loss:0.000295\n",
      "Epoch: 140/200, Iteration: 7000/10000 --- Training Loss:0.000304\n",
      "Epoch: 140 finished ! Train Loss: 0.00026, Test Loss: 0.00210\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 141/200, Iteration: 7002/10000 --- Training Loss:0.000146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 141/200, Iteration: 7004/10000 --- Training Loss:0.000228\n",
      "Epoch: 141/200, Iteration: 7006/10000 --- Training Loss:0.000223\n",
      "Epoch: 141/200, Iteration: 7008/10000 --- Training Loss:0.000102\n",
      "Epoch: 141/200, Iteration: 7010/10000 --- Training Loss:0.000183\n",
      "Epoch: 141/200, Iteration: 7012/10000 --- Training Loss:0.000111\n",
      "Epoch: 141/200, Iteration: 7014/10000 --- Training Loss:0.000122\n",
      "Epoch: 141/200, Iteration: 7016/10000 --- Training Loss:0.000142\n",
      "Epoch: 141/200, Iteration: 7018/10000 --- Training Loss:0.000243\n",
      "Epoch: 141/200, Iteration: 7020/10000 --- Training Loss:0.000091\n",
      "Epoch: 141/200, Iteration: 7022/10000 --- Training Loss:0.000166\n",
      "Epoch: 141/200, Iteration: 7024/10000 --- Training Loss:0.000148\n",
      "Epoch: 141/200, Iteration: 7026/10000 --- Training Loss:0.000111\n",
      "Epoch: 141/200, Iteration: 7028/10000 --- Training Loss:0.000097\n",
      "Epoch: 141/200, Iteration: 7030/10000 --- Training Loss:0.000197\n",
      "Epoch: 141/200, Iteration: 7032/10000 --- Training Loss:0.000132\n",
      "Epoch: 141/200, Iteration: 7034/10000 --- Training Loss:0.000348\n",
      "Epoch: 141/200, Iteration: 7036/10000 --- Training Loss:0.000142\n",
      "Epoch: 141/200, Iteration: 7038/10000 --- Training Loss:0.000125\n",
      "Epoch: 141/200, Iteration: 7040/10000 --- Training Loss:0.000100\n",
      "Epoch: 141/200, Iteration: 7042/10000 --- Training Loss:0.000236\n",
      "Epoch: 141/200, Iteration: 7044/10000 --- Training Loss:0.000255\n",
      "Epoch: 141/200, Iteration: 7046/10000 --- Training Loss:0.000144\n",
      "Epoch: 141/200, Iteration: 7048/10000 --- Training Loss:0.000129\n",
      "Epoch: 141/200, Iteration: 7050/10000 --- Training Loss:0.000364\n",
      "Epoch: 141 finished ! Train Loss: 0.00018, Test Loss: 0.00115\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 70 percent completed\n",
      "Epoch: 142/200, Iteration: 7052/10000 --- Training Loss:0.000116\n",
      "Epoch: 142/200, Iteration: 7054/10000 --- Training Loss:0.000223\n",
      "Epoch: 142/200, Iteration: 7056/10000 --- Training Loss:0.000149\n",
      "Epoch: 142/200, Iteration: 7058/10000 --- Training Loss:0.000182\n",
      "Epoch: 142/200, Iteration: 7060/10000 --- Training Loss:0.000152\n",
      "Epoch: 142/200, Iteration: 7062/10000 --- Training Loss:0.000133\n",
      "Epoch: 142/200, Iteration: 7064/10000 --- Training Loss:0.000200\n",
      "Epoch: 142/200, Iteration: 7066/10000 --- Training Loss:0.000122\n",
      "Epoch: 142/200, Iteration: 7068/10000 --- Training Loss:0.000191\n",
      "Epoch: 142/200, Iteration: 7070/10000 --- Training Loss:0.000135\n",
      "Epoch: 142/200, Iteration: 7072/10000 --- Training Loss:0.000096\n",
      "Epoch: 142/200, Iteration: 7074/10000 --- Training Loss:0.000136\n",
      "Epoch: 142/200, Iteration: 7076/10000 --- Training Loss:0.000102\n",
      "Epoch: 142/200, Iteration: 7078/10000 --- Training Loss:0.000177\n",
      "Epoch: 142/200, Iteration: 7080/10000 --- Training Loss:0.000133\n",
      "Epoch: 142/200, Iteration: 7082/10000 --- Training Loss:0.000329\n",
      "Epoch: 142/200, Iteration: 7084/10000 --- Training Loss:0.000179\n",
      "Epoch: 142/200, Iteration: 7086/10000 --- Training Loss:0.000140\n",
      "Epoch: 142/200, Iteration: 7088/10000 --- Training Loss:0.000087\n",
      "Epoch: 142/200, Iteration: 7090/10000 --- Training Loss:0.000112\n",
      "Epoch: 142/200, Iteration: 7092/10000 --- Training Loss:0.000178\n",
      "Epoch: 142/200, Iteration: 7094/10000 --- Training Loss:0.000098\n",
      "Epoch: 142/200, Iteration: 7096/10000 --- Training Loss:0.000180\n",
      "Epoch: 142/200, Iteration: 7098/10000 --- Training Loss:0.000110\n",
      "Epoch: 142/200, Iteration: 7100/10000 --- Training Loss:0.000189\n",
      "Epoch: 142 finished ! Train Loss: 0.00016, Test Loss: 0.00158\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 143/200, Iteration: 7102/10000 --- Training Loss:0.000175\n",
      "Epoch: 143/200, Iteration: 7104/10000 --- Training Loss:0.000105\n",
      "Epoch: 143/200, Iteration: 7106/10000 --- Training Loss:0.000085\n",
      "Epoch: 143/200, Iteration: 7108/10000 --- Training Loss:0.000087\n",
      "Epoch: 143/200, Iteration: 7110/10000 --- Training Loss:0.000116\n",
      "Epoch: 143/200, Iteration: 7112/10000 --- Training Loss:0.000068\n",
      "Epoch: 143/200, Iteration: 7114/10000 --- Training Loss:0.000253\n",
      "Epoch: 143/200, Iteration: 7116/10000 --- Training Loss:0.000182\n",
      "Epoch: 143/200, Iteration: 7118/10000 --- Training Loss:0.000119\n",
      "Epoch: 143/200, Iteration: 7120/10000 --- Training Loss:0.000182\n",
      "Epoch: 143/200, Iteration: 7122/10000 --- Training Loss:0.000146\n",
      "Epoch: 143/200, Iteration: 7124/10000 --- Training Loss:0.000137\n",
      "Epoch: 143/200, Iteration: 7126/10000 --- Training Loss:0.000200\n",
      "Epoch: 143/200, Iteration: 7128/10000 --- Training Loss:0.000149\n",
      "Epoch: 143/200, Iteration: 7130/10000 --- Training Loss:0.000137\n",
      "Epoch: 143/200, Iteration: 7132/10000 --- Training Loss:0.000143\n",
      "Epoch: 143/200, Iteration: 7134/10000 --- Training Loss:0.000201\n",
      "Epoch: 143/200, Iteration: 7136/10000 --- Training Loss:0.000231\n",
      "Epoch: 143/200, Iteration: 7138/10000 --- Training Loss:0.000465\n",
      "Epoch: 143/200, Iteration: 7140/10000 --- Training Loss:0.000434\n",
      "Epoch: 143/200, Iteration: 7142/10000 --- Training Loss:0.000377\n",
      "Epoch: 143/200, Iteration: 7144/10000 --- Training Loss:0.000250\n",
      "Epoch: 143/200, Iteration: 7146/10000 --- Training Loss:0.000584\n",
      "Epoch: 143/200, Iteration: 7148/10000 --- Training Loss:0.000249\n",
      "Epoch: 143/200, Iteration: 7150/10000 --- Training Loss:0.000389\n",
      "Epoch: 143 finished ! Train Loss: 0.00023, Test Loss: 0.00039\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 71 percent completed\n",
      "Epoch: 144/200, Iteration: 7152/10000 --- Training Loss:0.000355\n",
      "Epoch: 144/200, Iteration: 7154/10000 --- Training Loss:0.000318\n",
      "Epoch: 144/200, Iteration: 7156/10000 --- Training Loss:0.000170\n",
      "Epoch: 144/200, Iteration: 7158/10000 --- Training Loss:0.000241\n",
      "Epoch: 144/200, Iteration: 7160/10000 --- Training Loss:0.000214\n",
      "Epoch: 144/200, Iteration: 7162/10000 --- Training Loss:0.000099\n",
      "Epoch: 144/200, Iteration: 7164/10000 --- Training Loss:0.000198\n",
      "Epoch: 144/200, Iteration: 7166/10000 --- Training Loss:0.000116\n",
      "Epoch: 144/200, Iteration: 7168/10000 --- Training Loss:0.000117\n",
      "Epoch: 144/200, Iteration: 7170/10000 --- Training Loss:0.000175\n",
      "Epoch: 144/200, Iteration: 7172/10000 --- Training Loss:0.000211\n",
      "Epoch: 144/200, Iteration: 7174/10000 --- Training Loss:0.000116\n",
      "Epoch: 144/200, Iteration: 7176/10000 --- Training Loss:0.000114\n",
      "Epoch: 144/200, Iteration: 7178/10000 --- Training Loss:0.000090\n",
      "Epoch: 144/200, Iteration: 7180/10000 --- Training Loss:0.000112\n",
      "Epoch: 144/200, Iteration: 7182/10000 --- Training Loss:0.005780\n",
      "Epoch: 144/200, Iteration: 7184/10000 --- Training Loss:0.000863\n",
      "Epoch: 144/200, Iteration: 7186/10000 --- Training Loss:0.001453\n",
      "Epoch: 144/200, Iteration: 7188/10000 --- Training Loss:0.000565\n",
      "Epoch: 144/200, Iteration: 7190/10000 --- Training Loss:0.000459\n",
      "Epoch: 144/200, Iteration: 7192/10000 --- Training Loss:0.000457\n",
      "Epoch: 144/200, Iteration: 7194/10000 --- Training Loss:0.000952\n",
      "Epoch: 144/200, Iteration: 7196/10000 --- Training Loss:0.000784\n",
      "Epoch: 144/200, Iteration: 7198/10000 --- Training Loss:0.000655\n",
      "Epoch: 144/200, Iteration: 7200/10000 --- Training Loss:0.000551\n",
      "Epoch: 144 finished ! Train Loss: 0.00049, Test Loss: 0.00334\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 145/200, Iteration: 7202/10000 --- Training Loss:0.000343\n",
      "Epoch: 145/200, Iteration: 7204/10000 --- Training Loss:0.000624\n",
      "Epoch: 145/200, Iteration: 7206/10000 --- Training Loss:0.000382\n",
      "Epoch: 145/200, Iteration: 7208/10000 --- Training Loss:0.000497\n",
      "Epoch: 145/200, Iteration: 7210/10000 --- Training Loss:0.000340\n",
      "Epoch: 145/200, Iteration: 7212/10000 --- Training Loss:0.000308\n",
      "Epoch: 145/200, Iteration: 7214/10000 --- Training Loss:0.000230\n",
      "Epoch: 145/200, Iteration: 7216/10000 --- Training Loss:0.000299\n",
      "Epoch: 145/200, Iteration: 7218/10000 --- Training Loss:0.000446\n",
      "Epoch: 145/200, Iteration: 7220/10000 --- Training Loss:0.000227\n",
      "Epoch: 145/200, Iteration: 7222/10000 --- Training Loss:0.000226\n",
      "Epoch: 145/200, Iteration: 7224/10000 --- Training Loss:0.000295\n",
      "Epoch: 145/200, Iteration: 7226/10000 --- Training Loss:0.000187\n",
      "Epoch: 145/200, Iteration: 7228/10000 --- Training Loss:0.000197\n",
      "Epoch: 145/200, Iteration: 7230/10000 --- Training Loss:0.000371\n",
      "Epoch: 145/200, Iteration: 7232/10000 --- Training Loss:0.000368\n",
      "Epoch: 145/200, Iteration: 7234/10000 --- Training Loss:0.000461\n",
      "Epoch: 145/200, Iteration: 7236/10000 --- Training Loss:0.000231\n",
      "Epoch: 145/200, Iteration: 7238/10000 --- Training Loss:0.000397\n",
      "Epoch: 145/200, Iteration: 7240/10000 --- Training Loss:0.000238\n",
      "Epoch: 145/200, Iteration: 7242/10000 --- Training Loss:0.000274\n",
      "Epoch: 145/200, Iteration: 7244/10000 --- Training Loss:0.000266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 145/200, Iteration: 7246/10000 --- Training Loss:0.000171\n",
      "Epoch: 145/200, Iteration: 7248/10000 --- Training Loss:0.000238\n",
      "Epoch: 145/200, Iteration: 7250/10000 --- Training Loss:0.000250\n",
      "Epoch: 145 finished ! Train Loss: 0.00033, Test Loss: 0.00251\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 146/200, Iteration: 7252/10000 --- Training Loss:0.000165\n",
      "Epoch: 146/200, Iteration: 7254/10000 --- Training Loss:0.000225\n",
      "Epoch: 146/200, Iteration: 7256/10000 --- Training Loss:0.000196\n",
      "Epoch: 146/200, Iteration: 7258/10000 --- Training Loss:0.000290\n",
      "Epoch: 146/200, Iteration: 7260/10000 --- Training Loss:0.000160\n",
      "Epoch: 146/200, Iteration: 7262/10000 --- Training Loss:0.000156\n",
      "Epoch: 146/200, Iteration: 7264/10000 --- Training Loss:0.000857\n",
      "Epoch: 146/200, Iteration: 7266/10000 --- Training Loss:0.000238\n",
      "Epoch: 146/200, Iteration: 7268/10000 --- Training Loss:0.000472\n",
      "Epoch: 146/200, Iteration: 7270/10000 --- Training Loss:0.000278\n",
      "Epoch: 146/200, Iteration: 7272/10000 --- Training Loss:0.000213\n",
      "Epoch: 146/200, Iteration: 7274/10000 --- Training Loss:0.000338\n",
      "Epoch: 146/200, Iteration: 7276/10000 --- Training Loss:0.000233\n",
      "Epoch: 146/200, Iteration: 7278/10000 --- Training Loss:0.000372\n",
      "Epoch: 146/200, Iteration: 7280/10000 --- Training Loss:0.000309\n",
      "Epoch: 146/200, Iteration: 7282/10000 --- Training Loss:0.000170\n",
      "Epoch: 146/200, Iteration: 7284/10000 --- Training Loss:0.000151\n",
      "Epoch: 146/200, Iteration: 7286/10000 --- Training Loss:0.000383\n",
      "Epoch: 146/200, Iteration: 7288/10000 --- Training Loss:0.000269\n",
      "Epoch: 146/200, Iteration: 7290/10000 --- Training Loss:0.000212\n",
      "Epoch: 146/200, Iteration: 7292/10000 --- Training Loss:0.000227\n",
      "Epoch: 146/200, Iteration: 7294/10000 --- Training Loss:0.000278\n",
      "Epoch: 146/200, Iteration: 7296/10000 --- Training Loss:0.000187\n",
      "Epoch: 146/200, Iteration: 7298/10000 --- Training Loss:0.000887\n",
      "Epoch: 146/200, Iteration: 7300/10000 --- Training Loss:0.000227\n",
      "Epoch: 146 finished ! Train Loss: 0.00026, Test Loss: 0.00021\n",
      "Epoch consuming time: 0m 1s\n",
      "Trained model saved: 73 percent completed\n",
      "Epoch: 147/200, Iteration: 7302/10000 --- Training Loss:0.000243\n",
      "Epoch: 147/200, Iteration: 7304/10000 --- Training Loss:0.000106\n",
      "Epoch: 147/200, Iteration: 7306/10000 --- Training Loss:0.000202\n",
      "Epoch: 147/200, Iteration: 7308/10000 --- Training Loss:0.000207\n",
      "Epoch: 147/200, Iteration: 7310/10000 --- Training Loss:0.000139\n",
      "Epoch: 147/200, Iteration: 7312/10000 --- Training Loss:0.000130\n",
      "Epoch: 147/200, Iteration: 7314/10000 --- Training Loss:0.000174\n",
      "Epoch: 147/200, Iteration: 7316/10000 --- Training Loss:0.000204\n",
      "Epoch: 147/200, Iteration: 7318/10000 --- Training Loss:0.000166\n",
      "Epoch: 147/200, Iteration: 7320/10000 --- Training Loss:0.000248\n",
      "Epoch: 147/200, Iteration: 7322/10000 --- Training Loss:0.000134\n",
      "Epoch: 147/200, Iteration: 7324/10000 --- Training Loss:0.000149\n",
      "Epoch: 147/200, Iteration: 7326/10000 --- Training Loss:0.000368\n",
      "Epoch: 147/200, Iteration: 7328/10000 --- Training Loss:0.000187\n",
      "Epoch: 147/200, Iteration: 7330/10000 --- Training Loss:0.000134\n",
      "Epoch: 147/200, Iteration: 7332/10000 --- Training Loss:0.000174\n",
      "Epoch: 147/200, Iteration: 7334/10000 --- Training Loss:0.000284\n",
      "Epoch: 147/200, Iteration: 7336/10000 --- Training Loss:0.000170\n",
      "Epoch: 147/200, Iteration: 7338/10000 --- Training Loss:0.000284\n",
      "Epoch: 147/200, Iteration: 7340/10000 --- Training Loss:0.000539\n",
      "Epoch: 147/200, Iteration: 7342/10000 --- Training Loss:0.000313\n",
      "Epoch: 147/200, Iteration: 7344/10000 --- Training Loss:0.000116\n",
      "Epoch: 147/200, Iteration: 7346/10000 --- Training Loss:0.000223\n",
      "Epoch: 147/200, Iteration: 7348/10000 --- Training Loss:0.000253\n",
      "Epoch: 147/200, Iteration: 7350/10000 --- Training Loss:0.000225\n",
      "Epoch: 147 finished ! Train Loss: 0.00025, Test Loss: 0.00162\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 148/200, Iteration: 7352/10000 --- Training Loss:0.000231\n",
      "Epoch: 148/200, Iteration: 7354/10000 --- Training Loss:0.000243\n",
      "Epoch: 148/200, Iteration: 7356/10000 --- Training Loss:0.000355\n",
      "Epoch: 148/200, Iteration: 7358/10000 --- Training Loss:0.000268\n",
      "Epoch: 148/200, Iteration: 7360/10000 --- Training Loss:0.000170\n",
      "Epoch: 148/200, Iteration: 7362/10000 --- Training Loss:0.000261\n",
      "Epoch: 148/200, Iteration: 7364/10000 --- Training Loss:0.000191\n",
      "Epoch: 148/200, Iteration: 7366/10000 --- Training Loss:0.000171\n",
      "Epoch: 148/200, Iteration: 7368/10000 --- Training Loss:0.000113\n",
      "Epoch: 148/200, Iteration: 7370/10000 --- Training Loss:0.000126\n",
      "Epoch: 148/200, Iteration: 7372/10000 --- Training Loss:0.000392\n",
      "Epoch: 148/200, Iteration: 7374/10000 --- Training Loss:0.000249\n",
      "Epoch: 148/200, Iteration: 7376/10000 --- Training Loss:0.000192\n",
      "Epoch: 148/200, Iteration: 7378/10000 --- Training Loss:0.000209\n",
      "Epoch: 148/200, Iteration: 7380/10000 --- Training Loss:0.000167\n",
      "Epoch: 148/200, Iteration: 7382/10000 --- Training Loss:0.000178\n",
      "Epoch: 148/200, Iteration: 7384/10000 --- Training Loss:0.000379\n",
      "Epoch: 148/200, Iteration: 7386/10000 --- Training Loss:0.000196\n",
      "Epoch: 148/200, Iteration: 7388/10000 --- Training Loss:0.000210\n",
      "Epoch: 148/200, Iteration: 7390/10000 --- Training Loss:0.000179\n",
      "Epoch: 148/200, Iteration: 7392/10000 --- Training Loss:0.000215\n",
      "Epoch: 148/200, Iteration: 7394/10000 --- Training Loss:0.000155\n",
      "Epoch: 148/200, Iteration: 7396/10000 --- Training Loss:0.000141\n",
      "Epoch: 148/200, Iteration: 7398/10000 --- Training Loss:0.000108\n",
      "Epoch: 148/200, Iteration: 7400/10000 --- Training Loss:0.000121\n",
      "Epoch: 148 finished ! Train Loss: 0.00019, Test Loss: 0.00057\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 149/200, Iteration: 7402/10000 --- Training Loss:0.000103\n",
      "Epoch: 149/200, Iteration: 7404/10000 --- Training Loss:0.000118\n",
      "Epoch: 149/200, Iteration: 7406/10000 --- Training Loss:0.000127\n",
      "Epoch: 149/200, Iteration: 7408/10000 --- Training Loss:0.000111\n",
      "Epoch: 149/200, Iteration: 7410/10000 --- Training Loss:0.000204\n",
      "Epoch: 149/200, Iteration: 7412/10000 --- Training Loss:0.000184\n",
      "Epoch: 149/200, Iteration: 7414/10000 --- Training Loss:0.000106\n",
      "Epoch: 149/200, Iteration: 7416/10000 --- Training Loss:0.000152\n",
      "Epoch: 149/200, Iteration: 7418/10000 --- Training Loss:0.000110\n",
      "Epoch: 149/200, Iteration: 7420/10000 --- Training Loss:0.000234\n",
      "Epoch: 149/200, Iteration: 7422/10000 --- Training Loss:0.000224\n",
      "Epoch: 149/200, Iteration: 7424/10000 --- Training Loss:0.000319\n",
      "Epoch: 149/200, Iteration: 7426/10000 --- Training Loss:0.000107\n",
      "Epoch: 149/200, Iteration: 7428/10000 --- Training Loss:0.000113\n",
      "Epoch: 149/200, Iteration: 7430/10000 --- Training Loss:0.000407\n",
      "Epoch: 149/200, Iteration: 7432/10000 --- Training Loss:0.000193\n",
      "Epoch: 149/200, Iteration: 7434/10000 --- Training Loss:0.000221\n",
      "Epoch: 149/200, Iteration: 7436/10000 --- Training Loss:0.000112\n",
      "Epoch: 149/200, Iteration: 7438/10000 --- Training Loss:0.000135\n",
      "Epoch: 149/200, Iteration: 7440/10000 --- Training Loss:0.000101\n",
      "Epoch: 149/200, Iteration: 7442/10000 --- Training Loss:0.000082\n",
      "Epoch: 149/200, Iteration: 7444/10000 --- Training Loss:0.000104\n",
      "Epoch: 149/200, Iteration: 7446/10000 --- Training Loss:0.000108\n",
      "Epoch: 149/200, Iteration: 7448/10000 --- Training Loss:0.000125\n",
      "Epoch: 149/200, Iteration: 7450/10000 --- Training Loss:0.000085\n",
      "Epoch: 149 finished ! Train Loss: 0.00014, Test Loss: 0.00118\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 150/200, Iteration: 7452/10000 --- Training Loss:0.000097\n",
      "Epoch: 150/200, Iteration: 7454/10000 --- Training Loss:0.000099\n",
      "Epoch: 150/200, Iteration: 7456/10000 --- Training Loss:0.000078\n",
      "Epoch: 150/200, Iteration: 7458/10000 --- Training Loss:0.000135\n",
      "Epoch: 150/200, Iteration: 7460/10000 --- Training Loss:0.000080\n",
      "Epoch: 150/200, Iteration: 7462/10000 --- Training Loss:0.000075\n",
      "Epoch: 150/200, Iteration: 7464/10000 --- Training Loss:0.000209\n",
      "Epoch: 150/200, Iteration: 7466/10000 --- Training Loss:0.000109\n",
      "Epoch: 150/200, Iteration: 7468/10000 --- Training Loss:0.000109\n",
      "Epoch: 150/200, Iteration: 7470/10000 --- Training Loss:0.000161\n",
      "Epoch: 150/200, Iteration: 7472/10000 --- Training Loss:0.000108\n",
      "Epoch: 150/200, Iteration: 7474/10000 --- Training Loss:0.000091\n",
      "Epoch: 150/200, Iteration: 7476/10000 --- Training Loss:0.000078\n",
      "Epoch: 150/200, Iteration: 7478/10000 --- Training Loss:0.000135\n",
      "Epoch: 150/200, Iteration: 7480/10000 --- Training Loss:0.000158\n",
      "Epoch: 150/200, Iteration: 7482/10000 --- Training Loss:0.000101\n",
      "Epoch: 150/200, Iteration: 7484/10000 --- Training Loss:0.000121\n",
      "Epoch: 150/200, Iteration: 7486/10000 --- Training Loss:0.000066\n",
      "Epoch: 150/200, Iteration: 7488/10000 --- Training Loss:0.000169\n",
      "Epoch: 150/200, Iteration: 7490/10000 --- Training Loss:0.000125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150/200, Iteration: 7492/10000 --- Training Loss:0.000132\n",
      "Epoch: 150/200, Iteration: 7494/10000 --- Training Loss:0.000095\n",
      "Epoch: 150/200, Iteration: 7496/10000 --- Training Loss:0.000059\n",
      "Epoch: 150/200, Iteration: 7498/10000 --- Training Loss:0.000070\n",
      "Epoch: 150/200, Iteration: 7500/10000 --- Training Loss:0.000083\n",
      "Epoch: 150 finished ! Train Loss: 0.00012, Test Loss: 0.00164\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 151/200, Iteration: 7502/10000 --- Training Loss:0.000052\n",
      "Epoch: 151/200, Iteration: 7504/10000 --- Training Loss:0.000106\n",
      "Epoch: 151/200, Iteration: 7506/10000 --- Training Loss:0.000107\n",
      "Epoch: 151/200, Iteration: 7508/10000 --- Training Loss:0.000095\n",
      "Epoch: 151/200, Iteration: 7510/10000 --- Training Loss:0.000132\n",
      "Epoch: 151/200, Iteration: 7512/10000 --- Training Loss:0.000217\n",
      "Epoch: 151/200, Iteration: 7514/10000 --- Training Loss:0.000197\n",
      "Epoch: 151/200, Iteration: 7516/10000 --- Training Loss:0.000148\n",
      "Epoch: 151/200, Iteration: 7518/10000 --- Training Loss:0.000165\n",
      "Epoch: 151/200, Iteration: 7520/10000 --- Training Loss:0.000145\n",
      "Epoch: 151/200, Iteration: 7522/10000 --- Training Loss:0.000150\n",
      "Epoch: 151/200, Iteration: 7524/10000 --- Training Loss:0.000146\n",
      "Epoch: 151/200, Iteration: 7526/10000 --- Training Loss:0.000132\n",
      "Epoch: 151/200, Iteration: 7528/10000 --- Training Loss:0.000183\n",
      "Epoch: 151/200, Iteration: 7530/10000 --- Training Loss:0.000075\n",
      "Epoch: 151/200, Iteration: 7532/10000 --- Training Loss:0.000109\n",
      "Epoch: 151/200, Iteration: 7534/10000 --- Training Loss:0.000127\n",
      "Epoch: 151/200, Iteration: 7536/10000 --- Training Loss:0.000110\n",
      "Epoch: 151/200, Iteration: 7538/10000 --- Training Loss:0.000086\n",
      "Epoch: 151/200, Iteration: 7540/10000 --- Training Loss:0.000102\n",
      "Epoch: 151/200, Iteration: 7542/10000 --- Training Loss:0.000086\n",
      "Epoch: 151/200, Iteration: 7544/10000 --- Training Loss:0.000056\n",
      "Epoch: 151/200, Iteration: 7546/10000 --- Training Loss:0.000108\n",
      "Epoch: 151/200, Iteration: 7548/10000 --- Training Loss:0.000117\n",
      "Epoch: 151/200, Iteration: 7550/10000 --- Training Loss:0.000100\n",
      "Epoch: 151 finished ! Train Loss: 0.00013, Test Loss: 0.00165\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 152/200, Iteration: 7552/10000 --- Training Loss:0.000069\n",
      "Epoch: 152/200, Iteration: 7554/10000 --- Training Loss:0.000085\n",
      "Epoch: 152/200, Iteration: 7556/10000 --- Training Loss:0.000084\n",
      "Epoch: 152/200, Iteration: 7558/10000 --- Training Loss:0.000152\n",
      "Epoch: 152/200, Iteration: 7560/10000 --- Training Loss:0.000074\n",
      "Epoch: 152/200, Iteration: 7562/10000 --- Training Loss:0.000152\n",
      "Epoch: 152/200, Iteration: 7564/10000 --- Training Loss:0.000098\n",
      "Epoch: 152/200, Iteration: 7566/10000 --- Training Loss:0.000062\n",
      "Epoch: 152/200, Iteration: 7568/10000 --- Training Loss:0.000106\n",
      "Epoch: 152/200, Iteration: 7570/10000 --- Training Loss:0.000077\n",
      "Epoch: 152/200, Iteration: 7572/10000 --- Training Loss:0.000240\n",
      "Epoch: 152/200, Iteration: 7574/10000 --- Training Loss:0.000113\n",
      "Epoch: 152/200, Iteration: 7576/10000 --- Training Loss:0.000094\n",
      "Epoch: 152/200, Iteration: 7578/10000 --- Training Loss:0.000089\n",
      "Epoch: 152/200, Iteration: 7580/10000 --- Training Loss:0.000098\n",
      "Epoch: 152/200, Iteration: 7582/10000 --- Training Loss:0.000064\n",
      "Epoch: 152/200, Iteration: 7584/10000 --- Training Loss:0.000059\n",
      "Epoch: 152/200, Iteration: 7586/10000 --- Training Loss:0.000064\n",
      "Epoch: 152/200, Iteration: 7588/10000 --- Training Loss:0.000112\n",
      "Epoch: 152/200, Iteration: 7590/10000 --- Training Loss:0.000097\n",
      "Epoch: 152/200, Iteration: 7592/10000 --- Training Loss:0.000601\n",
      "Epoch: 152/200, Iteration: 7594/10000 --- Training Loss:0.000111\n",
      "Epoch: 152/200, Iteration: 7596/10000 --- Training Loss:0.000193\n",
      "Epoch: 152/200, Iteration: 7598/10000 --- Training Loss:0.000150\n",
      "Epoch: 152/200, Iteration: 7600/10000 --- Training Loss:0.000084\n",
      "Epoch: 152 finished ! Train Loss: 0.00011, Test Loss: 0.00066\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 153/200, Iteration: 7602/10000 --- Training Loss:0.000109\n",
      "Epoch: 153/200, Iteration: 7604/10000 --- Training Loss:0.000108\n",
      "Epoch: 153/200, Iteration: 7606/10000 --- Training Loss:0.000192\n",
      "Epoch: 153/200, Iteration: 7608/10000 --- Training Loss:0.000178\n",
      "Epoch: 153/200, Iteration: 7610/10000 --- Training Loss:0.000168\n",
      "Epoch: 153/200, Iteration: 7612/10000 --- Training Loss:0.000223\n",
      "Epoch: 153/200, Iteration: 7614/10000 --- Training Loss:0.000095\n",
      "Epoch: 153/200, Iteration: 7616/10000 --- Training Loss:0.000185\n",
      "Epoch: 153/200, Iteration: 7618/10000 --- Training Loss:0.000080\n",
      "Epoch: 153/200, Iteration: 7620/10000 --- Training Loss:0.000143\n",
      "Epoch: 153/200, Iteration: 7622/10000 --- Training Loss:0.000159\n",
      "Epoch: 153/200, Iteration: 7624/10000 --- Training Loss:0.000075\n",
      "Epoch: 153/200, Iteration: 7626/10000 --- Training Loss:0.000157\n",
      "Epoch: 153/200, Iteration: 7628/10000 --- Training Loss:0.000088\n",
      "Epoch: 153/200, Iteration: 7630/10000 --- Training Loss:0.000096\n",
      "Epoch: 153/200, Iteration: 7632/10000 --- Training Loss:0.000115\n",
      "Epoch: 153/200, Iteration: 7634/10000 --- Training Loss:0.000082\n",
      "Epoch: 153/200, Iteration: 7636/10000 --- Training Loss:0.000119\n",
      "Epoch: 153/200, Iteration: 7638/10000 --- Training Loss:0.000131\n",
      "Epoch: 153/200, Iteration: 7640/10000 --- Training Loss:0.000065\n",
      "Epoch: 153/200, Iteration: 7642/10000 --- Training Loss:0.000151\n",
      "Epoch: 153/200, Iteration: 7644/10000 --- Training Loss:0.000071\n",
      "Epoch: 153/200, Iteration: 7646/10000 --- Training Loss:0.000085\n",
      "Epoch: 153/200, Iteration: 7648/10000 --- Training Loss:0.000186\n",
      "Epoch: 153/200, Iteration: 7650/10000 --- Training Loss:0.000089\n",
      "Epoch: 153 finished ! Train Loss: 0.00013, Test Loss: 0.00230\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 154/200, Iteration: 7652/10000 --- Training Loss:0.000088\n",
      "Epoch: 154/200, Iteration: 7654/10000 --- Training Loss:0.000141\n",
      "Epoch: 154/200, Iteration: 7656/10000 --- Training Loss:0.000125\n",
      "Epoch: 154/200, Iteration: 7658/10000 --- Training Loss:0.000117\n",
      "Epoch: 154/200, Iteration: 7660/10000 --- Training Loss:0.000118\n",
      "Epoch: 154/200, Iteration: 7662/10000 --- Training Loss:0.000145\n",
      "Epoch: 154/200, Iteration: 7664/10000 --- Training Loss:0.000141\n",
      "Epoch: 154/200, Iteration: 7666/10000 --- Training Loss:0.000081\n",
      "Epoch: 154/200, Iteration: 7668/10000 --- Training Loss:0.000114\n",
      "Epoch: 154/200, Iteration: 7670/10000 --- Training Loss:0.000077\n",
      "Epoch: 154/200, Iteration: 7672/10000 --- Training Loss:0.000184\n",
      "Epoch: 154/200, Iteration: 7674/10000 --- Training Loss:0.000102\n",
      "Epoch: 154/200, Iteration: 7676/10000 --- Training Loss:0.000135\n",
      "Epoch: 154/200, Iteration: 7678/10000 --- Training Loss:0.000159\n",
      "Epoch: 154/200, Iteration: 7680/10000 --- Training Loss:0.000122\n",
      "Epoch: 154/200, Iteration: 7682/10000 --- Training Loss:0.000095\n",
      "Epoch: 154/200, Iteration: 7684/10000 --- Training Loss:0.000146\n",
      "Epoch: 154/200, Iteration: 7686/10000 --- Training Loss:0.000091\n",
      "Epoch: 154/200, Iteration: 7688/10000 --- Training Loss:0.000123\n",
      "Epoch: 154/200, Iteration: 7690/10000 --- Training Loss:0.000075\n",
      "Epoch: 154/200, Iteration: 7692/10000 --- Training Loss:0.000123\n",
      "Epoch: 154/200, Iteration: 7694/10000 --- Training Loss:0.000149\n",
      "Epoch: 154/200, Iteration: 7696/10000 --- Training Loss:0.000112\n",
      "Epoch: 154/200, Iteration: 7698/10000 --- Training Loss:0.000278\n",
      "Epoch: 154/200, Iteration: 7700/10000 --- Training Loss:0.000094\n",
      "Epoch: 154 finished ! Train Loss: 0.00013, Test Loss: 0.00236\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 155/200, Iteration: 7702/10000 --- Training Loss:0.000114\n",
      "Epoch: 155/200, Iteration: 7704/10000 --- Training Loss:0.000074\n",
      "Epoch: 155/200, Iteration: 7706/10000 --- Training Loss:0.000129\n",
      "Epoch: 155/200, Iteration: 7708/10000 --- Training Loss:0.000154\n",
      "Epoch: 155/200, Iteration: 7710/10000 --- Training Loss:0.000094\n",
      "Epoch: 155/200, Iteration: 7712/10000 --- Training Loss:0.000100\n",
      "Epoch: 155/200, Iteration: 7714/10000 --- Training Loss:0.000128\n",
      "Epoch: 155/200, Iteration: 7716/10000 --- Training Loss:0.000097\n",
      "Epoch: 155/200, Iteration: 7718/10000 --- Training Loss:0.000086\n",
      "Epoch: 155/200, Iteration: 7720/10000 --- Training Loss:0.000158\n",
      "Epoch: 155/200, Iteration: 7722/10000 --- Training Loss:0.000103\n",
      "Epoch: 155/200, Iteration: 7724/10000 --- Training Loss:0.000145\n",
      "Epoch: 155/200, Iteration: 7726/10000 --- Training Loss:0.000131\n",
      "Epoch: 155/200, Iteration: 7728/10000 --- Training Loss:0.000138\n",
      "Epoch: 155/200, Iteration: 7730/10000 --- Training Loss:0.000082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155/200, Iteration: 7732/10000 --- Training Loss:0.000224\n",
      "Epoch: 155/200, Iteration: 7734/10000 --- Training Loss:0.000130\n",
      "Epoch: 155/200, Iteration: 7736/10000 --- Training Loss:0.000114\n",
      "Epoch: 155/200, Iteration: 7738/10000 --- Training Loss:0.000165\n",
      "Epoch: 155/200, Iteration: 7740/10000 --- Training Loss:0.000104\n",
      "Epoch: 155/200, Iteration: 7742/10000 --- Training Loss:0.000102\n",
      "Epoch: 155/200, Iteration: 7744/10000 --- Training Loss:0.000103\n",
      "Epoch: 155/200, Iteration: 7746/10000 --- Training Loss:0.000083\n",
      "Epoch: 155/200, Iteration: 7748/10000 --- Training Loss:0.000086\n",
      "Epoch: 155/200, Iteration: 7750/10000 --- Training Loss:0.000070\n",
      "Epoch: 155 finished ! Train Loss: 0.00013, Test Loss: 0.00154\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 156/200, Iteration: 7752/10000 --- Training Loss:0.000084\n",
      "Epoch: 156/200, Iteration: 7754/10000 --- Training Loss:0.000313\n",
      "Epoch: 156/200, Iteration: 7756/10000 --- Training Loss:0.000200\n",
      "Epoch: 156/200, Iteration: 7758/10000 --- Training Loss:0.000142\n",
      "Epoch: 156/200, Iteration: 7760/10000 --- Training Loss:0.000083\n",
      "Epoch: 156/200, Iteration: 7762/10000 --- Training Loss:0.000106\n",
      "Epoch: 156/200, Iteration: 7764/10000 --- Training Loss:0.000121\n",
      "Epoch: 156/200, Iteration: 7766/10000 --- Training Loss:0.000138\n",
      "Epoch: 156/200, Iteration: 7768/10000 --- Training Loss:0.000116\n",
      "Epoch: 156/200, Iteration: 7770/10000 --- Training Loss:0.000128\n",
      "Epoch: 156/200, Iteration: 7772/10000 --- Training Loss:0.000148\n",
      "Epoch: 156/200, Iteration: 7774/10000 --- Training Loss:0.000142\n",
      "Epoch: 156/200, Iteration: 7776/10000 --- Training Loss:0.000136\n",
      "Epoch: 156/200, Iteration: 7778/10000 --- Training Loss:0.000093\n",
      "Epoch: 156/200, Iteration: 7780/10000 --- Training Loss:0.000118\n",
      "Epoch: 156/200, Iteration: 7782/10000 --- Training Loss:0.000232\n",
      "Epoch: 156/200, Iteration: 7784/10000 --- Training Loss:0.000154\n",
      "Epoch: 156/200, Iteration: 7786/10000 --- Training Loss:0.000082\n",
      "Epoch: 156/200, Iteration: 7788/10000 --- Training Loss:0.000428\n",
      "Epoch: 156/200, Iteration: 7790/10000 --- Training Loss:0.001383\n",
      "Epoch: 156/200, Iteration: 7792/10000 --- Training Loss:0.000607\n",
      "Epoch: 156/200, Iteration: 7794/10000 --- Training Loss:0.001566\n",
      "Epoch: 156/200, Iteration: 7796/10000 --- Training Loss:0.001325\n",
      "Epoch: 156/200, Iteration: 7798/10000 --- Training Loss:0.005246\n",
      "Epoch: 156/200, Iteration: 7800/10000 --- Training Loss:0.007588\n",
      "Epoch: 156 finished ! Train Loss: 0.00071, Test Loss: 0.15893\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 157/200, Iteration: 7802/10000 --- Training Loss:0.003384\n",
      "Epoch: 157/200, Iteration: 7804/10000 --- Training Loss:0.006130\n",
      "Epoch: 157/200, Iteration: 7806/10000 --- Training Loss:0.002163\n",
      "Epoch: 157/200, Iteration: 7808/10000 --- Training Loss:0.002241\n",
      "Epoch: 157/200, Iteration: 7810/10000 --- Training Loss:0.001692\n",
      "Epoch: 157/200, Iteration: 7812/10000 --- Training Loss:0.008098\n",
      "Epoch: 157/200, Iteration: 7814/10000 --- Training Loss:0.003115\n",
      "Epoch: 157/200, Iteration: 7816/10000 --- Training Loss:0.002320\n",
      "Epoch: 157/200, Iteration: 7818/10000 --- Training Loss:0.003798\n",
      "Epoch: 157/200, Iteration: 7820/10000 --- Training Loss:0.002334\n",
      "Epoch: 157/200, Iteration: 7822/10000 --- Training Loss:0.002368\n",
      "Epoch: 157/200, Iteration: 7824/10000 --- Training Loss:0.002500\n",
      "Epoch: 157/200, Iteration: 7826/10000 --- Training Loss:0.001645\n",
      "Epoch: 157/200, Iteration: 7828/10000 --- Training Loss:0.001553\n",
      "Epoch: 157/200, Iteration: 7830/10000 --- Training Loss:0.001068\n",
      "Epoch: 157/200, Iteration: 7832/10000 --- Training Loss:0.001074\n",
      "Epoch: 157/200, Iteration: 7834/10000 --- Training Loss:0.001035\n",
      "Epoch: 157/200, Iteration: 7836/10000 --- Training Loss:0.000809\n",
      "Epoch: 157/200, Iteration: 7838/10000 --- Training Loss:0.000596\n",
      "Epoch: 157/200, Iteration: 7840/10000 --- Training Loss:0.000926\n",
      "Epoch: 157/200, Iteration: 7842/10000 --- Training Loss:0.001334\n",
      "Epoch: 157/200, Iteration: 7844/10000 --- Training Loss:0.000796\n",
      "Epoch: 157/200, Iteration: 7846/10000 --- Training Loss:0.000875\n",
      "Epoch: 157/200, Iteration: 7848/10000 --- Training Loss:0.000744\n",
      "Epoch: 157/200, Iteration: 7850/10000 --- Training Loss:0.000994\n",
      "Epoch: 157 finished ! Train Loss: 0.00206, Test Loss: 0.00077\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 158/200, Iteration: 7852/10000 --- Training Loss:0.000505\n",
      "Epoch: 158/200, Iteration: 7854/10000 --- Training Loss:0.001090\n",
      "Epoch: 158/200, Iteration: 7856/10000 --- Training Loss:0.001318\n",
      "Epoch: 158/200, Iteration: 7858/10000 --- Training Loss:0.000906\n",
      "Epoch: 158/200, Iteration: 7860/10000 --- Training Loss:0.000533\n",
      "Epoch: 158/200, Iteration: 7862/10000 --- Training Loss:0.000782\n",
      "Epoch: 158/200, Iteration: 7864/10000 --- Training Loss:0.000557\n",
      "Epoch: 158/200, Iteration: 7866/10000 --- Training Loss:0.000555\n",
      "Epoch: 158/200, Iteration: 7868/10000 --- Training Loss:0.000426\n",
      "Epoch: 158/200, Iteration: 7870/10000 --- Training Loss:0.000474\n",
      "Epoch: 158/200, Iteration: 7872/10000 --- Training Loss:0.000373\n",
      "Epoch: 158/200, Iteration: 7874/10000 --- Training Loss:0.000439\n",
      "Epoch: 158/200, Iteration: 7876/10000 --- Training Loss:0.000432\n",
      "Epoch: 158/200, Iteration: 7878/10000 --- Training Loss:0.000400\n",
      "Epoch: 158/200, Iteration: 7880/10000 --- Training Loss:0.000485\n",
      "Epoch: 158/200, Iteration: 7882/10000 --- Training Loss:0.000381\n",
      "Epoch: 158/200, Iteration: 7884/10000 --- Training Loss:0.000798\n",
      "Epoch: 158/200, Iteration: 7886/10000 --- Training Loss:0.000280\n",
      "Epoch: 158/200, Iteration: 7888/10000 --- Training Loss:0.000282\n",
      "Epoch: 158/200, Iteration: 7890/10000 --- Training Loss:0.000186\n",
      "Epoch: 158/200, Iteration: 7892/10000 --- Training Loss:0.000264\n",
      "Epoch: 158/200, Iteration: 7894/10000 --- Training Loss:0.000595\n",
      "Epoch: 158/200, Iteration: 7896/10000 --- Training Loss:0.000309\n",
      "Epoch: 158/200, Iteration: 7898/10000 --- Training Loss:0.000617\n",
      "Epoch: 158/200, Iteration: 7900/10000 --- Training Loss:0.000234\n",
      "Epoch: 158 finished ! Train Loss: 0.00062, Test Loss: 0.00093\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 159/200, Iteration: 7902/10000 --- Training Loss:0.000300\n",
      "Epoch: 159/200, Iteration: 7904/10000 --- Training Loss:0.000236\n",
      "Epoch: 159/200, Iteration: 7906/10000 --- Training Loss:0.000399\n",
      "Epoch: 159/200, Iteration: 7908/10000 --- Training Loss:0.000587\n",
      "Epoch: 159/200, Iteration: 7910/10000 --- Training Loss:0.000268\n",
      "Epoch: 159/200, Iteration: 7912/10000 --- Training Loss:0.000254\n",
      "Epoch: 159/200, Iteration: 7914/10000 --- Training Loss:0.000318\n",
      "Epoch: 159/200, Iteration: 7916/10000 --- Training Loss:0.000306\n",
      "Epoch: 159/200, Iteration: 7918/10000 --- Training Loss:0.000346\n",
      "Epoch: 159/200, Iteration: 7920/10000 --- Training Loss:0.000300\n",
      "Epoch: 159/200, Iteration: 7922/10000 --- Training Loss:0.000254\n",
      "Epoch: 159/200, Iteration: 7924/10000 --- Training Loss:0.000254\n",
      "Epoch: 159/200, Iteration: 7926/10000 --- Training Loss:0.000505\n",
      "Epoch: 159/200, Iteration: 7928/10000 --- Training Loss:0.000270\n",
      "Epoch: 159/200, Iteration: 7930/10000 --- Training Loss:0.000302\n",
      "Epoch: 159/200, Iteration: 7932/10000 --- Training Loss:0.000347\n",
      "Epoch: 159/200, Iteration: 7934/10000 --- Training Loss:0.000239\n",
      "Epoch: 159/200, Iteration: 7936/10000 --- Training Loss:0.000248\n",
      "Epoch: 159/200, Iteration: 7938/10000 --- Training Loss:0.000537\n",
      "Epoch: 159/200, Iteration: 7940/10000 --- Training Loss:0.000311\n",
      "Epoch: 159/200, Iteration: 7942/10000 --- Training Loss:0.000337\n",
      "Epoch: 159/200, Iteration: 7944/10000 --- Training Loss:0.000326\n",
      "Epoch: 159/200, Iteration: 7946/10000 --- Training Loss:0.000376\n",
      "Epoch: 159/200, Iteration: 7948/10000 --- Training Loss:0.000314\n",
      "Epoch: 159/200, Iteration: 7950/10000 --- Training Loss:0.000200\n",
      "Epoch: 159 finished ! Train Loss: 0.00038, Test Loss: 0.00421\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 160/200, Iteration: 7952/10000 --- Training Loss:0.000170\n",
      "Epoch: 160/200, Iteration: 7954/10000 --- Training Loss:0.000291\n",
      "Epoch: 160/200, Iteration: 7956/10000 --- Training Loss:0.000373\n",
      "Epoch: 160/200, Iteration: 7958/10000 --- Training Loss:0.000311\n",
      "Epoch: 160/200, Iteration: 7960/10000 --- Training Loss:0.000298\n",
      "Epoch: 160/200, Iteration: 7962/10000 --- Training Loss:0.000315\n",
      "Epoch: 160/200, Iteration: 7964/10000 --- Training Loss:0.000294\n",
      "Epoch: 160/200, Iteration: 7966/10000 --- Training Loss:0.000181\n",
      "Epoch: 160/200, Iteration: 7968/10000 --- Training Loss:0.000317\n",
      "Epoch: 160/200, Iteration: 7970/10000 --- Training Loss:0.000171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160/200, Iteration: 7972/10000 --- Training Loss:0.000163\n",
      "Epoch: 160/200, Iteration: 7974/10000 --- Training Loss:0.000940\n",
      "Epoch: 160/200, Iteration: 7976/10000 --- Training Loss:0.000210\n",
      "Epoch: 160/200, Iteration: 7978/10000 --- Training Loss:0.000660\n",
      "Epoch: 160/200, Iteration: 7980/10000 --- Training Loss:0.000309\n",
      "Epoch: 160/200, Iteration: 7982/10000 --- Training Loss:0.000349\n",
      "Epoch: 160/200, Iteration: 7984/10000 --- Training Loss:0.000352\n",
      "Epoch: 160/200, Iteration: 7986/10000 --- Training Loss:0.000459\n",
      "Epoch: 160/200, Iteration: 7988/10000 --- Training Loss:0.000692\n",
      "Epoch: 160/200, Iteration: 7990/10000 --- Training Loss:0.000282\n",
      "Epoch: 160/200, Iteration: 7992/10000 --- Training Loss:0.000192\n",
      "Epoch: 160/200, Iteration: 7994/10000 --- Training Loss:0.000430\n",
      "Epoch: 160/200, Iteration: 7996/10000 --- Training Loss:0.000236\n",
      "Epoch: 160/200, Iteration: 7998/10000 --- Training Loss:0.000251\n",
      "Epoch: 160/200, Iteration: 8000/10000 --- Training Loss:0.000274\n",
      "Epoch: 160 finished ! Train Loss: 0.00032, Test Loss: 0.00201\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 161/200, Iteration: 8002/10000 --- Training Loss:0.000295\n",
      "Epoch: 161/200, Iteration: 8004/10000 --- Training Loss:0.000268\n",
      "Epoch: 161/200, Iteration: 8006/10000 --- Training Loss:0.000205\n",
      "Epoch: 161/200, Iteration: 8008/10000 --- Training Loss:0.000812\n",
      "Epoch: 161/200, Iteration: 8010/10000 --- Training Loss:0.000168\n",
      "Epoch: 161/200, Iteration: 8012/10000 --- Training Loss:0.000238\n",
      "Epoch: 161/200, Iteration: 8014/10000 --- Training Loss:0.000099\n",
      "Epoch: 161/200, Iteration: 8016/10000 --- Training Loss:0.000282\n",
      "Epoch: 161/200, Iteration: 8018/10000 --- Training Loss:0.001143\n",
      "Epoch: 161/200, Iteration: 8020/10000 --- Training Loss:0.000659\n",
      "Epoch: 161/200, Iteration: 8022/10000 --- Training Loss:0.000190\n",
      "Epoch: 161/200, Iteration: 8024/10000 --- Training Loss:0.000177\n",
      "Epoch: 161/200, Iteration: 8026/10000 --- Training Loss:0.000314\n",
      "Epoch: 161/200, Iteration: 8028/10000 --- Training Loss:0.000245\n",
      "Epoch: 161/200, Iteration: 8030/10000 --- Training Loss:0.000280\n",
      "Epoch: 161/200, Iteration: 8032/10000 --- Training Loss:0.000194\n",
      "Epoch: 161/200, Iteration: 8034/10000 --- Training Loss:0.000216\n",
      "Epoch: 161/200, Iteration: 8036/10000 --- Training Loss:0.000200\n",
      "Epoch: 161/200, Iteration: 8038/10000 --- Training Loss:0.000218\n",
      "Epoch: 161/200, Iteration: 8040/10000 --- Training Loss:0.000196\n",
      "Epoch: 161/200, Iteration: 8042/10000 --- Training Loss:0.000289\n",
      "Epoch: 161/200, Iteration: 8044/10000 --- Training Loss:0.000576\n",
      "Epoch: 161/200, Iteration: 8046/10000 --- Training Loss:0.000420\n",
      "Epoch: 161/200, Iteration: 8048/10000 --- Training Loss:0.000240\n",
      "Epoch: 161/200, Iteration: 8050/10000 --- Training Loss:0.000188\n",
      "Epoch: 161 finished ! Train Loss: 0.00031, Test Loss: 0.00337\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 162/200, Iteration: 8052/10000 --- Training Loss:0.000191\n",
      "Epoch: 162/200, Iteration: 8054/10000 --- Training Loss:0.000139\n",
      "Epoch: 162/200, Iteration: 8056/10000 --- Training Loss:0.000257\n",
      "Epoch: 162/200, Iteration: 8058/10000 --- Training Loss:0.000211\n",
      "Epoch: 162/200, Iteration: 8060/10000 --- Training Loss:0.000139\n",
      "Epoch: 162/200, Iteration: 8062/10000 --- Training Loss:0.000526\n",
      "Epoch: 162/200, Iteration: 8064/10000 --- Training Loss:0.000188\n",
      "Epoch: 162/200, Iteration: 8066/10000 --- Training Loss:0.000191\n",
      "Epoch: 162/200, Iteration: 8068/10000 --- Training Loss:0.000298\n",
      "Epoch: 162/200, Iteration: 8070/10000 --- Training Loss:0.000168\n",
      "Epoch: 162/200, Iteration: 8072/10000 --- Training Loss:0.000314\n",
      "Epoch: 162/200, Iteration: 8074/10000 --- Training Loss:0.000425\n",
      "Epoch: 162/200, Iteration: 8076/10000 --- Training Loss:0.000208\n",
      "Epoch: 162/200, Iteration: 8078/10000 --- Training Loss:0.000142\n",
      "Epoch: 162/200, Iteration: 8080/10000 --- Training Loss:0.000190\n",
      "Epoch: 162/200, Iteration: 8082/10000 --- Training Loss:0.000338\n",
      "Epoch: 162/200, Iteration: 8084/10000 --- Training Loss:0.000154\n",
      "Epoch: 162/200, Iteration: 8086/10000 --- Training Loss:0.000223\n",
      "Epoch: 162/200, Iteration: 8088/10000 --- Training Loss:0.000149\n",
      "Epoch: 162/200, Iteration: 8090/10000 --- Training Loss:0.000321\n",
      "Epoch: 162/200, Iteration: 8092/10000 --- Training Loss:0.000236\n",
      "Epoch: 162/200, Iteration: 8094/10000 --- Training Loss:0.000162\n",
      "Epoch: 162/200, Iteration: 8096/10000 --- Training Loss:0.000189\n",
      "Epoch: 162/200, Iteration: 8098/10000 --- Training Loss:0.000260\n",
      "Epoch: 162/200, Iteration: 8100/10000 --- Training Loss:0.000120\n",
      "Epoch: 162 finished ! Train Loss: 0.00025, Test Loss: 0.00230\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 163/200, Iteration: 8102/10000 --- Training Loss:0.000156\n",
      "Epoch: 163/200, Iteration: 8104/10000 --- Training Loss:0.000329\n",
      "Epoch: 163/200, Iteration: 8106/10000 --- Training Loss:0.000286\n",
      "Epoch: 163/200, Iteration: 8108/10000 --- Training Loss:0.000175\n",
      "Epoch: 163/200, Iteration: 8110/10000 --- Training Loss:0.000201\n",
      "Epoch: 163/200, Iteration: 8112/10000 --- Training Loss:0.000236\n",
      "Epoch: 163/200, Iteration: 8114/10000 --- Training Loss:0.000129\n",
      "Epoch: 163/200, Iteration: 8116/10000 --- Training Loss:0.000230\n",
      "Epoch: 163/200, Iteration: 8118/10000 --- Training Loss:0.000251\n",
      "Epoch: 163/200, Iteration: 8120/10000 --- Training Loss:0.000215\n",
      "Epoch: 163/200, Iteration: 8122/10000 --- Training Loss:0.000285\n",
      "Epoch: 163/200, Iteration: 8124/10000 --- Training Loss:0.000190\n",
      "Epoch: 163/200, Iteration: 8126/10000 --- Training Loss:0.000197\n",
      "Epoch: 163/200, Iteration: 8128/10000 --- Training Loss:0.000197\n",
      "Epoch: 163/200, Iteration: 8130/10000 --- Training Loss:0.000554\n",
      "Epoch: 163/200, Iteration: 8132/10000 --- Training Loss:0.000175\n",
      "Epoch: 163/200, Iteration: 8134/10000 --- Training Loss:0.000309\n",
      "Epoch: 163/200, Iteration: 8136/10000 --- Training Loss:0.000231\n",
      "Epoch: 163/200, Iteration: 8138/10000 --- Training Loss:0.000131\n",
      "Epoch: 163/200, Iteration: 8140/10000 --- Training Loss:0.000165\n",
      "Epoch: 163/200, Iteration: 8142/10000 --- Training Loss:0.000136\n",
      "Epoch: 163/200, Iteration: 8144/10000 --- Training Loss:0.000161\n",
      "Epoch: 163/200, Iteration: 8146/10000 --- Training Loss:0.000240\n",
      "Epoch: 163/200, Iteration: 8148/10000 --- Training Loss:0.000185\n",
      "Epoch: 163/200, Iteration: 8150/10000 --- Training Loss:0.000248\n",
      "Epoch: 163 finished ! Train Loss: 0.00024, Test Loss: 0.00212\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 164/200, Iteration: 8152/10000 --- Training Loss:0.000182\n",
      "Epoch: 164/200, Iteration: 8154/10000 --- Training Loss:0.000205\n",
      "Epoch: 164/200, Iteration: 8156/10000 --- Training Loss:0.000185\n",
      "Epoch: 164/200, Iteration: 8158/10000 --- Training Loss:0.000120\n",
      "Epoch: 164/200, Iteration: 8160/10000 --- Training Loss:0.000081\n",
      "Epoch: 164/200, Iteration: 8162/10000 --- Training Loss:0.000261\n",
      "Epoch: 164/200, Iteration: 8164/10000 --- Training Loss:0.000145\n",
      "Epoch: 164/200, Iteration: 8166/10000 --- Training Loss:0.000167\n",
      "Epoch: 164/200, Iteration: 8168/10000 --- Training Loss:0.000182\n",
      "Epoch: 164/200, Iteration: 8170/10000 --- Training Loss:0.000409\n",
      "Epoch: 164/200, Iteration: 8172/10000 --- Training Loss:0.000134\n",
      "Epoch: 164/200, Iteration: 8174/10000 --- Training Loss:0.000225\n",
      "Epoch: 164/200, Iteration: 8176/10000 --- Training Loss:0.000130\n",
      "Epoch: 164/200, Iteration: 8178/10000 --- Training Loss:0.000170\n",
      "Epoch: 164/200, Iteration: 8180/10000 --- Training Loss:0.000275\n",
      "Epoch: 164/200, Iteration: 8182/10000 --- Training Loss:0.000092\n",
      "Epoch: 164/200, Iteration: 8184/10000 --- Training Loss:0.000375\n",
      "Epoch: 164/200, Iteration: 8186/10000 --- Training Loss:0.000240\n",
      "Epoch: 164/200, Iteration: 8188/10000 --- Training Loss:0.000175\n",
      "Epoch: 164/200, Iteration: 8190/10000 --- Training Loss:0.000186\n",
      "Epoch: 164/200, Iteration: 8192/10000 --- Training Loss:0.000097\n",
      "Epoch: 164/200, Iteration: 8194/10000 --- Training Loss:0.000195\n",
      "Epoch: 164/200, Iteration: 8196/10000 --- Training Loss:0.000256\n",
      "Epoch: 164/200, Iteration: 8198/10000 --- Training Loss:0.000131\n",
      "Epoch: 164/200, Iteration: 8200/10000 --- Training Loss:0.000228\n",
      "Epoch: 164 finished ! Train Loss: 0.00020, Test Loss: 0.00133\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 165/200, Iteration: 8202/10000 --- Training Loss:0.000115\n",
      "Epoch: 165/200, Iteration: 8204/10000 --- Training Loss:0.000114\n",
      "Epoch: 165/200, Iteration: 8206/10000 --- Training Loss:0.000151\n",
      "Epoch: 165/200, Iteration: 8208/10000 --- Training Loss:0.000131\n",
      "Epoch: 165/200, Iteration: 8210/10000 --- Training Loss:0.000090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 165/200, Iteration: 8212/10000 --- Training Loss:0.000059\n",
      "Epoch: 165/200, Iteration: 8214/10000 --- Training Loss:0.001426\n",
      "Epoch: 165/200, Iteration: 8216/10000 --- Training Loss:0.000170\n",
      "Epoch: 165/200, Iteration: 8218/10000 --- Training Loss:0.000406\n",
      "Epoch: 165/200, Iteration: 8220/10000 --- Training Loss:0.000158\n",
      "Epoch: 165/200, Iteration: 8222/10000 --- Training Loss:0.000320\n",
      "Epoch: 165/200, Iteration: 8224/10000 --- Training Loss:0.000645\n",
      "Epoch: 165/200, Iteration: 8226/10000 --- Training Loss:0.000320\n",
      "Epoch: 165/200, Iteration: 8228/10000 --- Training Loss:0.000329\n",
      "Epoch: 165/200, Iteration: 8230/10000 --- Training Loss:0.000226\n",
      "Epoch: 165/200, Iteration: 8232/10000 --- Training Loss:0.000374\n",
      "Epoch: 165/200, Iteration: 8234/10000 --- Training Loss:0.000338\n",
      "Epoch: 165/200, Iteration: 8236/10000 --- Training Loss:0.000337\n",
      "Epoch: 165/200, Iteration: 8238/10000 --- Training Loss:0.000140\n",
      "Epoch: 165/200, Iteration: 8240/10000 --- Training Loss:0.000245\n",
      "Epoch: 165/200, Iteration: 8242/10000 --- Training Loss:0.000162\n",
      "Epoch: 165/200, Iteration: 8244/10000 --- Training Loss:0.000340\n",
      "Epoch: 165/200, Iteration: 8246/10000 --- Training Loss:0.000162\n",
      "Epoch: 165/200, Iteration: 8248/10000 --- Training Loss:0.000371\n",
      "Epoch: 165/200, Iteration: 8250/10000 --- Training Loss:0.000349\n",
      "Epoch: 165 finished ! Train Loss: 0.00028, Test Loss: 0.00033\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 166/200, Iteration: 8252/10000 --- Training Loss:0.000160\n",
      "Epoch: 166/200, Iteration: 8254/10000 --- Training Loss:0.000145\n",
      "Epoch: 166/200, Iteration: 8256/10000 --- Training Loss:0.000141\n",
      "Epoch: 166/200, Iteration: 8258/10000 --- Training Loss:0.000251\n",
      "Epoch: 166/200, Iteration: 8260/10000 --- Training Loss:0.000159\n",
      "Epoch: 166/200, Iteration: 8262/10000 --- Training Loss:0.000190\n",
      "Epoch: 166/200, Iteration: 8264/10000 --- Training Loss:0.000201\n",
      "Epoch: 166/200, Iteration: 8266/10000 --- Training Loss:0.000210\n",
      "Epoch: 166/200, Iteration: 8268/10000 --- Training Loss:0.000149\n",
      "Epoch: 166/200, Iteration: 8270/10000 --- Training Loss:0.000132\n",
      "Epoch: 166/200, Iteration: 8272/10000 --- Training Loss:0.000140\n",
      "Epoch: 166/200, Iteration: 8274/10000 --- Training Loss:0.000080\n",
      "Epoch: 166/200, Iteration: 8276/10000 --- Training Loss:0.000320\n",
      "Epoch: 166/200, Iteration: 8278/10000 --- Training Loss:0.000085\n",
      "Epoch: 166/200, Iteration: 8280/10000 --- Training Loss:0.000154\n",
      "Epoch: 166/200, Iteration: 8282/10000 --- Training Loss:0.000171\n",
      "Epoch: 166/200, Iteration: 8284/10000 --- Training Loss:0.000120\n",
      "Epoch: 166/200, Iteration: 8286/10000 --- Training Loss:0.000130\n",
      "Epoch: 166/200, Iteration: 8288/10000 --- Training Loss:0.000147\n",
      "Epoch: 166/200, Iteration: 8290/10000 --- Training Loss:0.000135\n",
      "Epoch: 166/200, Iteration: 8292/10000 --- Training Loss:0.000133\n",
      "Epoch: 166/200, Iteration: 8294/10000 --- Training Loss:0.000112\n",
      "Epoch: 166/200, Iteration: 8296/10000 --- Training Loss:0.000106\n",
      "Epoch: 166/200, Iteration: 8298/10000 --- Training Loss:0.000150\n",
      "Epoch: 166/200, Iteration: 8300/10000 --- Training Loss:0.000148\n",
      "Epoch: 166 finished ! Train Loss: 0.00017, Test Loss: 0.00244\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 167/200, Iteration: 8302/10000 --- Training Loss:0.000115\n",
      "Epoch: 167/200, Iteration: 8304/10000 --- Training Loss:0.000160\n",
      "Epoch: 167/200, Iteration: 8306/10000 --- Training Loss:0.000334\n",
      "Epoch: 167/200, Iteration: 8308/10000 --- Training Loss:0.000099\n",
      "Epoch: 167/200, Iteration: 8310/10000 --- Training Loss:0.000161\n",
      "Epoch: 167/200, Iteration: 8312/10000 --- Training Loss:0.000140\n",
      "Epoch: 167/200, Iteration: 8314/10000 --- Training Loss:0.000159\n",
      "Epoch: 167/200, Iteration: 8316/10000 --- Training Loss:0.000113\n",
      "Epoch: 167/200, Iteration: 8318/10000 --- Training Loss:0.000127\n",
      "Epoch: 167/200, Iteration: 8320/10000 --- Training Loss:0.000106\n",
      "Epoch: 167/200, Iteration: 8322/10000 --- Training Loss:0.000211\n",
      "Epoch: 167/200, Iteration: 8324/10000 --- Training Loss:0.000149\n",
      "Epoch: 167/200, Iteration: 8326/10000 --- Training Loss:0.000107\n",
      "Epoch: 167/200, Iteration: 8328/10000 --- Training Loss:0.000145\n",
      "Epoch: 167/200, Iteration: 8330/10000 --- Training Loss:0.000216\n",
      "Epoch: 167/200, Iteration: 8332/10000 --- Training Loss:0.000063\n",
      "Epoch: 167/200, Iteration: 8334/10000 --- Training Loss:0.000114\n",
      "Epoch: 167/200, Iteration: 8336/10000 --- Training Loss:0.000109\n",
      "Epoch: 167/200, Iteration: 8338/10000 --- Training Loss:0.000184\n",
      "Epoch: 167/200, Iteration: 8340/10000 --- Training Loss:0.000244\n",
      "Epoch: 167/200, Iteration: 8342/10000 --- Training Loss:0.000121\n",
      "Epoch: 167/200, Iteration: 8344/10000 --- Training Loss:0.000176\n",
      "Epoch: 167/200, Iteration: 8346/10000 --- Training Loss:0.000232\n",
      "Epoch: 167/200, Iteration: 8348/10000 --- Training Loss:0.000119\n",
      "Epoch: 167/200, Iteration: 8350/10000 --- Training Loss:0.000166\n",
      "Epoch: 167 finished ! Train Loss: 0.00017, Test Loss: 0.00186\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 168/200, Iteration: 8352/10000 --- Training Loss:0.000213\n",
      "Epoch: 168/200, Iteration: 8354/10000 --- Training Loss:0.000145\n",
      "Epoch: 168/200, Iteration: 8356/10000 --- Training Loss:0.000179\n",
      "Epoch: 168/200, Iteration: 8358/10000 --- Training Loss:0.000146\n",
      "Epoch: 168/200, Iteration: 8360/10000 --- Training Loss:0.000164\n",
      "Epoch: 168/200, Iteration: 8362/10000 --- Training Loss:0.000127\n",
      "Epoch: 168/200, Iteration: 8364/10000 --- Training Loss:0.000190\n",
      "Epoch: 168/200, Iteration: 8366/10000 --- Training Loss:0.000179\n",
      "Epoch: 168/200, Iteration: 8368/10000 --- Training Loss:0.000201\n",
      "Epoch: 168/200, Iteration: 8370/10000 --- Training Loss:0.000185\n",
      "Epoch: 168/200, Iteration: 8372/10000 --- Training Loss:0.000092\n",
      "Epoch: 168/200, Iteration: 8374/10000 --- Training Loss:0.000137\n",
      "Epoch: 168/200, Iteration: 8376/10000 --- Training Loss:0.000193\n",
      "Epoch: 168/200, Iteration: 8378/10000 --- Training Loss:0.000194\n",
      "Epoch: 168/200, Iteration: 8380/10000 --- Training Loss:0.000109\n",
      "Epoch: 168/200, Iteration: 8382/10000 --- Training Loss:0.000287\n",
      "Epoch: 168/200, Iteration: 8384/10000 --- Training Loss:0.000181\n",
      "Epoch: 168/200, Iteration: 8386/10000 --- Training Loss:0.000245\n",
      "Epoch: 168/200, Iteration: 8388/10000 --- Training Loss:0.000088\n",
      "Epoch: 168/200, Iteration: 8390/10000 --- Training Loss:0.000191\n",
      "Epoch: 168/200, Iteration: 8392/10000 --- Training Loss:0.000197\n",
      "Epoch: 168/200, Iteration: 8394/10000 --- Training Loss:0.000212\n",
      "Epoch: 168/200, Iteration: 8396/10000 --- Training Loss:0.000124\n",
      "Epoch: 168/200, Iteration: 8398/10000 --- Training Loss:0.000184\n",
      "Epoch: 168/200, Iteration: 8400/10000 --- Training Loss:0.000209\n",
      "Epoch: 168 finished ! Train Loss: 0.00022, Test Loss: 0.00212\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 169/200, Iteration: 8402/10000 --- Training Loss:0.000119\n",
      "Epoch: 169/200, Iteration: 8404/10000 --- Training Loss:0.000113\n",
      "Epoch: 169/200, Iteration: 8406/10000 --- Training Loss:0.000183\n",
      "Epoch: 169/200, Iteration: 8408/10000 --- Training Loss:0.000139\n",
      "Epoch: 169/200, Iteration: 8410/10000 --- Training Loss:0.000294\n",
      "Epoch: 169/200, Iteration: 8412/10000 --- Training Loss:0.000103\n",
      "Epoch: 169/200, Iteration: 8414/10000 --- Training Loss:0.000101\n",
      "Epoch: 169/200, Iteration: 8416/10000 --- Training Loss:0.000148\n",
      "Epoch: 169/200, Iteration: 8418/10000 --- Training Loss:0.000210\n",
      "Epoch: 169/200, Iteration: 8420/10000 --- Training Loss:0.000297\n",
      "Epoch: 169/200, Iteration: 8422/10000 --- Training Loss:0.000222\n",
      "Epoch: 169/200, Iteration: 8424/10000 --- Training Loss:0.000125\n",
      "Epoch: 169/200, Iteration: 8426/10000 --- Training Loss:0.000161\n",
      "Epoch: 169/200, Iteration: 8428/10000 --- Training Loss:0.000157\n",
      "Epoch: 169/200, Iteration: 8430/10000 --- Training Loss:0.000238\n",
      "Epoch: 169/200, Iteration: 8432/10000 --- Training Loss:0.000477\n",
      "Epoch: 169/200, Iteration: 8434/10000 --- Training Loss:0.000474\n",
      "Epoch: 169/200, Iteration: 8436/10000 --- Training Loss:0.000108\n",
      "Epoch: 169/200, Iteration: 8438/10000 --- Training Loss:0.000140\n",
      "Epoch: 169/200, Iteration: 8440/10000 --- Training Loss:0.000142\n",
      "Epoch: 169/200, Iteration: 8442/10000 --- Training Loss:0.000118\n",
      "Epoch: 169/200, Iteration: 8444/10000 --- Training Loss:0.000161\n",
      "Epoch: 169/200, Iteration: 8446/10000 --- Training Loss:0.000294\n",
      "Epoch: 169/200, Iteration: 8448/10000 --- Training Loss:0.000077\n",
      "Epoch: 169/200, Iteration: 8450/10000 --- Training Loss:0.000147\n",
      "Epoch: 169 finished ! Train Loss: 0.00019, Test Loss: 0.00050\n",
      "Epoch consuming time: 0m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170/200, Iteration: 8452/10000 --- Training Loss:0.000178\n",
      "Epoch: 170/200, Iteration: 8454/10000 --- Training Loss:0.000095\n",
      "Epoch: 170/200, Iteration: 8456/10000 --- Training Loss:0.000893\n",
      "Epoch: 170/200, Iteration: 8458/10000 --- Training Loss:0.000267\n",
      "Epoch: 170/200, Iteration: 8460/10000 --- Training Loss:0.000272\n",
      "Epoch: 170/200, Iteration: 8462/10000 --- Training Loss:0.000112\n",
      "Epoch: 170/200, Iteration: 8464/10000 --- Training Loss:0.000256\n",
      "Epoch: 170/200, Iteration: 8466/10000 --- Training Loss:0.000244\n",
      "Epoch: 170/200, Iteration: 8468/10000 --- Training Loss:0.000132\n",
      "Epoch: 170/200, Iteration: 8470/10000 --- Training Loss:0.000221\n",
      "Epoch: 170/200, Iteration: 8472/10000 --- Training Loss:0.000140\n",
      "Epoch: 170/200, Iteration: 8474/10000 --- Training Loss:0.000093\n",
      "Epoch: 170/200, Iteration: 8476/10000 --- Training Loss:0.000184\n",
      "Epoch: 170/200, Iteration: 8478/10000 --- Training Loss:0.000076\n",
      "Epoch: 170/200, Iteration: 8480/10000 --- Training Loss:0.000164\n",
      "Epoch: 170/200, Iteration: 8482/10000 --- Training Loss:0.000128\n",
      "Epoch: 170/200, Iteration: 8484/10000 --- Training Loss:0.000098\n",
      "Epoch: 170/200, Iteration: 8486/10000 --- Training Loss:0.000134\n",
      "Epoch: 170/200, Iteration: 8488/10000 --- Training Loss:0.000129\n",
      "Epoch: 170/200, Iteration: 8490/10000 --- Training Loss:0.000094\n",
      "Epoch: 170/200, Iteration: 8492/10000 --- Training Loss:0.000093\n",
      "Epoch: 170/200, Iteration: 8494/10000 --- Training Loss:0.000159\n",
      "Epoch: 170/200, Iteration: 8496/10000 --- Training Loss:0.000113\n",
      "Epoch: 170/200, Iteration: 8498/10000 --- Training Loss:0.000208\n",
      "Epoch: 170/200, Iteration: 8500/10000 --- Training Loss:0.000313\n",
      "Epoch: 170 finished ! Train Loss: 0.00018, Test Loss: 0.00195\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 171/200, Iteration: 8502/10000 --- Training Loss:0.000124\n",
      "Epoch: 171/200, Iteration: 8504/10000 --- Training Loss:0.000163\n",
      "Epoch: 171/200, Iteration: 8506/10000 --- Training Loss:0.000176\n",
      "Epoch: 171/200, Iteration: 8508/10000 --- Training Loss:0.000136\n",
      "Epoch: 171/200, Iteration: 8510/10000 --- Training Loss:0.000385\n",
      "Epoch: 171/200, Iteration: 8512/10000 --- Training Loss:0.000135\n",
      "Epoch: 171/200, Iteration: 8514/10000 --- Training Loss:0.000223\n",
      "Epoch: 171/200, Iteration: 8516/10000 --- Training Loss:0.000482\n",
      "Epoch: 171/200, Iteration: 8518/10000 --- Training Loss:0.000157\n",
      "Epoch: 171/200, Iteration: 8520/10000 --- Training Loss:0.000335\n",
      "Epoch: 171/200, Iteration: 8522/10000 --- Training Loss:0.000169\n",
      "Epoch: 171/200, Iteration: 8524/10000 --- Training Loss:0.000112\n",
      "Epoch: 171/200, Iteration: 8526/10000 --- Training Loss:0.000257\n",
      "Epoch: 171/200, Iteration: 8528/10000 --- Training Loss:0.000257\n",
      "Epoch: 171/200, Iteration: 8530/10000 --- Training Loss:0.000101\n",
      "Epoch: 171/200, Iteration: 8532/10000 --- Training Loss:0.000137\n",
      "Epoch: 171/200, Iteration: 8534/10000 --- Training Loss:0.000249\n",
      "Epoch: 171/200, Iteration: 8536/10000 --- Training Loss:0.000112\n",
      "Epoch: 171/200, Iteration: 8538/10000 --- Training Loss:0.000169\n",
      "Epoch: 171/200, Iteration: 8540/10000 --- Training Loss:0.000085\n",
      "Epoch: 171/200, Iteration: 8542/10000 --- Training Loss:0.000087\n",
      "Epoch: 171/200, Iteration: 8544/10000 --- Training Loss:0.000136\n",
      "Epoch: 171/200, Iteration: 8546/10000 --- Training Loss:0.000112\n",
      "Epoch: 171/200, Iteration: 8548/10000 --- Training Loss:0.000437\n",
      "Epoch: 171/200, Iteration: 8550/10000 --- Training Loss:0.000053\n",
      "Epoch: 171 finished ! Train Loss: 0.00019, Test Loss: 0.00085\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 172/200, Iteration: 8552/10000 --- Training Loss:0.000114\n",
      "Epoch: 172/200, Iteration: 8554/10000 --- Training Loss:0.000165\n",
      "Epoch: 172/200, Iteration: 8556/10000 --- Training Loss:0.000322\n",
      "Epoch: 172/200, Iteration: 8558/10000 --- Training Loss:0.000180\n",
      "Epoch: 172/200, Iteration: 8560/10000 --- Training Loss:0.000175\n",
      "Epoch: 172/200, Iteration: 8562/10000 --- Training Loss:0.000141\n",
      "Epoch: 172/200, Iteration: 8564/10000 --- Training Loss:0.000117\n",
      "Epoch: 172/200, Iteration: 8566/10000 --- Training Loss:0.000147\n",
      "Epoch: 172/200, Iteration: 8568/10000 --- Training Loss:0.000156\n",
      "Epoch: 172/200, Iteration: 8570/10000 --- Training Loss:0.000103\n",
      "Epoch: 172/200, Iteration: 8572/10000 --- Training Loss:0.000532\n",
      "Epoch: 172/200, Iteration: 8574/10000 --- Training Loss:0.000137\n",
      "Epoch: 172/200, Iteration: 8576/10000 --- Training Loss:0.000191\n",
      "Epoch: 172/200, Iteration: 8578/10000 --- Training Loss:0.000210\n",
      "Epoch: 172/200, Iteration: 8580/10000 --- Training Loss:0.000122\n",
      "Epoch: 172/200, Iteration: 8582/10000 --- Training Loss:0.000348\n",
      "Epoch: 172/200, Iteration: 8584/10000 --- Training Loss:0.000171\n",
      "Epoch: 172/200, Iteration: 8586/10000 --- Training Loss:0.000167\n",
      "Epoch: 172/200, Iteration: 8588/10000 --- Training Loss:0.000118\n",
      "Epoch: 172/200, Iteration: 8590/10000 --- Training Loss:0.000146\n",
      "Epoch: 172/200, Iteration: 8592/10000 --- Training Loss:0.000338\n",
      "Epoch: 172/200, Iteration: 8594/10000 --- Training Loss:0.000113\n",
      "Epoch: 172/200, Iteration: 8596/10000 --- Training Loss:0.000093\n",
      "Epoch: 172/200, Iteration: 8598/10000 --- Training Loss:0.000082\n",
      "Epoch: 172/200, Iteration: 8600/10000 --- Training Loss:0.000150\n",
      "Epoch: 172 finished ! Train Loss: 0.00019, Test Loss: 0.00115\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 173/200, Iteration: 8602/10000 --- Training Loss:0.000121\n",
      "Epoch: 173/200, Iteration: 8604/10000 --- Training Loss:0.000085\n",
      "Epoch: 173/200, Iteration: 8606/10000 --- Training Loss:0.000098\n",
      "Epoch: 173/200, Iteration: 8608/10000 --- Training Loss:0.000058\n",
      "Epoch: 173/200, Iteration: 8610/10000 --- Training Loss:0.000576\n",
      "Epoch: 173/200, Iteration: 8612/10000 --- Training Loss:0.000154\n",
      "Epoch: 173/200, Iteration: 8614/10000 --- Training Loss:0.000107\n",
      "Epoch: 173/200, Iteration: 8616/10000 --- Training Loss:0.000147\n",
      "Epoch: 173/200, Iteration: 8618/10000 --- Training Loss:0.000250\n",
      "Epoch: 173/200, Iteration: 8620/10000 --- Training Loss:0.000237\n",
      "Epoch: 173/200, Iteration: 8622/10000 --- Training Loss:0.000151\n",
      "Epoch: 173/200, Iteration: 8624/10000 --- Training Loss:0.000220\n",
      "Epoch: 173/200, Iteration: 8626/10000 --- Training Loss:0.000127\n",
      "Epoch: 173/200, Iteration: 8628/10000 --- Training Loss:0.000173\n",
      "Epoch: 173/200, Iteration: 8630/10000 --- Training Loss:0.000137\n",
      "Epoch: 173/200, Iteration: 8632/10000 --- Training Loss:0.000110\n",
      "Epoch: 173/200, Iteration: 8634/10000 --- Training Loss:0.000095\n",
      "Epoch: 173/200, Iteration: 8636/10000 --- Training Loss:0.000164\n",
      "Epoch: 173/200, Iteration: 8638/10000 --- Training Loss:0.000088\n",
      "Epoch: 173/200, Iteration: 8640/10000 --- Training Loss:0.000171\n",
      "Epoch: 173/200, Iteration: 8642/10000 --- Training Loss:0.000257\n",
      "Epoch: 173/200, Iteration: 8644/10000 --- Training Loss:0.000062\n",
      "Epoch: 173/200, Iteration: 8646/10000 --- Training Loss:0.000211\n",
      "Epoch: 173/200, Iteration: 8648/10000 --- Training Loss:0.000095\n",
      "Epoch: 173/200, Iteration: 8650/10000 --- Training Loss:0.000096\n",
      "Epoch: 173 finished ! Train Loss: 0.00015, Test Loss: 0.00044\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 174/200, Iteration: 8652/10000 --- Training Loss:0.000094\n",
      "Epoch: 174/200, Iteration: 8654/10000 --- Training Loss:0.000098\n",
      "Epoch: 174/200, Iteration: 8656/10000 --- Training Loss:0.000140\n",
      "Epoch: 174/200, Iteration: 8658/10000 --- Training Loss:0.000114\n",
      "Epoch: 174/200, Iteration: 8660/10000 --- Training Loss:0.000141\n",
      "Epoch: 174/200, Iteration: 8662/10000 --- Training Loss:0.000122\n",
      "Epoch: 174/200, Iteration: 8664/10000 --- Training Loss:0.000096\n",
      "Epoch: 174/200, Iteration: 8666/10000 --- Training Loss:0.000104\n",
      "Epoch: 174/200, Iteration: 8668/10000 --- Training Loss:0.000181\n",
      "Epoch: 174/200, Iteration: 8670/10000 --- Training Loss:0.000193\n",
      "Epoch: 174/200, Iteration: 8672/10000 --- Training Loss:0.000254\n",
      "Epoch: 174/200, Iteration: 8674/10000 --- Training Loss:0.000225\n",
      "Epoch: 174/200, Iteration: 8676/10000 --- Training Loss:0.000104\n",
      "Epoch: 174/200, Iteration: 8678/10000 --- Training Loss:0.000093\n",
      "Epoch: 174/200, Iteration: 8680/10000 --- Training Loss:0.000145\n",
      "Epoch: 174/200, Iteration: 8682/10000 --- Training Loss:0.000067\n",
      "Epoch: 174/200, Iteration: 8684/10000 --- Training Loss:0.000088\n",
      "Epoch: 174/200, Iteration: 8686/10000 --- Training Loss:0.000112\n",
      "Epoch: 174/200, Iteration: 8688/10000 --- Training Loss:0.000105\n",
      "Epoch: 174/200, Iteration: 8690/10000 --- Training Loss:0.000091\n",
      "Epoch: 174/200, Iteration: 8692/10000 --- Training Loss:0.000140\n",
      "Epoch: 174/200, Iteration: 8694/10000 --- Training Loss:0.000073\n",
      "Epoch: 174/200, Iteration: 8696/10000 --- Training Loss:0.000103\n",
      "Epoch: 174/200, Iteration: 8698/10000 --- Training Loss:0.000195\n",
      "Epoch: 174/200, Iteration: 8700/10000 --- Training Loss:0.000085\n",
      "Epoch: 174 finished ! Train Loss: 0.00013, Test Loss: 0.00234\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 175/200, Iteration: 8702/10000 --- Training Loss:0.000071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 175/200, Iteration: 8704/10000 --- Training Loss:0.000059\n",
      "Epoch: 175/200, Iteration: 8706/10000 --- Training Loss:0.000087\n",
      "Epoch: 175/200, Iteration: 8708/10000 --- Training Loss:0.000100\n",
      "Epoch: 175/200, Iteration: 8710/10000 --- Training Loss:0.000175\n",
      "Epoch: 175/200, Iteration: 8712/10000 --- Training Loss:0.000080\n",
      "Epoch: 175/200, Iteration: 8714/10000 --- Training Loss:0.000089\n",
      "Epoch: 175/200, Iteration: 8716/10000 --- Training Loss:0.000096\n",
      "Epoch: 175/200, Iteration: 8718/10000 --- Training Loss:0.000139\n",
      "Epoch: 175/200, Iteration: 8720/10000 --- Training Loss:0.000146\n",
      "Epoch: 175/200, Iteration: 8722/10000 --- Training Loss:0.000091\n",
      "Epoch: 175/200, Iteration: 8724/10000 --- Training Loss:0.000095\n",
      "Epoch: 175/200, Iteration: 8726/10000 --- Training Loss:0.000134\n",
      "Epoch: 175/200, Iteration: 8728/10000 --- Training Loss:0.000085\n",
      "Epoch: 175/200, Iteration: 8730/10000 --- Training Loss:0.000085\n",
      "Epoch: 175/200, Iteration: 8732/10000 --- Training Loss:0.000101\n",
      "Epoch: 175/200, Iteration: 8734/10000 --- Training Loss:0.000340\n",
      "Epoch: 175/200, Iteration: 8736/10000 --- Training Loss:0.000144\n",
      "Epoch: 175/200, Iteration: 8738/10000 --- Training Loss:0.000068\n",
      "Epoch: 175/200, Iteration: 8740/10000 --- Training Loss:0.000210\n",
      "Epoch: 175/200, Iteration: 8742/10000 --- Training Loss:0.000135\n",
      "Epoch: 175/200, Iteration: 8744/10000 --- Training Loss:0.000117\n",
      "Epoch: 175/200, Iteration: 8746/10000 --- Training Loss:0.000169\n",
      "Epoch: 175/200, Iteration: 8748/10000 --- Training Loss:0.000153\n",
      "Epoch: 175/200, Iteration: 8750/10000 --- Training Loss:0.000262\n",
      "Epoch: 175 finished ! Train Loss: 0.00014, Test Loss: 0.00355\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 176/200, Iteration: 8752/10000 --- Training Loss:0.000090\n",
      "Epoch: 176/200, Iteration: 8754/10000 --- Training Loss:0.000146\n",
      "Epoch: 176/200, Iteration: 8756/10000 --- Training Loss:0.000100\n",
      "Epoch: 176/200, Iteration: 8758/10000 --- Training Loss:0.000077\n",
      "Epoch: 176/200, Iteration: 8760/10000 --- Training Loss:0.000113\n",
      "Epoch: 176/200, Iteration: 8762/10000 --- Training Loss:0.000138\n",
      "Epoch: 176/200, Iteration: 8764/10000 --- Training Loss:0.000286\n",
      "Epoch: 176/200, Iteration: 8766/10000 --- Training Loss:0.000079\n",
      "Epoch: 176/200, Iteration: 8768/10000 --- Training Loss:0.000178\n",
      "Epoch: 176/200, Iteration: 8770/10000 --- Training Loss:0.000280\n",
      "Epoch: 176/200, Iteration: 8772/10000 --- Training Loss:0.000121\n",
      "Epoch: 176/200, Iteration: 8774/10000 --- Training Loss:0.000278\n",
      "Epoch: 176/200, Iteration: 8776/10000 --- Training Loss:0.000118\n",
      "Epoch: 176/200, Iteration: 8778/10000 --- Training Loss:0.000119\n",
      "Epoch: 176/200, Iteration: 8780/10000 --- Training Loss:0.000078\n",
      "Epoch: 176/200, Iteration: 8782/10000 --- Training Loss:0.000208\n",
      "Epoch: 176/200, Iteration: 8784/10000 --- Training Loss:0.000165\n",
      "Epoch: 176/200, Iteration: 8786/10000 --- Training Loss:0.000159\n",
      "Epoch: 176/200, Iteration: 8788/10000 --- Training Loss:0.000121\n",
      "Epoch: 176/200, Iteration: 8790/10000 --- Training Loss:0.000098\n",
      "Epoch: 176/200, Iteration: 8792/10000 --- Training Loss:0.000220\n",
      "Epoch: 176/200, Iteration: 8794/10000 --- Training Loss:0.000116\n",
      "Epoch: 176/200, Iteration: 8796/10000 --- Training Loss:0.000119\n",
      "Epoch: 176/200, Iteration: 8798/10000 --- Training Loss:0.000117\n",
      "Epoch: 176/200, Iteration: 8800/10000 --- Training Loss:0.000095\n",
      "Epoch: 176 finished ! Train Loss: 0.00015, Test Loss: 0.00112\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 177/200, Iteration: 8802/10000 --- Training Loss:0.000077\n",
      "Epoch: 177/200, Iteration: 8804/10000 --- Training Loss:0.000147\n",
      "Epoch: 177/200, Iteration: 8806/10000 --- Training Loss:0.000056\n",
      "Epoch: 177/200, Iteration: 8808/10000 --- Training Loss:0.000081\n",
      "Epoch: 177/200, Iteration: 8810/10000 --- Training Loss:0.000141\n",
      "Epoch: 177/200, Iteration: 8812/10000 --- Training Loss:0.000104\n",
      "Epoch: 177/200, Iteration: 8814/10000 --- Training Loss:0.000082\n",
      "Epoch: 177/200, Iteration: 8816/10000 --- Training Loss:0.000216\n",
      "Epoch: 177/200, Iteration: 8818/10000 --- Training Loss:0.000107\n",
      "Epoch: 177/200, Iteration: 8820/10000 --- Training Loss:0.000107\n",
      "Epoch: 177/200, Iteration: 8822/10000 --- Training Loss:0.000071\n",
      "Epoch: 177/200, Iteration: 8824/10000 --- Training Loss:0.000116\n",
      "Epoch: 177/200, Iteration: 8826/10000 --- Training Loss:0.000051\n",
      "Epoch: 177/200, Iteration: 8828/10000 --- Training Loss:0.000086\n",
      "Epoch: 177/200, Iteration: 8830/10000 --- Training Loss:0.000106\n",
      "Epoch: 177/200, Iteration: 8832/10000 --- Training Loss:0.000133\n",
      "Epoch: 177/200, Iteration: 8834/10000 --- Training Loss:0.000143\n",
      "Epoch: 177/200, Iteration: 8836/10000 --- Training Loss:0.000084\n",
      "Epoch: 177/200, Iteration: 8838/10000 --- Training Loss:0.000099\n",
      "Epoch: 177/200, Iteration: 8840/10000 --- Training Loss:0.000135\n",
      "Epoch: 177/200, Iteration: 8842/10000 --- Training Loss:0.000053\n",
      "Epoch: 177/200, Iteration: 8844/10000 --- Training Loss:0.000114\n",
      "Epoch: 177/200, Iteration: 8846/10000 --- Training Loss:0.000102\n",
      "Epoch: 177/200, Iteration: 8848/10000 --- Training Loss:0.000121\n",
      "Epoch: 177/200, Iteration: 8850/10000 --- Training Loss:0.000138\n",
      "Epoch: 177 finished ! Train Loss: 0.00011, Test Loss: 0.00024\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 178/200, Iteration: 8852/10000 --- Training Loss:0.000119\n",
      "Epoch: 178/200, Iteration: 8854/10000 --- Training Loss:0.000111\n",
      "Epoch: 178/200, Iteration: 8856/10000 --- Training Loss:0.000161\n",
      "Epoch: 178/200, Iteration: 8858/10000 --- Training Loss:0.000076\n",
      "Epoch: 178/200, Iteration: 8860/10000 --- Training Loss:0.000162\n",
      "Epoch: 178/200, Iteration: 8862/10000 --- Training Loss:0.000235\n",
      "Epoch: 178/200, Iteration: 8864/10000 --- Training Loss:0.000058\n",
      "Epoch: 178/200, Iteration: 8866/10000 --- Training Loss:0.000081\n",
      "Epoch: 178/200, Iteration: 8868/10000 --- Training Loss:0.000097\n",
      "Epoch: 178/200, Iteration: 8870/10000 --- Training Loss:0.000077\n",
      "Epoch: 178/200, Iteration: 8872/10000 --- Training Loss:0.000271\n",
      "Epoch: 178/200, Iteration: 8874/10000 --- Training Loss:0.000378\n",
      "Epoch: 178/200, Iteration: 8876/10000 --- Training Loss:0.000183\n",
      "Epoch: 178/200, Iteration: 8878/10000 --- Training Loss:0.000173\n",
      "Epoch: 178/200, Iteration: 8880/10000 --- Training Loss:0.000084\n",
      "Epoch: 178/200, Iteration: 8882/10000 --- Training Loss:0.000136\n",
      "Epoch: 178/200, Iteration: 8884/10000 --- Training Loss:0.000087\n",
      "Epoch: 178/200, Iteration: 8886/10000 --- Training Loss:0.000084\n",
      "Epoch: 178/200, Iteration: 8888/10000 --- Training Loss:0.000076\n",
      "Epoch: 178/200, Iteration: 8890/10000 --- Training Loss:0.000060\n",
      "Epoch: 178/200, Iteration: 8892/10000 --- Training Loss:0.000156\n",
      "Epoch: 178/200, Iteration: 8894/10000 --- Training Loss:0.000137\n",
      "Epoch: 178/200, Iteration: 8896/10000 --- Training Loss:0.000145\n",
      "Epoch: 178/200, Iteration: 8898/10000 --- Training Loss:0.000153\n",
      "Epoch: 178/200, Iteration: 8900/10000 --- Training Loss:0.000086\n",
      "Epoch: 178 finished ! Train Loss: 0.00013, Test Loss: 0.00246\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 179/200, Iteration: 8902/10000 --- Training Loss:0.000136\n",
      "Epoch: 179/200, Iteration: 8904/10000 --- Training Loss:0.000076\n",
      "Epoch: 179/200, Iteration: 8906/10000 --- Training Loss:0.000264\n",
      "Epoch: 179/200, Iteration: 8908/10000 --- Training Loss:0.000132\n",
      "Epoch: 179/200, Iteration: 8910/10000 --- Training Loss:0.000202\n",
      "Epoch: 179/200, Iteration: 8912/10000 --- Training Loss:0.000077\n",
      "Epoch: 179/200, Iteration: 8914/10000 --- Training Loss:0.000188\n",
      "Epoch: 179/200, Iteration: 8916/10000 --- Training Loss:0.000168\n",
      "Epoch: 179/200, Iteration: 8918/10000 --- Training Loss:0.000087\n",
      "Epoch: 179/200, Iteration: 8920/10000 --- Training Loss:0.000381\n",
      "Epoch: 179/200, Iteration: 8922/10000 --- Training Loss:0.000158\n",
      "Epoch: 179/200, Iteration: 8924/10000 --- Training Loss:0.000076\n",
      "Epoch: 179/200, Iteration: 8926/10000 --- Training Loss:0.000138\n",
      "Epoch: 179/200, Iteration: 8928/10000 --- Training Loss:0.000145\n",
      "Epoch: 179/200, Iteration: 8930/10000 --- Training Loss:0.000132\n",
      "Epoch: 179/200, Iteration: 8932/10000 --- Training Loss:0.000117\n",
      "Epoch: 179/200, Iteration: 8934/10000 --- Training Loss:0.000095\n",
      "Epoch: 179/200, Iteration: 8936/10000 --- Training Loss:0.000100\n",
      "Epoch: 179/200, Iteration: 8938/10000 --- Training Loss:0.000120\n",
      "Epoch: 179/200, Iteration: 8940/10000 --- Training Loss:0.000116\n",
      "Epoch: 179/200, Iteration: 8942/10000 --- Training Loss:0.000103\n",
      "Epoch: 179/200, Iteration: 8944/10000 --- Training Loss:0.000128\n",
      "Epoch: 179/200, Iteration: 8946/10000 --- Training Loss:0.000084\n",
      "Epoch: 179/200, Iteration: 8948/10000 --- Training Loss:0.000129\n",
      "Epoch: 179/200, Iteration: 8950/10000 --- Training Loss:0.000102\n",
      "Epoch: 179 finished ! Train Loss: 0.00013, Test Loss: 0.00355\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 180/200, Iteration: 8952/10000 --- Training Loss:0.000109\n",
      "Epoch: 180/200, Iteration: 8954/10000 --- Training Loss:0.000072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 180/200, Iteration: 8956/10000 --- Training Loss:0.000065\n",
      "Epoch: 180/200, Iteration: 8958/10000 --- Training Loss:0.000055\n",
      "Epoch: 180/200, Iteration: 8960/10000 --- Training Loss:0.000107\n",
      "Epoch: 180/200, Iteration: 8962/10000 --- Training Loss:0.000064\n",
      "Epoch: 180/200, Iteration: 8964/10000 --- Training Loss:0.000076\n",
      "Epoch: 180/200, Iteration: 8966/10000 --- Training Loss:0.000520\n",
      "Epoch: 180/200, Iteration: 8968/10000 --- Training Loss:0.000128\n",
      "Epoch: 180/200, Iteration: 8970/10000 --- Training Loss:0.000153\n",
      "Epoch: 180/200, Iteration: 8972/10000 --- Training Loss:0.000075\n",
      "Epoch: 180/200, Iteration: 8974/10000 --- Training Loss:0.000114\n",
      "Epoch: 180/200, Iteration: 8976/10000 --- Training Loss:0.000098\n",
      "Epoch: 180/200, Iteration: 8978/10000 --- Training Loss:0.000126\n",
      "Epoch: 180/200, Iteration: 8980/10000 --- Training Loss:0.000092\n",
      "Epoch: 180/200, Iteration: 8982/10000 --- Training Loss:0.000101\n",
      "Epoch: 180/200, Iteration: 8984/10000 --- Training Loss:0.000170\n",
      "Epoch: 180/200, Iteration: 8986/10000 --- Training Loss:0.000127\n",
      "Epoch: 180/200, Iteration: 8988/10000 --- Training Loss:0.000172\n",
      "Epoch: 180/200, Iteration: 8990/10000 --- Training Loss:0.000082\n",
      "Epoch: 180/200, Iteration: 8992/10000 --- Training Loss:0.000099\n",
      "Epoch: 180/200, Iteration: 8994/10000 --- Training Loss:0.000136\n",
      "Epoch: 180/200, Iteration: 8996/10000 --- Training Loss:0.000067\n",
      "Epoch: 180/200, Iteration: 8998/10000 --- Training Loss:0.000109\n",
      "Epoch: 180/200, Iteration: 9000/10000 --- Training Loss:0.000116\n",
      "Epoch: 180 finished ! Train Loss: 0.00012, Test Loss: 0.00226\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 181/200, Iteration: 9002/10000 --- Training Loss:0.000092\n",
      "Epoch: 181/200, Iteration: 9004/10000 --- Training Loss:0.000144\n",
      "Epoch: 181/200, Iteration: 9006/10000 --- Training Loss:0.000080\n",
      "Epoch: 181/200, Iteration: 9008/10000 --- Training Loss:0.000050\n",
      "Epoch: 181/200, Iteration: 9010/10000 --- Training Loss:0.000057\n",
      "Epoch: 181/200, Iteration: 9012/10000 --- Training Loss:0.000107\n",
      "Epoch: 181/200, Iteration: 9014/10000 --- Training Loss:0.000097\n",
      "Epoch: 181/200, Iteration: 9016/10000 --- Training Loss:0.000072\n",
      "Epoch: 181/200, Iteration: 9018/10000 --- Training Loss:0.000055\n",
      "Epoch: 181/200, Iteration: 9020/10000 --- Training Loss:0.000069\n",
      "Epoch: 181/200, Iteration: 9022/10000 --- Training Loss:0.000132\n",
      "Epoch: 181/200, Iteration: 9024/10000 --- Training Loss:0.000093\n",
      "Epoch: 181/200, Iteration: 9026/10000 --- Training Loss:0.000086\n",
      "Epoch: 181/200, Iteration: 9028/10000 --- Training Loss:0.000107\n",
      "Epoch: 181/200, Iteration: 9030/10000 --- Training Loss:0.000061\n",
      "Epoch: 181/200, Iteration: 9032/10000 --- Training Loss:0.000146\n",
      "Epoch: 181/200, Iteration: 9034/10000 --- Training Loss:0.000144\n",
      "Epoch: 181/200, Iteration: 9036/10000 --- Training Loss:0.000088\n",
      "Epoch: 181/200, Iteration: 9038/10000 --- Training Loss:0.000069\n",
      "Epoch: 181/200, Iteration: 9040/10000 --- Training Loss:0.000106\n",
      "Epoch: 181/200, Iteration: 9042/10000 --- Training Loss:0.000060\n",
      "Epoch: 181/200, Iteration: 9044/10000 --- Training Loss:0.000140\n",
      "Epoch: 181/200, Iteration: 9046/10000 --- Training Loss:0.000044\n",
      "Epoch: 181/200, Iteration: 9048/10000 --- Training Loss:0.000059\n",
      "Epoch: 181/200, Iteration: 9050/10000 --- Training Loss:0.000077\n",
      "Epoch: 181 finished ! Train Loss: 0.00010, Test Loss: 0.00092\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 182/200, Iteration: 9052/10000 --- Training Loss:0.000062\n",
      "Epoch: 182/200, Iteration: 9054/10000 --- Training Loss:0.000062\n",
      "Epoch: 182/200, Iteration: 9056/10000 --- Training Loss:0.000066\n",
      "Epoch: 182/200, Iteration: 9058/10000 --- Training Loss:0.000094\n",
      "Epoch: 182/200, Iteration: 9060/10000 --- Training Loss:0.000052\n",
      "Epoch: 182/200, Iteration: 9062/10000 --- Training Loss:0.000044\n",
      "Epoch: 182/200, Iteration: 9064/10000 --- Training Loss:0.000052\n",
      "Epoch: 182/200, Iteration: 9066/10000 --- Training Loss:0.000060\n",
      "Epoch: 182/200, Iteration: 9068/10000 --- Training Loss:0.000093\n",
      "Epoch: 182/200, Iteration: 9070/10000 --- Training Loss:0.000049\n",
      "Epoch: 182/200, Iteration: 9072/10000 --- Training Loss:0.000088\n",
      "Epoch: 182/200, Iteration: 9074/10000 --- Training Loss:0.000074\n",
      "Epoch: 182/200, Iteration: 9076/10000 --- Training Loss:0.000137\n",
      "Epoch: 182/200, Iteration: 9078/10000 --- Training Loss:0.000140\n",
      "Epoch: 182/200, Iteration: 9080/10000 --- Training Loss:0.000078\n",
      "Epoch: 182/200, Iteration: 9082/10000 --- Training Loss:0.000136\n",
      "Epoch: 182/200, Iteration: 9084/10000 --- Training Loss:0.000124\n",
      "Epoch: 182/200, Iteration: 9086/10000 --- Training Loss:0.000109\n",
      "Epoch: 182/200, Iteration: 9088/10000 --- Training Loss:0.000090\n",
      "Epoch: 182/200, Iteration: 9090/10000 --- Training Loss:0.000106\n",
      "Epoch: 182/200, Iteration: 9092/10000 --- Training Loss:0.000114\n",
      "Epoch: 182/200, Iteration: 9094/10000 --- Training Loss:0.000131\n",
      "Epoch: 182/200, Iteration: 9096/10000 --- Training Loss:0.000163\n",
      "Epoch: 182/200, Iteration: 9098/10000 --- Training Loss:0.000083\n",
      "Epoch: 182/200, Iteration: 9100/10000 --- Training Loss:0.000117\n",
      "Epoch: 182 finished ! Train Loss: 0.00011, Test Loss: 0.00136\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 183/200, Iteration: 9102/10000 --- Training Loss:0.000060\n",
      "Epoch: 183/200, Iteration: 9104/10000 --- Training Loss:0.000083\n",
      "Epoch: 183/200, Iteration: 9106/10000 --- Training Loss:0.000086\n",
      "Epoch: 183/200, Iteration: 9108/10000 --- Training Loss:0.000100\n",
      "Epoch: 183/200, Iteration: 9110/10000 --- Training Loss:0.000060\n",
      "Epoch: 183/200, Iteration: 9112/10000 --- Training Loss:0.000048\n",
      "Epoch: 183/200, Iteration: 9114/10000 --- Training Loss:0.000083\n",
      "Epoch: 183/200, Iteration: 9116/10000 --- Training Loss:0.000076\n",
      "Epoch: 183/200, Iteration: 9118/10000 --- Training Loss:0.000103\n",
      "Epoch: 183/200, Iteration: 9120/10000 --- Training Loss:0.000264\n",
      "Epoch: 183/200, Iteration: 9122/10000 --- Training Loss:0.000293\n",
      "Epoch: 183/200, Iteration: 9124/10000 --- Training Loss:0.000146\n",
      "Epoch: 183/200, Iteration: 9126/10000 --- Training Loss:0.000119\n",
      "Epoch: 183/200, Iteration: 9128/10000 --- Training Loss:0.000124\n",
      "Epoch: 183/200, Iteration: 9130/10000 --- Training Loss:0.000095\n",
      "Epoch: 183/200, Iteration: 9132/10000 --- Training Loss:0.000093\n",
      "Epoch: 183/200, Iteration: 9134/10000 --- Training Loss:0.000087\n",
      "Epoch: 183/200, Iteration: 9136/10000 --- Training Loss:0.000070\n",
      "Epoch: 183/200, Iteration: 9138/10000 --- Training Loss:0.000304\n",
      "Epoch: 183/200, Iteration: 9140/10000 --- Training Loss:0.000078\n",
      "Epoch: 183/200, Iteration: 9142/10000 --- Training Loss:0.000080\n",
      "Epoch: 183/200, Iteration: 9144/10000 --- Training Loss:0.000125\n",
      "Epoch: 183/200, Iteration: 9146/10000 --- Training Loss:0.000178\n",
      "Epoch: 183/200, Iteration: 9148/10000 --- Training Loss:0.000117\n",
      "Epoch: 183/200, Iteration: 9150/10000 --- Training Loss:0.000101\n",
      "Epoch: 183 finished ! Train Loss: 0.00013, Test Loss: 0.00151\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 184/200, Iteration: 9152/10000 --- Training Loss:0.000140\n",
      "Epoch: 184/200, Iteration: 9154/10000 --- Training Loss:0.000080\n",
      "Epoch: 184/200, Iteration: 9156/10000 --- Training Loss:0.000083\n",
      "Epoch: 184/200, Iteration: 9158/10000 --- Training Loss:0.000126\n",
      "Epoch: 184/200, Iteration: 9160/10000 --- Training Loss:0.000107\n",
      "Epoch: 184/200, Iteration: 9162/10000 --- Training Loss:0.000058\n",
      "Epoch: 184/200, Iteration: 9164/10000 --- Training Loss:0.000062\n",
      "Epoch: 184/200, Iteration: 9166/10000 --- Training Loss:0.000059\n",
      "Epoch: 184/200, Iteration: 9168/10000 --- Training Loss:0.000055\n",
      "Epoch: 184/200, Iteration: 9170/10000 --- Training Loss:0.000054\n",
      "Epoch: 184/200, Iteration: 9172/10000 --- Training Loss:0.000080\n",
      "Epoch: 184/200, Iteration: 9174/10000 --- Training Loss:0.000056\n",
      "Epoch: 184/200, Iteration: 9176/10000 --- Training Loss:0.000078\n",
      "Epoch: 184/200, Iteration: 9178/10000 --- Training Loss:0.000072\n",
      "Epoch: 184/200, Iteration: 9180/10000 --- Training Loss:0.000125\n",
      "Epoch: 184/200, Iteration: 9182/10000 --- Training Loss:0.000136\n",
      "Epoch: 184/200, Iteration: 9184/10000 --- Training Loss:0.000083\n",
      "Epoch: 184/200, Iteration: 9186/10000 --- Training Loss:0.000243\n",
      "Epoch: 184/200, Iteration: 9188/10000 --- Training Loss:0.000247\n",
      "Epoch: 184/200, Iteration: 9190/10000 --- Training Loss:0.000385\n",
      "Epoch: 184/200, Iteration: 9192/10000 --- Training Loss:0.000209\n",
      "Epoch: 184/200, Iteration: 9194/10000 --- Training Loss:0.000141\n",
      "Epoch: 184/200, Iteration: 9196/10000 --- Training Loss:0.000187\n",
      "Epoch: 184/200, Iteration: 9198/10000 --- Training Loss:0.000139\n",
      "Epoch: 184/200, Iteration: 9200/10000 --- Training Loss:0.000175\n",
      "Epoch: 184 finished ! Train Loss: 0.00016, Test Loss: 0.00240\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 185/200, Iteration: 9202/10000 --- Training Loss:0.000259\n",
      "Epoch: 185/200, Iteration: 9204/10000 --- Training Loss:0.000109\n",
      "Epoch: 185/200, Iteration: 9206/10000 --- Training Loss:0.000125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185/200, Iteration: 9208/10000 --- Training Loss:0.000185\n",
      "Epoch: 185/200, Iteration: 9210/10000 --- Training Loss:0.000152\n",
      "Epoch: 185/200, Iteration: 9212/10000 --- Training Loss:0.000124\n",
      "Epoch: 185/200, Iteration: 9214/10000 --- Training Loss:0.000096\n",
      "Epoch: 185/200, Iteration: 9216/10000 --- Training Loss:0.000109\n",
      "Epoch: 185/200, Iteration: 9218/10000 --- Training Loss:0.000177\n",
      "Epoch: 185/200, Iteration: 9220/10000 --- Training Loss:0.000285\n",
      "Epoch: 185/200, Iteration: 9222/10000 --- Training Loss:0.000381\n",
      "Epoch: 185/200, Iteration: 9224/10000 --- Training Loss:0.000128\n",
      "Epoch: 185/200, Iteration: 9226/10000 --- Training Loss:0.000257\n",
      "Epoch: 185/200, Iteration: 9228/10000 --- Training Loss:0.000324\n",
      "Epoch: 185/200, Iteration: 9230/10000 --- Training Loss:0.000183\n",
      "Epoch: 185/200, Iteration: 9232/10000 --- Training Loss:0.000241\n",
      "Epoch: 185/200, Iteration: 9234/10000 --- Training Loss:0.000150\n",
      "Epoch: 185/200, Iteration: 9236/10000 --- Training Loss:0.000193\n",
      "Epoch: 185/200, Iteration: 9238/10000 --- Training Loss:0.000118\n",
      "Epoch: 185/200, Iteration: 9240/10000 --- Training Loss:0.000211\n",
      "Epoch: 185/200, Iteration: 9242/10000 --- Training Loss:0.000129\n",
      "Epoch: 185/200, Iteration: 9244/10000 --- Training Loss:0.000453\n",
      "Epoch: 185/200, Iteration: 9246/10000 --- Training Loss:0.000163\n",
      "Epoch: 185/200, Iteration: 9248/10000 --- Training Loss:0.000136\n",
      "Epoch: 185/200, Iteration: 9250/10000 --- Training Loss:0.000164\n",
      "Epoch: 185 finished ! Train Loss: 0.00021, Test Loss: 0.00079\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 186/200, Iteration: 9252/10000 --- Training Loss:0.000154\n",
      "Epoch: 186/200, Iteration: 9254/10000 --- Training Loss:0.000154\n",
      "Epoch: 186/200, Iteration: 9256/10000 --- Training Loss:0.000108\n",
      "Epoch: 186/200, Iteration: 9258/10000 --- Training Loss:0.000092\n",
      "Epoch: 186/200, Iteration: 9260/10000 --- Training Loss:0.000115\n",
      "Epoch: 186/200, Iteration: 9262/10000 --- Training Loss:0.000074\n",
      "Epoch: 186/200, Iteration: 9264/10000 --- Training Loss:0.000133\n",
      "Epoch: 186/200, Iteration: 9266/10000 --- Training Loss:0.000102\n",
      "Epoch: 186/200, Iteration: 9268/10000 --- Training Loss:0.000094\n",
      "Epoch: 186/200, Iteration: 9270/10000 --- Training Loss:0.000085\n",
      "Epoch: 186/200, Iteration: 9272/10000 --- Training Loss:0.000112\n",
      "Epoch: 186/200, Iteration: 9274/10000 --- Training Loss:0.000134\n",
      "Epoch: 186/200, Iteration: 9276/10000 --- Training Loss:0.000129\n",
      "Epoch: 186/200, Iteration: 9278/10000 --- Training Loss:0.000110\n",
      "Epoch: 186/200, Iteration: 9280/10000 --- Training Loss:0.000096\n",
      "Epoch: 186/200, Iteration: 9282/10000 --- Training Loss:0.000096\n",
      "Epoch: 186/200, Iteration: 9284/10000 --- Training Loss:0.000099\n",
      "Epoch: 186/200, Iteration: 9286/10000 --- Training Loss:0.000074\n",
      "Epoch: 186/200, Iteration: 9288/10000 --- Training Loss:0.000137\n",
      "Epoch: 186/200, Iteration: 9290/10000 --- Training Loss:0.000148\n",
      "Epoch: 186/200, Iteration: 9292/10000 --- Training Loss:0.000123\n",
      "Epoch: 186/200, Iteration: 9294/10000 --- Training Loss:0.000097\n",
      "Epoch: 186/200, Iteration: 9296/10000 --- Training Loss:0.000083\n",
      "Epoch: 186/200, Iteration: 9298/10000 --- Training Loss:0.000136\n",
      "Epoch: 186/200, Iteration: 9300/10000 --- Training Loss:0.000047\n",
      "Epoch: 186 finished ! Train Loss: 0.00012, Test Loss: 0.00136\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 187/200, Iteration: 9302/10000 --- Training Loss:0.000067\n",
      "Epoch: 187/200, Iteration: 9304/10000 --- Training Loss:0.000076\n",
      "Epoch: 187/200, Iteration: 9306/10000 --- Training Loss:0.000096\n",
      "Epoch: 187/200, Iteration: 9308/10000 --- Training Loss:0.000195\n",
      "Epoch: 187/200, Iteration: 9310/10000 --- Training Loss:0.000075\n",
      "Epoch: 187/200, Iteration: 9312/10000 --- Training Loss:0.000054\n",
      "Epoch: 187/200, Iteration: 9314/10000 --- Training Loss:0.000213\n",
      "Epoch: 187/200, Iteration: 9316/10000 --- Training Loss:0.000109\n",
      "Epoch: 187/200, Iteration: 9318/10000 --- Training Loss:0.000055\n",
      "Epoch: 187/200, Iteration: 9320/10000 --- Training Loss:0.000074\n",
      "Epoch: 187/200, Iteration: 9322/10000 --- Training Loss:0.000104\n",
      "Epoch: 187/200, Iteration: 9324/10000 --- Training Loss:0.000090\n",
      "Epoch: 187/200, Iteration: 9326/10000 --- Training Loss:0.000051\n",
      "Epoch: 187/200, Iteration: 9328/10000 --- Training Loss:0.000208\n",
      "Epoch: 187/200, Iteration: 9330/10000 --- Training Loss:0.000078\n",
      "Epoch: 187/200, Iteration: 9332/10000 --- Training Loss:0.000133\n",
      "Epoch: 187/200, Iteration: 9334/10000 --- Training Loss:0.000057\n",
      "Epoch: 187/200, Iteration: 9336/10000 --- Training Loss:0.000078\n",
      "Epoch: 187/200, Iteration: 9338/10000 --- Training Loss:0.000062\n",
      "Epoch: 187/200, Iteration: 9340/10000 --- Training Loss:0.000045\n",
      "Epoch: 187/200, Iteration: 9342/10000 --- Training Loss:0.000075\n",
      "Epoch: 187/200, Iteration: 9344/10000 --- Training Loss:0.000112\n",
      "Epoch: 187/200, Iteration: 9346/10000 --- Training Loss:0.000080\n",
      "Epoch: 187/200, Iteration: 9348/10000 --- Training Loss:0.000075\n",
      "Epoch: 187/200, Iteration: 9350/10000 --- Training Loss:0.000062\n",
      "Epoch: 187 finished ! Train Loss: 0.00010, Test Loss: 0.00136\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 188/200, Iteration: 9352/10000 --- Training Loss:0.000099\n",
      "Epoch: 188/200, Iteration: 9354/10000 --- Training Loss:0.000065\n",
      "Epoch: 188/200, Iteration: 9356/10000 --- Training Loss:0.000150\n",
      "Epoch: 188/200, Iteration: 9358/10000 --- Training Loss:0.000277\n",
      "Epoch: 188/200, Iteration: 9360/10000 --- Training Loss:0.000182\n",
      "Epoch: 188/200, Iteration: 9362/10000 --- Training Loss:0.000075\n",
      "Epoch: 188/200, Iteration: 9364/10000 --- Training Loss:0.000161\n",
      "Epoch: 188/200, Iteration: 9366/10000 --- Training Loss:0.000050\n",
      "Epoch: 188/200, Iteration: 9368/10000 --- Training Loss:0.000060\n",
      "Epoch: 188/200, Iteration: 9370/10000 --- Training Loss:0.000077\n",
      "Epoch: 188/200, Iteration: 9372/10000 --- Training Loss:0.000128\n",
      "Epoch: 188/200, Iteration: 9374/10000 --- Training Loss:0.000065\n",
      "Epoch: 188/200, Iteration: 9376/10000 --- Training Loss:0.000111\n",
      "Epoch: 188/200, Iteration: 9378/10000 --- Training Loss:0.000148\n",
      "Epoch: 188/200, Iteration: 9380/10000 --- Training Loss:0.000227\n",
      "Epoch: 188/200, Iteration: 9382/10000 --- Training Loss:0.000114\n",
      "Epoch: 188/200, Iteration: 9384/10000 --- Training Loss:0.000041\n",
      "Epoch: 188/200, Iteration: 9386/10000 --- Training Loss:0.000073\n",
      "Epoch: 188/200, Iteration: 9388/10000 --- Training Loss:0.000105\n",
      "Epoch: 188/200, Iteration: 9390/10000 --- Training Loss:0.000081\n",
      "Epoch: 188/200, Iteration: 9392/10000 --- Training Loss:0.000079\n",
      "Epoch: 188/200, Iteration: 9394/10000 --- Training Loss:0.000093\n",
      "Epoch: 188/200, Iteration: 9396/10000 --- Training Loss:0.000121\n",
      "Epoch: 188/200, Iteration: 9398/10000 --- Training Loss:0.000120\n",
      "Epoch: 188/200, Iteration: 9400/10000 --- Training Loss:0.000083\n",
      "Epoch: 188 finished ! Train Loss: 0.00010, Test Loss: 0.00025\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 189/200, Iteration: 9402/10000 --- Training Loss:0.000106\n",
      "Epoch: 189/200, Iteration: 9404/10000 --- Training Loss:0.000074\n",
      "Epoch: 189/200, Iteration: 9406/10000 --- Training Loss:0.000051\n",
      "Epoch: 189/200, Iteration: 9408/10000 --- Training Loss:0.000072\n",
      "Epoch: 189/200, Iteration: 9410/10000 --- Training Loss:0.000064\n",
      "Epoch: 189/200, Iteration: 9412/10000 --- Training Loss:0.000085\n",
      "Epoch: 189/200, Iteration: 9414/10000 --- Training Loss:0.000118\n",
      "Epoch: 189/200, Iteration: 9416/10000 --- Training Loss:0.000091\n",
      "Epoch: 189/200, Iteration: 9418/10000 --- Training Loss:0.000261\n",
      "Epoch: 189/200, Iteration: 9420/10000 --- Training Loss:0.000082\n",
      "Epoch: 189/200, Iteration: 9422/10000 --- Training Loss:0.000188\n",
      "Epoch: 189/200, Iteration: 9424/10000 --- Training Loss:0.000214\n",
      "Epoch: 189/200, Iteration: 9426/10000 --- Training Loss:0.000153\n",
      "Epoch: 189/200, Iteration: 9428/10000 --- Training Loss:0.000136\n",
      "Epoch: 189/200, Iteration: 9430/10000 --- Training Loss:0.000076\n",
      "Epoch: 189/200, Iteration: 9432/10000 --- Training Loss:0.000123\n",
      "Epoch: 189/200, Iteration: 9434/10000 --- Training Loss:0.000179\n",
      "Epoch: 189/200, Iteration: 9436/10000 --- Training Loss:0.000085\n",
      "Epoch: 189/200, Iteration: 9438/10000 --- Training Loss:0.000128\n",
      "Epoch: 189/200, Iteration: 9440/10000 --- Training Loss:0.000125\n",
      "Epoch: 189/200, Iteration: 9442/10000 --- Training Loss:0.000117\n",
      "Epoch: 189/200, Iteration: 9444/10000 --- Training Loss:0.000109\n",
      "Epoch: 189/200, Iteration: 9446/10000 --- Training Loss:0.000086\n",
      "Epoch: 189/200, Iteration: 9448/10000 --- Training Loss:0.000088\n",
      "Epoch: 189/200, Iteration: 9450/10000 --- Training Loss:0.000086\n",
      "Epoch: 189 finished ! Train Loss: 0.00012, Test Loss: 0.00146\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 190/200, Iteration: 9452/10000 --- Training Loss:0.000048\n",
      "Epoch: 190/200, Iteration: 9454/10000 --- Training Loss:0.000209\n",
      "Epoch: 190/200, Iteration: 9456/10000 --- Training Loss:0.000103\n",
      "Epoch: 190/200, Iteration: 9458/10000 --- Training Loss:0.000056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190/200, Iteration: 9460/10000 --- Training Loss:0.000078\n",
      "Epoch: 190/200, Iteration: 9462/10000 --- Training Loss:0.000106\n",
      "Epoch: 190/200, Iteration: 9464/10000 --- Training Loss:0.000131\n",
      "Epoch: 190/200, Iteration: 9466/10000 --- Training Loss:0.000066\n",
      "Epoch: 190/200, Iteration: 9468/10000 --- Training Loss:0.000086\n",
      "Epoch: 190/200, Iteration: 9470/10000 --- Training Loss:0.000056\n",
      "Epoch: 190/200, Iteration: 9472/10000 --- Training Loss:0.000087\n",
      "Epoch: 190/200, Iteration: 9474/10000 --- Training Loss:0.000068\n",
      "Epoch: 190/200, Iteration: 9476/10000 --- Training Loss:0.000186\n",
      "Epoch: 190/200, Iteration: 9478/10000 --- Training Loss:0.000072\n",
      "Epoch: 190/200, Iteration: 9480/10000 --- Training Loss:0.000106\n",
      "Epoch: 190/200, Iteration: 9482/10000 --- Training Loss:0.000087\n",
      "Epoch: 190/200, Iteration: 9484/10000 --- Training Loss:0.000065\n",
      "Epoch: 190/200, Iteration: 9486/10000 --- Training Loss:0.000100\n",
      "Epoch: 190/200, Iteration: 9488/10000 --- Training Loss:0.000127\n",
      "Epoch: 190/200, Iteration: 9490/10000 --- Training Loss:0.000045\n",
      "Epoch: 190/200, Iteration: 9492/10000 --- Training Loss:0.000117\n",
      "Epoch: 190/200, Iteration: 9494/10000 --- Training Loss:0.000048\n",
      "Epoch: 190/200, Iteration: 9496/10000 --- Training Loss:0.000244\n",
      "Epoch: 190/200, Iteration: 9498/10000 --- Training Loss:0.000121\n",
      "Epoch: 190/200, Iteration: 9500/10000 --- Training Loss:0.000081\n",
      "Epoch: 190 finished ! Train Loss: 0.00010, Test Loss: 0.00195\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 191/200, Iteration: 9502/10000 --- Training Loss:0.000068\n",
      "Epoch: 191/200, Iteration: 9504/10000 --- Training Loss:0.000067\n",
      "Epoch: 191/200, Iteration: 9506/10000 --- Training Loss:0.000131\n",
      "Epoch: 191/200, Iteration: 9508/10000 --- Training Loss:0.000213\n",
      "Epoch: 191/200, Iteration: 9510/10000 --- Training Loss:0.000624\n",
      "Epoch: 191/200, Iteration: 9512/10000 --- Training Loss:0.000199\n",
      "Epoch: 191/200, Iteration: 9514/10000 --- Training Loss:0.000285\n",
      "Epoch: 191/200, Iteration: 9516/10000 --- Training Loss:0.000152\n",
      "Epoch: 191/200, Iteration: 9518/10000 --- Training Loss:0.000226\n",
      "Epoch: 191/200, Iteration: 9520/10000 --- Training Loss:0.000203\n",
      "Epoch: 191/200, Iteration: 9522/10000 --- Training Loss:0.000168\n",
      "Epoch: 191/200, Iteration: 9524/10000 --- Training Loss:0.000200\n",
      "Epoch: 191/200, Iteration: 9526/10000 --- Training Loss:0.000324\n",
      "Epoch: 191/200, Iteration: 9528/10000 --- Training Loss:0.000171\n",
      "Epoch: 191/200, Iteration: 9530/10000 --- Training Loss:0.000216\n",
      "Epoch: 191/200, Iteration: 9532/10000 --- Training Loss:0.000228\n",
      "Epoch: 191/200, Iteration: 9534/10000 --- Training Loss:0.000304\n",
      "Epoch: 191/200, Iteration: 9536/10000 --- Training Loss:0.000118\n",
      "Epoch: 191/200, Iteration: 9538/10000 --- Training Loss:0.000202\n",
      "Epoch: 191/200, Iteration: 9540/10000 --- Training Loss:0.000132\n",
      "Epoch: 191/200, Iteration: 9542/10000 --- Training Loss:0.000259\n",
      "Epoch: 191/200, Iteration: 9544/10000 --- Training Loss:0.000149\n",
      "Epoch: 191/200, Iteration: 9546/10000 --- Training Loss:0.000068\n",
      "Epoch: 191/200, Iteration: 9548/10000 --- Training Loss:0.000095\n",
      "Epoch: 191/200, Iteration: 9550/10000 --- Training Loss:0.000166\n",
      "Epoch: 191 finished ! Train Loss: 0.00021, Test Loss: 0.00252\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 192/200, Iteration: 9552/10000 --- Training Loss:0.000142\n",
      "Epoch: 192/200, Iteration: 9554/10000 --- Training Loss:0.000218\n",
      "Epoch: 192/200, Iteration: 9556/10000 --- Training Loss:0.000111\n",
      "Epoch: 192/200, Iteration: 9558/10000 --- Training Loss:0.000120\n",
      "Epoch: 192/200, Iteration: 9560/10000 --- Training Loss:0.000171\n",
      "Epoch: 192/200, Iteration: 9562/10000 --- Training Loss:0.000131\n",
      "Epoch: 192/200, Iteration: 9564/10000 --- Training Loss:0.000149\n",
      "Epoch: 192/200, Iteration: 9566/10000 --- Training Loss:0.000070\n",
      "Epoch: 192/200, Iteration: 9568/10000 --- Training Loss:0.000107\n",
      "Epoch: 192/200, Iteration: 9570/10000 --- Training Loss:0.000093\n",
      "Epoch: 192/200, Iteration: 9572/10000 --- Training Loss:0.000189\n",
      "Epoch: 192/200, Iteration: 9574/10000 --- Training Loss:0.000168\n",
      "Epoch: 192/200, Iteration: 9576/10000 --- Training Loss:0.000075\n",
      "Epoch: 192/200, Iteration: 9578/10000 --- Training Loss:0.000083\n",
      "Epoch: 192/200, Iteration: 9580/10000 --- Training Loss:0.000091\n",
      "Epoch: 192/200, Iteration: 9582/10000 --- Training Loss:0.000083\n",
      "Epoch: 192/200, Iteration: 9584/10000 --- Training Loss:0.000058\n",
      "Epoch: 192/200, Iteration: 9586/10000 --- Training Loss:0.000240\n",
      "Epoch: 192/200, Iteration: 9588/10000 --- Training Loss:0.000166\n",
      "Epoch: 192/200, Iteration: 9590/10000 --- Training Loss:0.000132\n",
      "Epoch: 192/200, Iteration: 9592/10000 --- Training Loss:0.000093\n",
      "Epoch: 192/200, Iteration: 9594/10000 --- Training Loss:0.000278\n",
      "Epoch: 192/200, Iteration: 9596/10000 --- Training Loss:0.000151\n",
      "Epoch: 192/200, Iteration: 9598/10000 --- Training Loss:0.000157\n",
      "Epoch: 192/200, Iteration: 9600/10000 --- Training Loss:0.000125\n",
      "Epoch: 192 finished ! Train Loss: 0.00015, Test Loss: 0.00246\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 193/200, Iteration: 9602/10000 --- Training Loss:0.000098\n",
      "Epoch: 193/200, Iteration: 9604/10000 --- Training Loss:0.001087\n",
      "Epoch: 193/200, Iteration: 9606/10000 --- Training Loss:0.000271\n",
      "Epoch: 193/200, Iteration: 9608/10000 --- Training Loss:0.000192\n",
      "Epoch: 193/200, Iteration: 9610/10000 --- Training Loss:0.000367\n",
      "Epoch: 193/200, Iteration: 9612/10000 --- Training Loss:0.000103\n",
      "Epoch: 193/200, Iteration: 9614/10000 --- Training Loss:0.000212\n",
      "Epoch: 193/200, Iteration: 9616/10000 --- Training Loss:0.000125\n",
      "Epoch: 193/200, Iteration: 9618/10000 --- Training Loss:0.000142\n",
      "Epoch: 193/200, Iteration: 9620/10000 --- Training Loss:0.000165\n",
      "Epoch: 193/200, Iteration: 9622/10000 --- Training Loss:0.000152\n",
      "Epoch: 193/200, Iteration: 9624/10000 --- Training Loss:0.000088\n",
      "Epoch: 193/200, Iteration: 9626/10000 --- Training Loss:0.000100\n",
      "Epoch: 193/200, Iteration: 9628/10000 --- Training Loss:0.000098\n",
      "Epoch: 193/200, Iteration: 9630/10000 --- Training Loss:0.000099\n",
      "Epoch: 193/200, Iteration: 9632/10000 --- Training Loss:0.000120\n",
      "Epoch: 193/200, Iteration: 9634/10000 --- Training Loss:0.000063\n",
      "Epoch: 193/200, Iteration: 9636/10000 --- Training Loss:0.000065\n",
      "Epoch: 193/200, Iteration: 9638/10000 --- Training Loss:0.000112\n",
      "Epoch: 193/200, Iteration: 9640/10000 --- Training Loss:0.000095\n",
      "Epoch: 193/200, Iteration: 9642/10000 --- Training Loss:0.000078\n",
      "Epoch: 193/200, Iteration: 9644/10000 --- Training Loss:0.000215\n",
      "Epoch: 193/200, Iteration: 9646/10000 --- Training Loss:0.000051\n",
      "Epoch: 193/200, Iteration: 9648/10000 --- Training Loss:0.000082\n",
      "Epoch: 193/200, Iteration: 9650/10000 --- Training Loss:0.000064\n",
      "Epoch: 193 finished ! Train Loss: 0.00016, Test Loss: 0.00489\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 194/200, Iteration: 9652/10000 --- Training Loss:0.000183\n",
      "Epoch: 194/200, Iteration: 9654/10000 --- Training Loss:0.000068\n",
      "Epoch: 194/200, Iteration: 9656/10000 --- Training Loss:0.000071\n",
      "Epoch: 194/200, Iteration: 9658/10000 --- Training Loss:0.000124\n",
      "Epoch: 194/200, Iteration: 9660/10000 --- Training Loss:0.000094\n",
      "Epoch: 194/200, Iteration: 9662/10000 --- Training Loss:0.000098\n",
      "Epoch: 194/200, Iteration: 9664/10000 --- Training Loss:0.000150\n",
      "Epoch: 194/200, Iteration: 9666/10000 --- Training Loss:0.000054\n",
      "Epoch: 194/200, Iteration: 9668/10000 --- Training Loss:0.000037\n",
      "Epoch: 194/200, Iteration: 9670/10000 --- Training Loss:0.000052\n",
      "Epoch: 194/200, Iteration: 9672/10000 --- Training Loss:0.000213\n",
      "Epoch: 194/200, Iteration: 9674/10000 --- Training Loss:0.000082\n",
      "Epoch: 194/200, Iteration: 9676/10000 --- Training Loss:0.000077\n",
      "Epoch: 194/200, Iteration: 9678/10000 --- Training Loss:0.000050\n",
      "Epoch: 194/200, Iteration: 9680/10000 --- Training Loss:0.000065\n",
      "Epoch: 194/200, Iteration: 9682/10000 --- Training Loss:0.000124\n",
      "Epoch: 194/200, Iteration: 9684/10000 --- Training Loss:0.000122\n",
      "Epoch: 194/200, Iteration: 9686/10000 --- Training Loss:0.000266\n",
      "Epoch: 194/200, Iteration: 9688/10000 --- Training Loss:0.000127\n",
      "Epoch: 194/200, Iteration: 9690/10000 --- Training Loss:0.000082\n",
      "Epoch: 194/200, Iteration: 9692/10000 --- Training Loss:0.000188\n",
      "Epoch: 194/200, Iteration: 9694/10000 --- Training Loss:0.000052\n",
      "Epoch: 194/200, Iteration: 9696/10000 --- Training Loss:0.000093\n",
      "Epoch: 194/200, Iteration: 9698/10000 --- Training Loss:0.000138\n",
      "Epoch: 194/200, Iteration: 9700/10000 --- Training Loss:0.000059\n",
      "Epoch: 194 finished ! Train Loss: 0.00010, Test Loss: 0.00175\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 195/200, Iteration: 9702/10000 --- Training Loss:0.000074\n",
      "Epoch: 195/200, Iteration: 9704/10000 --- Training Loss:0.000059\n",
      "Epoch: 195/200, Iteration: 9706/10000 --- Training Loss:0.000113\n",
      "Epoch: 195/200, Iteration: 9708/10000 --- Training Loss:0.000049\n",
      "Epoch: 195/200, Iteration: 9710/10000 --- Training Loss:0.000178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195/200, Iteration: 9712/10000 --- Training Loss:0.000097\n",
      "Epoch: 195/200, Iteration: 9714/10000 --- Training Loss:0.000257\n",
      "Epoch: 195/200, Iteration: 9716/10000 --- Training Loss:0.000145\n",
      "Epoch: 195/200, Iteration: 9718/10000 --- Training Loss:0.000173\n",
      "Epoch: 195/200, Iteration: 9720/10000 --- Training Loss:0.000125\n",
      "Epoch: 195/200, Iteration: 9722/10000 --- Training Loss:0.000121\n",
      "Epoch: 195/200, Iteration: 9724/10000 --- Training Loss:0.000102\n",
      "Epoch: 195/200, Iteration: 9726/10000 --- Training Loss:0.000105\n",
      "Epoch: 195/200, Iteration: 9728/10000 --- Training Loss:0.000065\n",
      "Epoch: 195/200, Iteration: 9730/10000 --- Training Loss:0.000097\n",
      "Epoch: 195/200, Iteration: 9732/10000 --- Training Loss:0.000138\n",
      "Epoch: 195/200, Iteration: 9734/10000 --- Training Loss:0.000060\n",
      "Epoch: 195/200, Iteration: 9736/10000 --- Training Loss:0.000131\n",
      "Epoch: 195/200, Iteration: 9738/10000 --- Training Loss:0.000062\n",
      "Epoch: 195/200, Iteration: 9740/10000 --- Training Loss:0.000087\n",
      "Epoch: 195/200, Iteration: 9742/10000 --- Training Loss:0.000057\n",
      "Epoch: 195/200, Iteration: 9744/10000 --- Training Loss:0.000078\n",
      "Epoch: 195/200, Iteration: 9746/10000 --- Training Loss:0.000040\n",
      "Epoch: 195/200, Iteration: 9748/10000 --- Training Loss:0.000099\n",
      "Epoch: 195/200, Iteration: 9750/10000 --- Training Loss:0.000082\n",
      "Epoch: 195 finished ! Train Loss: 0.00010, Test Loss: 0.00420\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 196/200, Iteration: 9752/10000 --- Training Loss:0.000124\n",
      "Epoch: 196/200, Iteration: 9754/10000 --- Training Loss:0.000073\n",
      "Epoch: 196/200, Iteration: 9756/10000 --- Training Loss:0.000101\n",
      "Epoch: 196/200, Iteration: 9758/10000 --- Training Loss:0.000060\n",
      "Epoch: 196/200, Iteration: 9760/10000 --- Training Loss:0.000069\n",
      "Epoch: 196/200, Iteration: 9762/10000 --- Training Loss:0.000082\n",
      "Epoch: 196/200, Iteration: 9764/10000 --- Training Loss:0.000055\n",
      "Epoch: 196/200, Iteration: 9766/10000 --- Training Loss:0.000062\n",
      "Epoch: 196/200, Iteration: 9768/10000 --- Training Loss:0.000059\n",
      "Epoch: 196/200, Iteration: 9770/10000 --- Training Loss:0.000093\n",
      "Epoch: 196/200, Iteration: 9772/10000 --- Training Loss:0.000075\n",
      "Epoch: 196/200, Iteration: 9774/10000 --- Training Loss:0.000083\n",
      "Epoch: 196/200, Iteration: 9776/10000 --- Training Loss:0.000028\n",
      "Epoch: 196/200, Iteration: 9778/10000 --- Training Loss:0.000038\n",
      "Epoch: 196/200, Iteration: 9780/10000 --- Training Loss:0.000088\n",
      "Epoch: 196/200, Iteration: 9782/10000 --- Training Loss:0.000083\n",
      "Epoch: 196/200, Iteration: 9784/10000 --- Training Loss:0.000049\n",
      "Epoch: 196/200, Iteration: 9786/10000 --- Training Loss:0.000066\n",
      "Epoch: 196/200, Iteration: 9788/10000 --- Training Loss:0.000069\n",
      "Epoch: 196/200, Iteration: 9790/10000 --- Training Loss:0.000300\n",
      "Epoch: 196/200, Iteration: 9792/10000 --- Training Loss:0.000084\n",
      "Epoch: 196/200, Iteration: 9794/10000 --- Training Loss:0.000118\n",
      "Epoch: 196/200, Iteration: 9796/10000 --- Training Loss:0.000115\n",
      "Epoch: 196/200, Iteration: 9798/10000 --- Training Loss:0.000094\n",
      "Epoch: 196/200, Iteration: 9800/10000 --- Training Loss:0.000134\n",
      "Epoch: 196 finished ! Train Loss: 0.00009, Test Loss: 0.00053\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 197/200, Iteration: 9802/10000 --- Training Loss:0.000144\n",
      "Epoch: 197/200, Iteration: 9804/10000 --- Training Loss:0.000098\n",
      "Epoch: 197/200, Iteration: 9806/10000 --- Training Loss:0.000069\n",
      "Epoch: 197/200, Iteration: 9808/10000 --- Training Loss:0.000058\n",
      "Epoch: 197/200, Iteration: 9810/10000 --- Training Loss:0.000069\n",
      "Epoch: 197/200, Iteration: 9812/10000 --- Training Loss:0.000079\n",
      "Epoch: 197/200, Iteration: 9814/10000 --- Training Loss:0.000076\n",
      "Epoch: 197/200, Iteration: 9816/10000 --- Training Loss:0.000111\n",
      "Epoch: 197/200, Iteration: 9818/10000 --- Training Loss:0.000074\n",
      "Epoch: 197/200, Iteration: 9820/10000 --- Training Loss:0.000094\n",
      "Epoch: 197/200, Iteration: 9822/10000 --- Training Loss:0.000120\n",
      "Epoch: 197/200, Iteration: 9824/10000 --- Training Loss:0.000068\n",
      "Epoch: 197/200, Iteration: 9826/10000 --- Training Loss:0.000187\n",
      "Epoch: 197/200, Iteration: 9828/10000 --- Training Loss:0.000138\n",
      "Epoch: 197/200, Iteration: 9830/10000 --- Training Loss:0.000592\n",
      "Epoch: 197/200, Iteration: 9832/10000 --- Training Loss:0.000128\n",
      "Epoch: 197/200, Iteration: 9834/10000 --- Training Loss:0.000145\n",
      "Epoch: 197/200, Iteration: 9836/10000 --- Training Loss:0.000150\n",
      "Epoch: 197/200, Iteration: 9838/10000 --- Training Loss:0.000210\n",
      "Epoch: 197/200, Iteration: 9840/10000 --- Training Loss:0.000074\n",
      "Epoch: 197/200, Iteration: 9842/10000 --- Training Loss:0.000162\n",
      "Epoch: 197/200, Iteration: 9844/10000 --- Training Loss:0.000181\n",
      "Epoch: 197/200, Iteration: 9846/10000 --- Training Loss:0.000149\n",
      "Epoch: 197/200, Iteration: 9848/10000 --- Training Loss:0.000236\n",
      "Epoch: 197/200, Iteration: 9850/10000 --- Training Loss:0.000157\n",
      "Epoch: 197 finished ! Train Loss: 0.00014, Test Loss: 0.00278\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 198/200, Iteration: 9852/10000 --- Training Loss:0.000127\n",
      "Epoch: 198/200, Iteration: 9854/10000 --- Training Loss:0.000137\n",
      "Epoch: 198/200, Iteration: 9856/10000 --- Training Loss:0.000239\n",
      "Epoch: 198/200, Iteration: 9858/10000 --- Training Loss:0.000112\n",
      "Epoch: 198/200, Iteration: 9860/10000 --- Training Loss:0.000191\n",
      "Epoch: 198/200, Iteration: 9862/10000 --- Training Loss:0.000081\n",
      "Epoch: 198/200, Iteration: 9864/10000 --- Training Loss:0.000292\n",
      "Epoch: 198/200, Iteration: 9866/10000 --- Training Loss:0.000102\n",
      "Epoch: 198/200, Iteration: 9868/10000 --- Training Loss:0.000122\n",
      "Epoch: 198/200, Iteration: 9870/10000 --- Training Loss:0.000129\n",
      "Epoch: 198/200, Iteration: 9872/10000 --- Training Loss:0.000194\n",
      "Epoch: 198/200, Iteration: 9874/10000 --- Training Loss:0.000175\n",
      "Epoch: 198/200, Iteration: 9876/10000 --- Training Loss:0.000104\n",
      "Epoch: 198/200, Iteration: 9878/10000 --- Training Loss:0.000196\n",
      "Epoch: 198/200, Iteration: 9880/10000 --- Training Loss:0.000361\n",
      "Epoch: 198/200, Iteration: 9882/10000 --- Training Loss:0.000158\n",
      "Epoch: 198/200, Iteration: 9884/10000 --- Training Loss:0.000163\n",
      "Epoch: 198/200, Iteration: 9886/10000 --- Training Loss:0.000296\n",
      "Epoch: 198/200, Iteration: 9888/10000 --- Training Loss:0.000231\n",
      "Epoch: 198/200, Iteration: 9890/10000 --- Training Loss:0.000206\n",
      "Epoch: 198/200, Iteration: 9892/10000 --- Training Loss:0.000200\n",
      "Epoch: 198/200, Iteration: 9894/10000 --- Training Loss:0.000178\n",
      "Epoch: 198/200, Iteration: 9896/10000 --- Training Loss:0.000153\n",
      "Epoch: 198/200, Iteration: 9898/10000 --- Training Loss:0.000128\n",
      "Epoch: 198/200, Iteration: 9900/10000 --- Training Loss:0.000091\n",
      "Epoch: 198 finished ! Train Loss: 0.00018, Test Loss: 0.00088\n",
      "Epoch consuming time: 0m 1s\n",
      "Epoch: 199/200, Iteration: 9902/10000 --- Training Loss:0.000090\n",
      "Epoch: 199/200, Iteration: 9904/10000 --- Training Loss:0.000309\n",
      "Epoch: 199/200, Iteration: 9906/10000 --- Training Loss:0.000123\n",
      "Epoch: 199/200, Iteration: 9908/10000 --- Training Loss:0.000514\n",
      "Epoch: 199/200, Iteration: 9910/10000 --- Training Loss:0.000233\n",
      "Epoch: 199/200, Iteration: 9912/10000 --- Training Loss:0.000226\n",
      "Epoch: 199/200, Iteration: 9914/10000 --- Training Loss:0.000122\n",
      "Epoch: 199/200, Iteration: 9916/10000 --- Training Loss:0.000260\n",
      "Epoch: 199/200, Iteration: 9918/10000 --- Training Loss:0.000164\n",
      "Epoch: 199/200, Iteration: 9920/10000 --- Training Loss:0.000131\n",
      "Epoch: 199/200, Iteration: 9922/10000 --- Training Loss:0.000206\n",
      "Epoch: 199/200, Iteration: 9924/10000 --- Training Loss:0.000124\n",
      "Epoch: 199/200, Iteration: 9926/10000 --- Training Loss:0.000080\n",
      "Epoch: 199/200, Iteration: 9928/10000 --- Training Loss:0.000122\n",
      "Epoch: 199/200, Iteration: 9930/10000 --- Training Loss:0.000139\n",
      "Epoch: 199/200, Iteration: 9932/10000 --- Training Loss:0.000110\n",
      "Epoch: 199/200, Iteration: 9934/10000 --- Training Loss:0.000064\n",
      "Epoch: 199/200, Iteration: 9936/10000 --- Training Loss:0.000085\n",
      "Epoch: 199/200, Iteration: 9938/10000 --- Training Loss:0.000102\n",
      "Epoch: 199/200, Iteration: 9940/10000 --- Training Loss:0.000090\n",
      "Epoch: 199/200, Iteration: 9942/10000 --- Training Loss:0.000097\n",
      "Epoch: 199/200, Iteration: 9944/10000 --- Training Loss:0.000149\n",
      "Epoch: 199/200, Iteration: 9946/10000 --- Training Loss:0.000419\n",
      "Epoch: 199/200, Iteration: 9948/10000 --- Training Loss:0.000094\n",
      "Epoch: 199/200, Iteration: 9950/10000 --- Training Loss:0.000160\n",
      "Epoch: 199 finished ! Train Loss: 0.00016, Test Loss: 0.00051\n",
      "Epoch consuming time: 0m 1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200/200, Iteration: 9952/10000 --- Training Loss:0.000113\n",
      "Epoch: 200/200, Iteration: 9954/10000 --- Training Loss:0.000146\n",
      "Epoch: 200/200, Iteration: 9956/10000 --- Training Loss:0.000133\n",
      "Epoch: 200/200, Iteration: 9958/10000 --- Training Loss:0.000183\n",
      "Epoch: 200/200, Iteration: 9960/10000 --- Training Loss:0.000123\n",
      "Epoch: 200/200, Iteration: 9962/10000 --- Training Loss:0.000075\n",
      "Epoch: 200/200, Iteration: 9964/10000 --- Training Loss:0.000077\n",
      "Epoch: 200/200, Iteration: 9966/10000 --- Training Loss:0.000115\n",
      "Epoch: 200/200, Iteration: 9968/10000 --- Training Loss:0.000087\n",
      "Epoch: 200/200, Iteration: 9970/10000 --- Training Loss:0.000048\n",
      "Epoch: 200/200, Iteration: 9972/10000 --- Training Loss:0.000131\n",
      "Epoch: 200/200, Iteration: 9974/10000 --- Training Loss:0.000144\n",
      "Epoch: 200/200, Iteration: 9976/10000 --- Training Loss:0.000114\n",
      "Epoch: 200/200, Iteration: 9978/10000 --- Training Loss:0.000118\n",
      "Epoch: 200/200, Iteration: 9980/10000 --- Training Loss:0.000060\n",
      "Epoch: 200/200, Iteration: 9982/10000 --- Training Loss:0.000116\n",
      "Epoch: 200/200, Iteration: 9984/10000 --- Training Loss:0.000057\n",
      "Epoch: 200/200, Iteration: 9986/10000 --- Training Loss:0.000058\n",
      "Epoch: 200/200, Iteration: 9988/10000 --- Training Loss:0.000082\n",
      "Epoch: 200/200, Iteration: 9990/10000 --- Training Loss:0.000173\n",
      "Epoch: 200/200, Iteration: 9992/10000 --- Training Loss:0.000081\n",
      "Epoch: 200/200, Iteration: 9994/10000 --- Training Loss:0.000071\n",
      "Epoch: 200/200, Iteration: 9996/10000 --- Training Loss:0.000065\n",
      "Epoch: 200/200, Iteration: 9998/10000 --- Training Loss:0.000092\n",
      "Epoch: 200/200, Iteration: 10000/10000 --- Training Loss:0.000189\n",
      "Epoch: 200 finished ! Train Loss: 0.00012, Test Loss: 0.00077\n",
      "Epoch consuming time: 0m 1s\n",
      "Training complete in 3m  17s\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "########            TRAINING            ########\n",
    "################################################\n",
    "\n",
    "print() \n",
    "print('*******************************************') \n",
    "print('*******************************************') \n",
    "print('           START TRAINING                  ') \n",
    "print('*******************************************') \n",
    "print('*******************************************') \n",
    "print() \n",
    "\n",
    "\n",
    "print ('Original data dimention:%s'      %  str(DataDim))\n",
    "print ('Downsampled data dimention:%s '  %  str(data_dsp_dim))\n",
    "print ('Original label dimention:%s'     %  str(ModelDim))\n",
    "print ('Downsampled label dimention:%s'  %  str(label_dsp_dim))\n",
    "print ('Training size:%d'                %  int(TrainSize))\n",
    "print ('Traning batch size:%d'           %  int(BatchSize))\n",
    "print ('Number of epochs:%d'             %  int(Epochs))\n",
    "print ('Learning rate:%.5f'              %  float(LearnRate))\n",
    "              \n",
    "# Initialization\n",
    "loss1  = 0.0\n",
    "loss2  = 0.0\n",
    "step   = np.int(TrainSize/BatchSize)\n",
    "start  = time.time()\n",
    "min_loss  = float('inf')\n",
    "\n",
    "for epoch in range(Epochs): \n",
    "    epoch_loss = 0.0\n",
    "    since      = time.time()\n",
    "    for i, (images,labels,_) in enumerate(train_loader):        \n",
    "        iteration  = epoch*step+i+1\n",
    "        # Set Net with train condition\n",
    "        net.train()\n",
    "        \n",
    "        # Reshape data size\n",
    "        images = images.view(BatchSize,Inchannels,data_dsp_dim[0],data_dsp_dim[1])\n",
    "        labels = labels.view(BatchSize, -1)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradient buffer\n",
    "        optimizer.zero_grad()     \n",
    "        \n",
    "        # Forward prediction\n",
    "        outputs = net(images,label_dsp_dim)\n",
    "        \n",
    "        # Calculate the MSE\n",
    "        loss    = F.mse_loss(outputs,labels,reduction='sum')/(label_dsp_dim[0]*label_dsp_dim[1]*BatchSize)\n",
    "        \n",
    "        if np.isnan(float(loss.item())):\n",
    "            raise ValueError('loss is nan while training')\n",
    "            \n",
    "        epoch_loss += loss.item()    \n",
    "        # Loss backward propagation    \n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss\n",
    "        if iteration % DisplayStep == 0:\n",
    "            print('Epoch: {}/{}, Iteration: {}/{} --- Training Loss:{:.6f}'.format(epoch+1, \\\n",
    "                                                                               Epochs,iteration, \\\n",
    "                                                                              step*Epochs,loss.item()))        \n",
    "    with torch.no_grad():\n",
    "        epoch_loss_test = 0.0\n",
    "        for j, (images,labels,_) in enumerate(test_loader):\n",
    "            net.eval()\n",
    "            images = images.view(TestBatchSize,Inchannels,data_dsp_dim[0],data_dsp_dim[1])\n",
    "            labels = labels.view(TestBatchSize,-1)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images,label_dsp_dim)\n",
    "            loss    = F.mse_loss(outputs,labels,reduction='sum')/(label_dsp_dim[0]*label_dsp_dim[1]*TestBatchSize)\n",
    "            epoch_loss_test += loss.item()\n",
    "    # Print loss and consuming time every epoch\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        #print ('Epoch [%d/%d], Loss: %.10f' % (epoch+1,Epochs,loss.item()))          \n",
    "        print('Epoch: {:d} finished ! Train Loss: {:.5f}, Test Loss: {:.5f}'.format(epoch+1,epoch_loss/i,epoch_loss_test/j))\n",
    "        loss1 = np.append(loss1,epoch_loss/i)\n",
    "        loss2 = np.append(loss2,epoch_loss_test/j)\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Epoch consuming time: {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "    # Save net parameters with lowest loss\n",
    "    if epoch_loss_test < min_loss:\n",
    "        torch.save(net.state_dict(),models_dir+modelname+'_epoch'+'_best'+'_velocity_'+str(Bubble_num)+'.pkl')\n",
    "        min_loss = epoch_loss_test\n",
    "        print ('Trained model saved: %d percent completed'% int((epoch+1)*100/Epochs))\n",
    "    \n",
    "\n",
    "# Record the consuming time\n",
    "time_elapsed = time.time() - start\n",
    "print('Training complete in {:.0f}m  {:.0f}s' .format(time_elapsed //60 , time_elapsed % 60))\n",
    "\n",
    "# Save the loss\n",
    "SaveTrainResults_velocity(loss=loss1,loss_test=loss2,SavePath=results_dir,bubble_num=Bubble_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070d4c8",
   "metadata": {},
   "source": [
    "# Load the Trained Velocity Inversion Network with Lowest Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7717248d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** Loading the best trained model *****************\n",
      "\n",
      "Finish downloading: /home/pingchuan/TokyoBayInversion_2022_3_14/models/SimulataModel/Simulate_FCNVMBModel_TrainSize500_Epoch200_BatchSize10_LR0.005_epoch_best_velocity_1.pkl\n",
      "***************** Loading Training DataSet *****************\n",
      "***************** Loading Testing DataSet  *****************\n",
      "***************** Loading Dataset Success! *****************\n"
     ]
    }
   ],
   "source": [
    "#velocity\n",
    "if ReUse_best:\n",
    "    print('***************** Loading the best trained model *****************')\n",
    "    print('')\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    device         = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "    net = NetModel_v(in_channels=Inchannels,is_deconv=True,is_batchnorm=True,data_dim=DataDim,label_dim=ModelDim)\n",
    "    premodel_file = models_dir+modelname+'_epoch'+'_best'+'_velocity_'+str(Bubble_num)+'.pkl'\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    ##Load generator parameters\n",
    "    net.load_state_dict(torch.load(premodel_file))\n",
    "    net.to(device)\n",
    "    print('Finish downloading:',str(premodel_file))\n",
    "    \n",
    "    print('***************** Loading Training DataSet *****************')\n",
    "    train_set,label_set,data_dsp_dim,label_dsp_dim,train_radius  = DataLoad_Train(train_size=TrainSize,train_data_dir=train_data_dir, \\\n",
    "                                                                 data_dim=DataDim, \\\n",
    "                                                                 model_dim=ModelDim,data_dsp_blk=data_dsp_blk, \\\n",
    "                                                                 label_dsp_blk=label_dsp_blk, \\\n",
    "                                                                 truthfilename=truthfilename,velocity_flag=velocity_flag, \\\n",
    "                                                                 position = position)\n",
    "    # Change data type (numpy --> tensor)\n",
    "    train        = data_utils.TensorDataset(torch.from_numpy(train_set),torch.from_numpy(label_set),torch.from_numpy(train_radius))\n",
    "    train_loader = data_utils.DataLoader(train,batch_size=BatchSize,shuffle=True)\n",
    "\n",
    "    print('***************** Loading Testing DataSet  *****************')\n",
    "\n",
    "    test_set,label_set_t,data_dsp_dim_t,label_dsp_dim_t,test_radius = DataLoad_Test(test_size=TestSize,test_data_dir=test_data_dir, \\\n",
    "                                                              data_dim=DataDim, \\\n",
    "                                                              model_dim=ModelDim,data_dsp_blk=data_dsp_blk, \\\n",
    "                                                              label_dsp_blk=label_dsp_blk, \\\n",
    "                                                              truthfilename=truthfilename,velocity_flag=velocity_flag, \\\n",
    "                                                              position = position)\n",
    "\n",
    "    test        = data_utils.TensorDataset(torch.from_numpy(test_set),torch.from_numpy(label_set_t),torch.from_numpy(test_radius))\n",
    "    test_loader = data_utils.DataLoader(test,batch_size=TestBatchSize,shuffle=False)\n",
    "    \n",
    "    print('***************** Loading Dataset Success! *****************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7970f14a",
   "metadata": {},
   "source": [
    "# Predicting Velocity From Waveforms by Velocity Inversion Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6553ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******************************************\n",
      "*******************************************\n",
      "            START TESTING                  \n",
      "*******************************************\n",
      "*******************************************\n",
      "\n",
      "Testing complete in  0m 0s\n"
     ]
    }
   ],
   "source": [
    "#Test velocity\n",
    "################################################\n",
    "########            TESTING             ########\n",
    "################################################\n",
    "\n",
    "print() \n",
    "print('*******************************************') \n",
    "print('*******************************************') \n",
    "print('            START TESTING                  ') \n",
    "print('*******************************************') \n",
    "print('*******************************************') \n",
    "print()\n",
    "\n",
    "# Initialization\n",
    "since            = time.time()\n",
    "Prediction_test  = np.zeros((TestSize,label_dsp_dim[0],label_dsp_dim[1]),dtype=float)\n",
    "GT               = np.zeros((TestSize,label_dsp_dim[0],label_dsp_dim[1]),dtype=float)\n",
    "Prediction_train = np.zeros((TrainSize,label_dsp_dim[0],label_dsp_dim[1]),dtype=float)\n",
    "radius_train     = np.zeros((TrainSize,Bubble_num),dtype=float)\n",
    "radius_test      = np.zeros((TestSize,Bubble_num),dtype=float)\n",
    "total      = 0\n",
    "\n",
    "#train dataset\n",
    "for i, (images,_,radius) in enumerate(train_loader):        \n",
    "    images = images.view(BatchSize,Inchannels,data_dsp_dim[0],data_dsp_dim[1])\n",
    "    images = images.to(device)\n",
    "    \n",
    "    # Predictions\n",
    "    net.eval() \n",
    "    outputs  = net(images,label_dsp_dim)\n",
    "    outputs  = outputs.view(BatchSize,label_dsp_dim[0],label_dsp_dim[1])\n",
    "    outputs  = outputs.data.cpu().numpy()\n",
    "    \n",
    "    # Store Velocity Prediction and corresponding radius for next Network\n",
    "    for k in range(BatchSize):\n",
    "        pd   = outputs[k,:,:].reshape(label_dsp_dim[0],label_dsp_dim[1])\n",
    "        radius_train[i*BatchSize+k,:] = radius[k].reshape(-1)\n",
    "        Prediction_train[i*BatchSize+k,:,:] = pd\n",
    "        total = total + 1\n",
    "\n",
    "#test dataset\n",
    "for i, (images,labels,radius) in enumerate(test_loader):        \n",
    "    images = images.view(TestBatchSize,Inchannels,data_dsp_dim[0],data_dsp_dim[1])\n",
    "    labels = labels.view(TestBatchSize,Nclasses,label_dsp_dim[0],label_dsp_dim[1])\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Predictions\n",
    "    net.eval() \n",
    "    outputs  = net(images,label_dsp_dim)\n",
    "    outputs  = outputs.view(TestBatchSize,label_dsp_dim[0],label_dsp_dim[1])\n",
    "    outputs  = outputs.data.cpu().numpy()\n",
    "    gts      = labels.data.cpu().numpy()\n",
    "    \n",
    "    # Store Velocity Prediction and corresponding radius for next Network\n",
    "    for k in range(TestBatchSize):\n",
    "        pd   = outputs[k,:,:].reshape(label_dsp_dim[0],label_dsp_dim[1])\n",
    "        gt   = gts[k,:,:].reshape(label_dsp_dim[0],label_dsp_dim[1])\n",
    "        Prediction_test[i*TestBatchSize+k,:,:] = pd\n",
    "        GT[i*TestBatchSize+k,:,:] = gt\n",
    "        radius_test[i*TestBatchSize+k,:] = radius[k].reshape(-1)\n",
    "        total = total + 1\n",
    "\n",
    "# Save Results\n",
    "SaveTestResults_velocity(Prediction_test,GT,radius_test,results_dir,Bubble_num)\n",
    "        \n",
    "# Record the consuming time\n",
    "time_elapsed = time.time() - since\n",
    "print('Testing complete in  {:.0f}m {:.0f}s' .format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca946a",
   "metadata": {},
   "source": [
    "# Train Radius Inversion Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70fb28bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Iteration: 2/10000 --- Training Loss:3063.921875\n",
      "Epoch: 1/200, Iteration: 4/10000 --- Training Loss:1830.165405\n",
      "Epoch: 1/200, Iteration: 6/10000 --- Training Loss:3112.053711\n",
      "Epoch: 1/200, Iteration: 8/10000 --- Training Loss:1441.832520\n",
      "Epoch: 1/200, Iteration: 10/10000 --- Training Loss:580.625916\n",
      "Epoch: 1/200, Iteration: 12/10000 --- Training Loss:963.934753\n",
      "Epoch: 1/200, Iteration: 14/10000 --- Training Loss:749.679993\n",
      "Epoch: 1/200, Iteration: 16/10000 --- Training Loss:1040.624390\n",
      "Epoch: 1/200, Iteration: 18/10000 --- Training Loss:1134.265259\n",
      "Epoch: 1/200, Iteration: 20/10000 --- Training Loss:536.356873\n",
      "Epoch: 1/200, Iteration: 22/10000 --- Training Loss:634.817871\n",
      "Epoch: 1/200, Iteration: 24/10000 --- Training Loss:815.229919\n",
      "Epoch: 1/200, Iteration: 26/10000 --- Training Loss:608.982910\n",
      "Epoch: 1/200, Iteration: 28/10000 --- Training Loss:631.593689\n",
      "Epoch: 1/200, Iteration: 30/10000 --- Training Loss:719.506470\n",
      "Epoch: 1/200, Iteration: 32/10000 --- Training Loss:883.698059\n",
      "Epoch: 1/200, Iteration: 34/10000 --- Training Loss:821.447266\n",
      "Epoch: 1/200, Iteration: 36/10000 --- Training Loss:1094.509766\n",
      "Epoch: 1/200, Iteration: 38/10000 --- Training Loss:530.207092\n",
      "Epoch: 1/200, Iteration: 40/10000 --- Training Loss:1019.650208\n",
      "Epoch: 1/200, Iteration: 42/10000 --- Training Loss:837.752747\n",
      "Epoch: 1/200, Iteration: 44/10000 --- Training Loss:1077.326050\n",
      "Epoch: 1/200, Iteration: 46/10000 --- Training Loss:750.148315\n",
      "Epoch: 1/200, Iteration: 48/10000 --- Training Loss:870.109009\n",
      "Epoch: 1/200, Iteration: 50/10000 --- Training Loss:475.821533\n",
      "Epoch: 1 finished ! Train Loss: 1089.09909, Test Loss: 1018.08255\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 0 percent completed\n",
      "Epoch: 2/200, Iteration: 52/10000 --- Training Loss:436.686737\n",
      "Epoch: 2/200, Iteration: 54/10000 --- Training Loss:320.622162\n",
      "Epoch: 2/200, Iteration: 56/10000 --- Training Loss:610.414062\n",
      "Epoch: 2/200, Iteration: 58/10000 --- Training Loss:844.450806\n",
      "Epoch: 2/200, Iteration: 60/10000 --- Training Loss:812.654175\n",
      "Epoch: 2/200, Iteration: 62/10000 --- Training Loss:267.831909\n",
      "Epoch: 2/200, Iteration: 64/10000 --- Training Loss:831.745544\n",
      "Epoch: 2/200, Iteration: 66/10000 --- Training Loss:585.321838\n",
      "Epoch: 2/200, Iteration: 68/10000 --- Training Loss:248.194611\n",
      "Epoch: 2/200, Iteration: 70/10000 --- Training Loss:428.620850\n",
      "Epoch: 2/200, Iteration: 72/10000 --- Training Loss:320.800293\n",
      "Epoch: 2/200, Iteration: 74/10000 --- Training Loss:398.171844\n",
      "Epoch: 2/200, Iteration: 76/10000 --- Training Loss:521.541992\n",
      "Epoch: 2/200, Iteration: 78/10000 --- Training Loss:493.034393\n",
      "Epoch: 2/200, Iteration: 80/10000 --- Training Loss:402.559937\n",
      "Epoch: 2/200, Iteration: 82/10000 --- Training Loss:373.160614\n",
      "Epoch: 2/200, Iteration: 84/10000 --- Training Loss:372.590210\n",
      "Epoch: 2/200, Iteration: 86/10000 --- Training Loss:702.251770\n",
      "Epoch: 2/200, Iteration: 88/10000 --- Training Loss:314.310059\n",
      "Epoch: 2/200, Iteration: 90/10000 --- Training Loss:300.179260\n",
      "Epoch: 2/200, Iteration: 92/10000 --- Training Loss:278.683411\n",
      "Epoch: 2/200, Iteration: 94/10000 --- Training Loss:332.053070\n",
      "Epoch: 2/200, Iteration: 96/10000 --- Training Loss:308.944946\n",
      "Epoch: 2/200, Iteration: 98/10000 --- Training Loss:331.985992\n",
      "Epoch: 2/200, Iteration: 100/10000 --- Training Loss:333.213501\n",
      "Epoch: 2 finished ! Train Loss: 487.99935, Test Loss: 515.76425\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 1 percent completed\n",
      "Epoch: 3/200, Iteration: 102/10000 --- Training Loss:465.850189\n",
      "Epoch: 3/200, Iteration: 104/10000 --- Training Loss:475.127197\n",
      "Epoch: 3/200, Iteration: 106/10000 --- Training Loss:149.365768\n",
      "Epoch: 3/200, Iteration: 108/10000 --- Training Loss:127.652916\n",
      "Epoch: 3/200, Iteration: 110/10000 --- Training Loss:318.125397\n",
      "Epoch: 3/200, Iteration: 112/10000 --- Training Loss:250.571777\n",
      "Epoch: 3/200, Iteration: 114/10000 --- Training Loss:301.631927\n",
      "Epoch: 3/200, Iteration: 116/10000 --- Training Loss:356.192932\n",
      "Epoch: 3/200, Iteration: 118/10000 --- Training Loss:202.207504\n",
      "Epoch: 3/200, Iteration: 120/10000 --- Training Loss:166.424393\n",
      "Epoch: 3/200, Iteration: 122/10000 --- Training Loss:200.818848\n",
      "Epoch: 3/200, Iteration: 124/10000 --- Training Loss:175.439865\n",
      "Epoch: 3/200, Iteration: 126/10000 --- Training Loss:284.490601\n",
      "Epoch: 3/200, Iteration: 128/10000 --- Training Loss:318.825348\n",
      "Epoch: 3/200, Iteration: 130/10000 --- Training Loss:411.442291\n",
      "Epoch: 3/200, Iteration: 132/10000 --- Training Loss:75.503189\n",
      "Epoch: 3/200, Iteration: 134/10000 --- Training Loss:259.249390\n",
      "Epoch: 3/200, Iteration: 136/10000 --- Training Loss:256.514069\n",
      "Epoch: 3/200, Iteration: 138/10000 --- Training Loss:243.344727\n",
      "Epoch: 3/200, Iteration: 140/10000 --- Training Loss:295.594208\n",
      "Epoch: 3/200, Iteration: 142/10000 --- Training Loss:156.043549\n",
      "Epoch: 3/200, Iteration: 144/10000 --- Training Loss:236.443649\n",
      "Epoch: 3/200, Iteration: 146/10000 --- Training Loss:286.337402\n",
      "Epoch: 3/200, Iteration: 148/10000 --- Training Loss:208.701218\n",
      "Epoch: 3/200, Iteration: 150/10000 --- Training Loss:200.446411\n",
      "Epoch: 3 finished ! Train Loss: 264.95286, Test Loss: 252.02026\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 1 percent completed\n",
      "Epoch: 4/200, Iteration: 152/10000 --- Training Loss:179.051636\n",
      "Epoch: 4/200, Iteration: 154/10000 --- Training Loss:311.474030\n",
      "Epoch: 4/200, Iteration: 156/10000 --- Training Loss:194.958572\n",
      "Epoch: 4/200, Iteration: 158/10000 --- Training Loss:42.520630\n",
      "Epoch: 4/200, Iteration: 160/10000 --- Training Loss:208.834702\n",
      "Epoch: 4/200, Iteration: 162/10000 --- Training Loss:274.277191\n",
      "Epoch: 4/200, Iteration: 164/10000 --- Training Loss:135.154327\n",
      "Epoch: 4/200, Iteration: 166/10000 --- Training Loss:226.534790\n",
      "Epoch: 4/200, Iteration: 168/10000 --- Training Loss:120.224205\n",
      "Epoch: 4/200, Iteration: 170/10000 --- Training Loss:100.319725\n",
      "Epoch: 4/200, Iteration: 172/10000 --- Training Loss:114.396111\n",
      "Epoch: 4/200, Iteration: 174/10000 --- Training Loss:194.337387\n",
      "Epoch: 4/200, Iteration: 176/10000 --- Training Loss:117.144493\n",
      "Epoch: 4/200, Iteration: 178/10000 --- Training Loss:74.696434\n",
      "Epoch: 4/200, Iteration: 180/10000 --- Training Loss:211.818237\n",
      "Epoch: 4/200, Iteration: 182/10000 --- Training Loss:111.122673\n",
      "Epoch: 4/200, Iteration: 184/10000 --- Training Loss:133.459030\n",
      "Epoch: 4/200, Iteration: 186/10000 --- Training Loss:212.518463\n",
      "Epoch: 4/200, Iteration: 188/10000 --- Training Loss:80.234222\n",
      "Epoch: 4/200, Iteration: 190/10000 --- Training Loss:59.955914\n",
      "Epoch: 4/200, Iteration: 192/10000 --- Training Loss:103.101028\n",
      "Epoch: 4/200, Iteration: 194/10000 --- Training Loss:177.904160\n",
      "Epoch: 4/200, Iteration: 196/10000 --- Training Loss:99.154991\n",
      "Epoch: 4/200, Iteration: 198/10000 --- Training Loss:97.377815\n",
      "Epoch: 4/200, Iteration: 200/10000 --- Training Loss:135.952087\n",
      "Epoch: 4 finished ! Train Loss: 146.43048, Test Loss: 122.47685\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 2 percent completed\n",
      "Epoch: 5/200, Iteration: 202/10000 --- Training Loss:115.030594\n",
      "Epoch: 5/200, Iteration: 204/10000 --- Training Loss:60.512512\n",
      "Epoch: 5/200, Iteration: 206/10000 --- Training Loss:191.674179\n",
      "Epoch: 5/200, Iteration: 208/10000 --- Training Loss:120.068825\n",
      "Epoch: 5/200, Iteration: 210/10000 --- Training Loss:126.375877\n",
      "Epoch: 5/200, Iteration: 212/10000 --- Training Loss:69.871422\n",
      "Epoch: 5/200, Iteration: 214/10000 --- Training Loss:200.863388\n",
      "Epoch: 5/200, Iteration: 216/10000 --- Training Loss:92.029671\n",
      "Epoch: 5/200, Iteration: 218/10000 --- Training Loss:57.347412\n",
      "Epoch: 5/200, Iteration: 220/10000 --- Training Loss:140.420654\n",
      "Epoch: 5/200, Iteration: 222/10000 --- Training Loss:30.565943\n",
      "Epoch: 5/200, Iteration: 224/10000 --- Training Loss:33.069866\n",
      "Epoch: 5/200, Iteration: 226/10000 --- Training Loss:56.025734\n",
      "Epoch: 5/200, Iteration: 228/10000 --- Training Loss:62.824429\n",
      "Epoch: 5/200, Iteration: 230/10000 --- Training Loss:144.812912\n",
      "Epoch: 5/200, Iteration: 232/10000 --- Training Loss:34.938519\n",
      "Epoch: 5/200, Iteration: 234/10000 --- Training Loss:126.381508\n",
      "Epoch: 5/200, Iteration: 236/10000 --- Training Loss:75.151695\n",
      "Epoch: 5/200, Iteration: 238/10000 --- Training Loss:95.326698\n",
      "Epoch: 5/200, Iteration: 240/10000 --- Training Loss:58.421093\n",
      "Epoch: 5/200, Iteration: 242/10000 --- Training Loss:35.730305\n",
      "Epoch: 5/200, Iteration: 244/10000 --- Training Loss:24.747309\n",
      "Epoch: 5/200, Iteration: 246/10000 --- Training Loss:27.445002\n",
      "Epoch: 5/200, Iteration: 248/10000 --- Training Loss:19.525652\n",
      "Epoch: 5/200, Iteration: 250/10000 --- Training Loss:50.085842\n",
      "Epoch: 5 finished ! Train Loss: 83.03563, Test Loss: 53.77830\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 2 percent completed\n",
      "Epoch: 6/200, Iteration: 252/10000 --- Training Loss:79.336006\n",
      "Epoch: 6/200, Iteration: 254/10000 --- Training Loss:19.316370\n",
      "Epoch: 6/200, Iteration: 256/10000 --- Training Loss:100.027832\n",
      "Epoch: 6/200, Iteration: 258/10000 --- Training Loss:26.940695\n",
      "Epoch: 6/200, Iteration: 260/10000 --- Training Loss:13.344596\n",
      "Epoch: 6/200, Iteration: 262/10000 --- Training Loss:52.709011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/200, Iteration: 264/10000 --- Training Loss:35.463043\n",
      "Epoch: 6/200, Iteration: 266/10000 --- Training Loss:53.912769\n",
      "Epoch: 6/200, Iteration: 268/10000 --- Training Loss:33.667492\n",
      "Epoch: 6/200, Iteration: 270/10000 --- Training Loss:39.100101\n",
      "Epoch: 6/200, Iteration: 272/10000 --- Training Loss:103.906090\n",
      "Epoch: 6/200, Iteration: 274/10000 --- Training Loss:28.505426\n",
      "Epoch: 6/200, Iteration: 276/10000 --- Training Loss:53.639015\n",
      "Epoch: 6/200, Iteration: 278/10000 --- Training Loss:65.009125\n",
      "Epoch: 6/200, Iteration: 280/10000 --- Training Loss:60.239979\n",
      "Epoch: 6/200, Iteration: 282/10000 --- Training Loss:47.636494\n",
      "Epoch: 6/200, Iteration: 284/10000 --- Training Loss:17.232407\n",
      "Epoch: 6/200, Iteration: 286/10000 --- Training Loss:19.999443\n",
      "Epoch: 6/200, Iteration: 288/10000 --- Training Loss:44.739250\n",
      "Epoch: 6/200, Iteration: 290/10000 --- Training Loss:55.179127\n",
      "Epoch: 6/200, Iteration: 292/10000 --- Training Loss:40.503033\n",
      "Epoch: 6/200, Iteration: 294/10000 --- Training Loss:22.375811\n",
      "Epoch: 6/200, Iteration: 296/10000 --- Training Loss:48.016384\n",
      "Epoch: 6/200, Iteration: 298/10000 --- Training Loss:25.642471\n",
      "Epoch: 6/200, Iteration: 300/10000 --- Training Loss:4.642479\n",
      "Epoch: 6 finished ! Train Loss: 47.54541, Test Loss: 19.73854\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 3 percent completed\n",
      "Epoch: 7/200, Iteration: 302/10000 --- Training Loss:50.861645\n",
      "Epoch: 7/200, Iteration: 304/10000 --- Training Loss:22.086206\n",
      "Epoch: 7/200, Iteration: 306/10000 --- Training Loss:23.303610\n",
      "Epoch: 7/200, Iteration: 308/10000 --- Training Loss:80.532654\n",
      "Epoch: 7/200, Iteration: 310/10000 --- Training Loss:23.432531\n",
      "Epoch: 7/200, Iteration: 312/10000 --- Training Loss:14.540652\n",
      "Epoch: 7/200, Iteration: 314/10000 --- Training Loss:13.560256\n",
      "Epoch: 7/200, Iteration: 316/10000 --- Training Loss:28.068176\n",
      "Epoch: 7/200, Iteration: 318/10000 --- Training Loss:52.280098\n",
      "Epoch: 7/200, Iteration: 320/10000 --- Training Loss:31.095001\n",
      "Epoch: 7/200, Iteration: 322/10000 --- Training Loss:30.394260\n",
      "Epoch: 7/200, Iteration: 324/10000 --- Training Loss:32.100716\n",
      "Epoch: 7/200, Iteration: 326/10000 --- Training Loss:8.330146\n",
      "Epoch: 7/200, Iteration: 328/10000 --- Training Loss:17.982426\n",
      "Epoch: 7/200, Iteration: 330/10000 --- Training Loss:13.185817\n",
      "Epoch: 7/200, Iteration: 332/10000 --- Training Loss:58.488617\n",
      "Epoch: 7/200, Iteration: 334/10000 --- Training Loss:3.807634\n",
      "Epoch: 7/200, Iteration: 336/10000 --- Training Loss:8.846434\n",
      "Epoch: 7/200, Iteration: 338/10000 --- Training Loss:7.483029\n",
      "Epoch: 7/200, Iteration: 340/10000 --- Training Loss:7.527454\n",
      "Epoch: 7/200, Iteration: 342/10000 --- Training Loss:4.921701\n",
      "Epoch: 7/200, Iteration: 344/10000 --- Training Loss:105.278343\n",
      "Epoch: 7/200, Iteration: 346/10000 --- Training Loss:31.809265\n",
      "Epoch: 7/200, Iteration: 348/10000 --- Training Loss:27.608479\n",
      "Epoch: 7/200, Iteration: 350/10000 --- Training Loss:34.171967\n",
      "Epoch: 7 finished ! Train Loss: 27.48905, Test Loss: 6.61645\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 3 percent completed\n",
      "Epoch: 8/200, Iteration: 352/10000 --- Training Loss:16.931999\n",
      "Epoch: 8/200, Iteration: 354/10000 --- Training Loss:7.445278\n",
      "Epoch: 8/200, Iteration: 356/10000 --- Training Loss:16.041138\n",
      "Epoch: 8/200, Iteration: 358/10000 --- Training Loss:44.121792\n",
      "Epoch: 8/200, Iteration: 360/10000 --- Training Loss:2.468472\n",
      "Epoch: 8/200, Iteration: 362/10000 --- Training Loss:14.086359\n",
      "Epoch: 8/200, Iteration: 364/10000 --- Training Loss:2.878043\n",
      "Epoch: 8/200, Iteration: 366/10000 --- Training Loss:4.223398\n",
      "Epoch: 8/200, Iteration: 368/10000 --- Training Loss:39.618717\n",
      "Epoch: 8/200, Iteration: 370/10000 --- Training Loss:56.824482\n",
      "Epoch: 8/200, Iteration: 372/10000 --- Training Loss:2.468620\n",
      "Epoch: 8/200, Iteration: 374/10000 --- Training Loss:33.375942\n",
      "Epoch: 8/200, Iteration: 376/10000 --- Training Loss:6.521083\n",
      "Epoch: 8/200, Iteration: 378/10000 --- Training Loss:4.838224\n",
      "Epoch: 8/200, Iteration: 380/10000 --- Training Loss:17.555832\n",
      "Epoch: 8/200, Iteration: 382/10000 --- Training Loss:14.145637\n",
      "Epoch: 8/200, Iteration: 384/10000 --- Training Loss:4.915478\n",
      "Epoch: 8/200, Iteration: 386/10000 --- Training Loss:15.102992\n",
      "Epoch: 8/200, Iteration: 388/10000 --- Training Loss:9.081233\n",
      "Epoch: 8/200, Iteration: 390/10000 --- Training Loss:1.464255\n",
      "Epoch: 8/200, Iteration: 392/10000 --- Training Loss:4.283744\n",
      "Epoch: 8/200, Iteration: 394/10000 --- Training Loss:10.310076\n",
      "Epoch: 8/200, Iteration: 396/10000 --- Training Loss:32.168545\n",
      "Epoch: 8/200, Iteration: 398/10000 --- Training Loss:7.723001\n",
      "Epoch: 8/200, Iteration: 400/10000 --- Training Loss:40.045776\n",
      "Epoch: 8 finished ! Train Loss: 16.67256, Test Loss: 2.48200\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 4 percent completed\n",
      "Epoch: 9/200, Iteration: 402/10000 --- Training Loss:16.303455\n",
      "Epoch: 9/200, Iteration: 404/10000 --- Training Loss:31.180052\n",
      "Epoch: 9/200, Iteration: 406/10000 --- Training Loss:22.721464\n",
      "Epoch: 9/200, Iteration: 408/10000 --- Training Loss:16.103809\n",
      "Epoch: 9/200, Iteration: 410/10000 --- Training Loss:9.756083\n",
      "Epoch: 9/200, Iteration: 412/10000 --- Training Loss:15.348523\n",
      "Epoch: 9/200, Iteration: 414/10000 --- Training Loss:13.014239\n",
      "Epoch: 9/200, Iteration: 416/10000 --- Training Loss:5.537445\n",
      "Epoch: 9/200, Iteration: 418/10000 --- Training Loss:4.091281\n",
      "Epoch: 9/200, Iteration: 420/10000 --- Training Loss:10.343078\n",
      "Epoch: 9/200, Iteration: 422/10000 --- Training Loss:13.774628\n",
      "Epoch: 9/200, Iteration: 424/10000 --- Training Loss:1.585260\n",
      "Epoch: 9/200, Iteration: 426/10000 --- Training Loss:21.383987\n",
      "Epoch: 9/200, Iteration: 428/10000 --- Training Loss:8.362718\n",
      "Epoch: 9/200, Iteration: 430/10000 --- Training Loss:3.270702\n",
      "Epoch: 9/200, Iteration: 432/10000 --- Training Loss:11.670199\n",
      "Epoch: 9/200, Iteration: 434/10000 --- Training Loss:21.721933\n",
      "Epoch: 9/200, Iteration: 436/10000 --- Training Loss:1.653702\n",
      "Epoch: 9/200, Iteration: 438/10000 --- Training Loss:7.393720\n",
      "Epoch: 9/200, Iteration: 440/10000 --- Training Loss:4.956019\n",
      "Epoch: 9/200, Iteration: 442/10000 --- Training Loss:12.274277\n",
      "Epoch: 9/200, Iteration: 444/10000 --- Training Loss:2.142408\n",
      "Epoch: 9/200, Iteration: 446/10000 --- Training Loss:8.500331\n",
      "Epoch: 9/200, Iteration: 448/10000 --- Training Loss:16.213556\n",
      "Epoch: 9/200, Iteration: 450/10000 --- Training Loss:14.033025\n",
      "Epoch: 9 finished ! Train Loss: 10.44649, Test Loss: 2.15941\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 4 percent completed\n",
      "Epoch: 10/200, Iteration: 452/10000 --- Training Loss:2.505756\n",
      "Epoch: 10/200, Iteration: 454/10000 --- Training Loss:10.568843\n",
      "Epoch: 10/200, Iteration: 456/10000 --- Training Loss:7.471820\n",
      "Epoch: 10/200, Iteration: 458/10000 --- Training Loss:2.798716\n",
      "Epoch: 10/200, Iteration: 460/10000 --- Training Loss:16.912910\n",
      "Epoch: 10/200, Iteration: 462/10000 --- Training Loss:1.303857\n",
      "Epoch: 10/200, Iteration: 464/10000 --- Training Loss:8.145623\n",
      "Epoch: 10/200, Iteration: 466/10000 --- Training Loss:2.258958\n",
      "Epoch: 10/200, Iteration: 468/10000 --- Training Loss:5.991802\n",
      "Epoch: 10/200, Iteration: 470/10000 --- Training Loss:16.580938\n",
      "Epoch: 10/200, Iteration: 472/10000 --- Training Loss:6.389072\n",
      "Epoch: 10/200, Iteration: 474/10000 --- Training Loss:7.527005\n",
      "Epoch: 10/200, Iteration: 476/10000 --- Training Loss:1.597875\n",
      "Epoch: 10/200, Iteration: 478/10000 --- Training Loss:6.548135\n",
      "Epoch: 10/200, Iteration: 480/10000 --- Training Loss:4.003034\n",
      "Epoch: 10/200, Iteration: 482/10000 --- Training Loss:2.219731\n",
      "Epoch: 10/200, Iteration: 484/10000 --- Training Loss:34.733273\n",
      "Epoch: 10/200, Iteration: 486/10000 --- Training Loss:2.917578\n",
      "Epoch: 10/200, Iteration: 488/10000 --- Training Loss:1.616096\n",
      "Epoch: 10/200, Iteration: 490/10000 --- Training Loss:0.854409\n",
      "Epoch: 10/200, Iteration: 492/10000 --- Training Loss:2.009104\n",
      "Epoch: 10/200, Iteration: 494/10000 --- Training Loss:11.591358\n",
      "Epoch: 10/200, Iteration: 496/10000 --- Training Loss:5.738864\n",
      "Epoch: 10/200, Iteration: 498/10000 --- Training Loss:1.053090\n",
      "Epoch: 10/200, Iteration: 500/10000 --- Training Loss:3.417933\n",
      "Epoch: 10 finished ! Train Loss: 6.69432, Test Loss: 3.79229\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 11/200, Iteration: 502/10000 --- Training Loss:3.511887\n",
      "Epoch: 11/200, Iteration: 504/10000 --- Training Loss:3.525839\n",
      "Epoch: 11/200, Iteration: 506/10000 --- Training Loss:4.608870\n",
      "Epoch: 11/200, Iteration: 508/10000 --- Training Loss:10.293199\n",
      "Epoch: 11/200, Iteration: 510/10000 --- Training Loss:0.585492\n",
      "Epoch: 11/200, Iteration: 512/10000 --- Training Loss:3.767070\n",
      "Epoch: 11/200, Iteration: 514/10000 --- Training Loss:20.566236\n",
      "Epoch: 11/200, Iteration: 516/10000 --- Training Loss:1.304912\n",
      "Epoch: 11/200, Iteration: 518/10000 --- Training Loss:6.305377\n",
      "Epoch: 11/200, Iteration: 520/10000 --- Training Loss:7.081758\n",
      "Epoch: 11/200, Iteration: 522/10000 --- Training Loss:1.327418\n",
      "Epoch: 11/200, Iteration: 524/10000 --- Training Loss:12.720785\n",
      "Epoch: 11/200, Iteration: 526/10000 --- Training Loss:8.384810\n",
      "Epoch: 11/200, Iteration: 528/10000 --- Training Loss:6.388521\n",
      "Epoch: 11/200, Iteration: 530/10000 --- Training Loss:6.818449\n",
      "Epoch: 11/200, Iteration: 532/10000 --- Training Loss:0.355077\n",
      "Epoch: 11/200, Iteration: 534/10000 --- Training Loss:15.549045\n",
      "Epoch: 11/200, Iteration: 536/10000 --- Training Loss:0.573171\n",
      "Epoch: 11/200, Iteration: 538/10000 --- Training Loss:11.150652\n",
      "Epoch: 11/200, Iteration: 540/10000 --- Training Loss:5.034116\n",
      "Epoch: 11/200, Iteration: 542/10000 --- Training Loss:10.929133\n",
      "Epoch: 11/200, Iteration: 544/10000 --- Training Loss:0.861551\n",
      "Epoch: 11/200, Iteration: 546/10000 --- Training Loss:0.175478\n",
      "Epoch: 11/200, Iteration: 548/10000 --- Training Loss:3.432051\n",
      "Epoch: 11/200, Iteration: 550/10000 --- Training Loss:5.981686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 finished ! Train Loss: 4.64770, Test Loss: 6.00746\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 12/200, Iteration: 552/10000 --- Training Loss:0.307975\n",
      "Epoch: 12/200, Iteration: 554/10000 --- Training Loss:7.371383\n",
      "Epoch: 12/200, Iteration: 556/10000 --- Training Loss:1.297691\n",
      "Epoch: 12/200, Iteration: 558/10000 --- Training Loss:1.029791\n",
      "Epoch: 12/200, Iteration: 560/10000 --- Training Loss:0.404721\n",
      "Epoch: 12/200, Iteration: 562/10000 --- Training Loss:10.615885\n",
      "Epoch: 12/200, Iteration: 564/10000 --- Training Loss:0.677924\n",
      "Epoch: 12/200, Iteration: 566/10000 --- Training Loss:1.972993\n",
      "Epoch: 12/200, Iteration: 568/10000 --- Training Loss:9.571351\n",
      "Epoch: 12/200, Iteration: 570/10000 --- Training Loss:3.061698\n",
      "Epoch: 12/200, Iteration: 572/10000 --- Training Loss:6.627300\n",
      "Epoch: 12/200, Iteration: 574/10000 --- Training Loss:0.395823\n",
      "Epoch: 12/200, Iteration: 576/10000 --- Training Loss:0.463499\n",
      "Epoch: 12/200, Iteration: 578/10000 --- Training Loss:4.707957\n",
      "Epoch: 12/200, Iteration: 580/10000 --- Training Loss:7.635268\n",
      "Epoch: 12/200, Iteration: 582/10000 --- Training Loss:1.426830\n",
      "Epoch: 12/200, Iteration: 584/10000 --- Training Loss:3.163303\n",
      "Epoch: 12/200, Iteration: 586/10000 --- Training Loss:3.029907\n",
      "Epoch: 12/200, Iteration: 588/10000 --- Training Loss:0.818031\n",
      "Epoch: 12/200, Iteration: 590/10000 --- Training Loss:2.958573\n",
      "Epoch: 12/200, Iteration: 592/10000 --- Training Loss:0.697021\n",
      "Epoch: 12/200, Iteration: 594/10000 --- Training Loss:2.828511\n",
      "Epoch: 12/200, Iteration: 596/10000 --- Training Loss:1.915534\n",
      "Epoch: 12/200, Iteration: 598/10000 --- Training Loss:2.034269\n",
      "Epoch: 12/200, Iteration: 600/10000 --- Training Loss:0.731967\n",
      "Epoch: 12 finished ! Train Loss: 3.43808, Test Loss: 8.14645\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 13/200, Iteration: 602/10000 --- Training Loss:0.930090\n",
      "Epoch: 13/200, Iteration: 604/10000 --- Training Loss:0.304403\n",
      "Epoch: 13/200, Iteration: 606/10000 --- Training Loss:1.487831\n",
      "Epoch: 13/200, Iteration: 608/10000 --- Training Loss:0.322495\n",
      "Epoch: 13/200, Iteration: 610/10000 --- Training Loss:0.659538\n",
      "Epoch: 13/200, Iteration: 612/10000 --- Training Loss:2.806226\n",
      "Epoch: 13/200, Iteration: 614/10000 --- Training Loss:0.900269\n",
      "Epoch: 13/200, Iteration: 616/10000 --- Training Loss:4.109172\n",
      "Epoch: 13/200, Iteration: 618/10000 --- Training Loss:0.992633\n",
      "Epoch: 13/200, Iteration: 620/10000 --- Training Loss:4.822468\n",
      "Epoch: 13/200, Iteration: 622/10000 --- Training Loss:1.097384\n",
      "Epoch: 13/200, Iteration: 624/10000 --- Training Loss:0.296884\n",
      "Epoch: 13/200, Iteration: 626/10000 --- Training Loss:1.742867\n",
      "Epoch: 13/200, Iteration: 628/10000 --- Training Loss:0.829566\n",
      "Epoch: 13/200, Iteration: 630/10000 --- Training Loss:2.043799\n",
      "Epoch: 13/200, Iteration: 632/10000 --- Training Loss:0.340360\n",
      "Epoch: 13/200, Iteration: 634/10000 --- Training Loss:4.110178\n",
      "Epoch: 13/200, Iteration: 636/10000 --- Training Loss:1.406160\n",
      "Epoch: 13/200, Iteration: 638/10000 --- Training Loss:1.931493\n",
      "Epoch: 13/200, Iteration: 640/10000 --- Training Loss:0.923554\n",
      "Epoch: 13/200, Iteration: 642/10000 --- Training Loss:1.469950\n",
      "Epoch: 13/200, Iteration: 644/10000 --- Training Loss:1.403855\n",
      "Epoch: 13/200, Iteration: 646/10000 --- Training Loss:2.717974\n",
      "Epoch: 13/200, Iteration: 648/10000 --- Training Loss:0.330292\n",
      "Epoch: 13/200, Iteration: 650/10000 --- Training Loss:1.490305\n",
      "Epoch: 13 finished ! Train Loss: 2.77334, Test Loss: 8.86158\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 14/200, Iteration: 652/10000 --- Training Loss:0.484530\n",
      "Epoch: 14/200, Iteration: 654/10000 --- Training Loss:2.825922\n",
      "Epoch: 14/200, Iteration: 656/10000 --- Training Loss:0.909253\n",
      "Epoch: 14/200, Iteration: 658/10000 --- Training Loss:0.600165\n",
      "Epoch: 14/200, Iteration: 660/10000 --- Training Loss:1.632345\n",
      "Epoch: 14/200, Iteration: 662/10000 --- Training Loss:0.562277\n",
      "Epoch: 14/200, Iteration: 664/10000 --- Training Loss:1.099269\n",
      "Epoch: 14/200, Iteration: 666/10000 --- Training Loss:9.187456\n",
      "Epoch: 14/200, Iteration: 668/10000 --- Training Loss:8.375637\n",
      "Epoch: 14/200, Iteration: 670/10000 --- Training Loss:7.218473\n",
      "Epoch: 14/200, Iteration: 672/10000 --- Training Loss:0.593772\n",
      "Epoch: 14/200, Iteration: 674/10000 --- Training Loss:1.006235\n",
      "Epoch: 14/200, Iteration: 676/10000 --- Training Loss:1.031096\n",
      "Epoch: 14/200, Iteration: 678/10000 --- Training Loss:2.422223\n",
      "Epoch: 14/200, Iteration: 680/10000 --- Training Loss:3.810004\n",
      "Epoch: 14/200, Iteration: 682/10000 --- Training Loss:0.343210\n",
      "Epoch: 14/200, Iteration: 684/10000 --- Training Loss:1.112328\n",
      "Epoch: 14/200, Iteration: 686/10000 --- Training Loss:1.116411\n",
      "Epoch: 14/200, Iteration: 688/10000 --- Training Loss:2.873062\n",
      "Epoch: 14/200, Iteration: 690/10000 --- Training Loss:5.561297\n",
      "Epoch: 14/200, Iteration: 692/10000 --- Training Loss:7.247650\n",
      "Epoch: 14/200, Iteration: 694/10000 --- Training Loss:0.431650\n",
      "Epoch: 14/200, Iteration: 696/10000 --- Training Loss:1.406982\n",
      "Epoch: 14/200, Iteration: 698/10000 --- Training Loss:0.659542\n",
      "Epoch: 14/200, Iteration: 700/10000 --- Training Loss:4.245359\n",
      "Epoch: 14 finished ! Train Loss: 2.30962, Test Loss: 10.22578\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 15/200, Iteration: 702/10000 --- Training Loss:3.507715\n",
      "Epoch: 15/200, Iteration: 704/10000 --- Training Loss:3.171473\n",
      "Epoch: 15/200, Iteration: 706/10000 --- Training Loss:0.326895\n",
      "Epoch: 15/200, Iteration: 708/10000 --- Training Loss:7.096977\n",
      "Epoch: 15/200, Iteration: 710/10000 --- Training Loss:0.989361\n",
      "Epoch: 15/200, Iteration: 712/10000 --- Training Loss:0.362449\n",
      "Epoch: 15/200, Iteration: 714/10000 --- Training Loss:2.639308\n",
      "Epoch: 15/200, Iteration: 716/10000 --- Training Loss:0.547615\n",
      "Epoch: 15/200, Iteration: 718/10000 --- Training Loss:1.231130\n",
      "Epoch: 15/200, Iteration: 720/10000 --- Training Loss:0.712572\n",
      "Epoch: 15/200, Iteration: 722/10000 --- Training Loss:2.847216\n",
      "Epoch: 15/200, Iteration: 724/10000 --- Training Loss:2.229160\n",
      "Epoch: 15/200, Iteration: 726/10000 --- Training Loss:0.670477\n",
      "Epoch: 15/200, Iteration: 728/10000 --- Training Loss:0.343396\n",
      "Epoch: 15/200, Iteration: 730/10000 --- Training Loss:1.619290\n",
      "Epoch: 15/200, Iteration: 732/10000 --- Training Loss:0.324957\n",
      "Epoch: 15/200, Iteration: 734/10000 --- Training Loss:6.203622\n",
      "Epoch: 15/200, Iteration: 736/10000 --- Training Loss:0.402336\n",
      "Epoch: 15/200, Iteration: 738/10000 --- Training Loss:0.536588\n",
      "Epoch: 15/200, Iteration: 740/10000 --- Training Loss:0.830785\n",
      "Epoch: 15/200, Iteration: 742/10000 --- Training Loss:4.364334\n",
      "Epoch: 15/200, Iteration: 744/10000 --- Training Loss:1.599621\n",
      "Epoch: 15/200, Iteration: 746/10000 --- Training Loss:1.257030\n",
      "Epoch: 15/200, Iteration: 748/10000 --- Training Loss:0.697042\n",
      "Epoch: 15/200, Iteration: 750/10000 --- Training Loss:1.400262\n",
      "Epoch: 15 finished ! Train Loss: 1.99233, Test Loss: 10.14093\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 16/200, Iteration: 752/10000 --- Training Loss:1.419419\n",
      "Epoch: 16/200, Iteration: 754/10000 --- Training Loss:2.276611\n",
      "Epoch: 16/200, Iteration: 756/10000 --- Training Loss:2.445735\n",
      "Epoch: 16/200, Iteration: 758/10000 --- Training Loss:6.538078\n",
      "Epoch: 16/200, Iteration: 760/10000 --- Training Loss:0.872501\n",
      "Epoch: 16/200, Iteration: 762/10000 --- Training Loss:0.418021\n",
      "Epoch: 16/200, Iteration: 764/10000 --- Training Loss:1.831635\n",
      "Epoch: 16/200, Iteration: 766/10000 --- Training Loss:2.608112\n",
      "Epoch: 16/200, Iteration: 768/10000 --- Training Loss:5.460978\n",
      "Epoch: 16/200, Iteration: 770/10000 --- Training Loss:3.553665\n",
      "Epoch: 16/200, Iteration: 772/10000 --- Training Loss:1.007031\n",
      "Epoch: 16/200, Iteration: 774/10000 --- Training Loss:0.801173\n",
      "Epoch: 16/200, Iteration: 776/10000 --- Training Loss:1.330965\n",
      "Epoch: 16/200, Iteration: 778/10000 --- Training Loss:2.375984\n",
      "Epoch: 16/200, Iteration: 780/10000 --- Training Loss:0.570535\n",
      "Epoch: 16/200, Iteration: 782/10000 --- Training Loss:0.908925\n",
      "Epoch: 16/200, Iteration: 784/10000 --- Training Loss:1.523828\n",
      "Epoch: 16/200, Iteration: 786/10000 --- Training Loss:2.063431\n",
      "Epoch: 16/200, Iteration: 788/10000 --- Training Loss:0.780698\n",
      "Epoch: 16/200, Iteration: 790/10000 --- Training Loss:2.750073\n",
      "Epoch: 16/200, Iteration: 792/10000 --- Training Loss:0.357981\n",
      "Epoch: 16/200, Iteration: 794/10000 --- Training Loss:4.865318\n",
      "Epoch: 16/200, Iteration: 796/10000 --- Training Loss:0.489372\n",
      "Epoch: 16/200, Iteration: 798/10000 --- Training Loss:1.548671\n",
      "Epoch: 16/200, Iteration: 800/10000 --- Training Loss:0.270090\n",
      "Epoch: 16 finished ! Train Loss: 1.80002, Test Loss: 11.23376\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 17/200, Iteration: 802/10000 --- Training Loss:2.013597\n",
      "Epoch: 17/200, Iteration: 804/10000 --- Training Loss:0.620682\n",
      "Epoch: 17/200, Iteration: 806/10000 --- Training Loss:0.770660\n",
      "Epoch: 17/200, Iteration: 808/10000 --- Training Loss:0.561402\n",
      "Epoch: 17/200, Iteration: 810/10000 --- Training Loss:0.649876\n",
      "Epoch: 17/200, Iteration: 812/10000 --- Training Loss:0.303777\n",
      "Epoch: 17/200, Iteration: 814/10000 --- Training Loss:6.230512\n",
      "Epoch: 17/200, Iteration: 816/10000 --- Training Loss:0.597701\n",
      "Epoch: 17/200, Iteration: 818/10000 --- Training Loss:5.601146\n",
      "Epoch: 17/200, Iteration: 820/10000 --- Training Loss:1.791478\n",
      "Epoch: 17/200, Iteration: 822/10000 --- Training Loss:1.029279\n",
      "Epoch: 17/200, Iteration: 824/10000 --- Training Loss:1.355760\n",
      "Epoch: 17/200, Iteration: 826/10000 --- Training Loss:2.071422\n",
      "Epoch: 17/200, Iteration: 828/10000 --- Training Loss:0.450461\n",
      "Epoch: 17/200, Iteration: 830/10000 --- Training Loss:6.955496\n",
      "Epoch: 17/200, Iteration: 832/10000 --- Training Loss:1.082524\n",
      "Epoch: 17/200, Iteration: 834/10000 --- Training Loss:1.399026\n",
      "Epoch: 17/200, Iteration: 836/10000 --- Training Loss:5.721475\n",
      "Epoch: 17/200, Iteration: 838/10000 --- Training Loss:0.787219\n",
      "Epoch: 17/200, Iteration: 840/10000 --- Training Loss:1.346272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/200, Iteration: 842/10000 --- Training Loss:0.726558\n",
      "Epoch: 17/200, Iteration: 844/10000 --- Training Loss:1.009157\n",
      "Epoch: 17/200, Iteration: 846/10000 --- Training Loss:0.982211\n",
      "Epoch: 17/200, Iteration: 848/10000 --- Training Loss:2.840250\n",
      "Epoch: 17/200, Iteration: 850/10000 --- Training Loss:3.350950\n",
      "Epoch: 17 finished ! Train Loss: 1.65198, Test Loss: 10.60896\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 18/200, Iteration: 852/10000 --- Training Loss:0.471154\n",
      "Epoch: 18/200, Iteration: 854/10000 --- Training Loss:0.728065\n",
      "Epoch: 18/200, Iteration: 856/10000 --- Training Loss:0.963144\n",
      "Epoch: 18/200, Iteration: 858/10000 --- Training Loss:1.243032\n",
      "Epoch: 18/200, Iteration: 860/10000 --- Training Loss:0.819592\n",
      "Epoch: 18/200, Iteration: 862/10000 --- Training Loss:3.335923\n",
      "Epoch: 18/200, Iteration: 864/10000 --- Training Loss:1.124298\n",
      "Epoch: 18/200, Iteration: 866/10000 --- Training Loss:0.492524\n",
      "Epoch: 18/200, Iteration: 868/10000 --- Training Loss:0.518238\n",
      "Epoch: 18/200, Iteration: 870/10000 --- Training Loss:1.079393\n",
      "Epoch: 18/200, Iteration: 872/10000 --- Training Loss:0.660858\n",
      "Epoch: 18/200, Iteration: 874/10000 --- Training Loss:1.183348\n",
      "Epoch: 18/200, Iteration: 876/10000 --- Training Loss:1.308259\n",
      "Epoch: 18/200, Iteration: 878/10000 --- Training Loss:0.573195\n",
      "Epoch: 18/200, Iteration: 880/10000 --- Training Loss:1.720037\n",
      "Epoch: 18/200, Iteration: 882/10000 --- Training Loss:3.583464\n",
      "Epoch: 18/200, Iteration: 884/10000 --- Training Loss:6.068201\n",
      "Epoch: 18/200, Iteration: 886/10000 --- Training Loss:0.456865\n",
      "Epoch: 18/200, Iteration: 888/10000 --- Training Loss:2.225622\n",
      "Epoch: 18/200, Iteration: 890/10000 --- Training Loss:0.247920\n",
      "Epoch: 18/200, Iteration: 892/10000 --- Training Loss:0.510495\n",
      "Epoch: 18/200, Iteration: 894/10000 --- Training Loss:0.876971\n",
      "Epoch: 18/200, Iteration: 896/10000 --- Training Loss:2.519191\n",
      "Epoch: 18/200, Iteration: 898/10000 --- Training Loss:0.597673\n",
      "Epoch: 18/200, Iteration: 900/10000 --- Training Loss:2.156990\n",
      "Epoch: 18 finished ! Train Loss: 1.58497, Test Loss: 10.99001\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 19/200, Iteration: 902/10000 --- Training Loss:0.807449\n",
      "Epoch: 19/200, Iteration: 904/10000 --- Training Loss:0.821151\n",
      "Epoch: 19/200, Iteration: 906/10000 --- Training Loss:0.796085\n",
      "Epoch: 19/200, Iteration: 908/10000 --- Training Loss:4.053314\n",
      "Epoch: 19/200, Iteration: 910/10000 --- Training Loss:4.871787\n",
      "Epoch: 19/200, Iteration: 912/10000 --- Training Loss:2.178919\n",
      "Epoch: 19/200, Iteration: 914/10000 --- Training Loss:1.409553\n",
      "Epoch: 19/200, Iteration: 916/10000 --- Training Loss:2.475033\n",
      "Epoch: 19/200, Iteration: 918/10000 --- Training Loss:0.490953\n",
      "Epoch: 19/200, Iteration: 920/10000 --- Training Loss:1.266841\n",
      "Epoch: 19/200, Iteration: 922/10000 --- Training Loss:0.526374\n",
      "Epoch: 19/200, Iteration: 924/10000 --- Training Loss:0.765177\n",
      "Epoch: 19/200, Iteration: 926/10000 --- Training Loss:1.018903\n",
      "Epoch: 19/200, Iteration: 928/10000 --- Training Loss:1.022036\n",
      "Epoch: 19/200, Iteration: 930/10000 --- Training Loss:1.115164\n",
      "Epoch: 19/200, Iteration: 932/10000 --- Training Loss:0.872633\n",
      "Epoch: 19/200, Iteration: 934/10000 --- Training Loss:1.469920\n",
      "Epoch: 19/200, Iteration: 936/10000 --- Training Loss:2.438828\n",
      "Epoch: 19/200, Iteration: 938/10000 --- Training Loss:1.163059\n",
      "Epoch: 19/200, Iteration: 940/10000 --- Training Loss:0.512447\n",
      "Epoch: 19/200, Iteration: 942/10000 --- Training Loss:3.507803\n",
      "Epoch: 19/200, Iteration: 944/10000 --- Training Loss:0.860223\n",
      "Epoch: 19/200, Iteration: 946/10000 --- Training Loss:0.544844\n",
      "Epoch: 19/200, Iteration: 948/10000 --- Training Loss:1.197036\n",
      "Epoch: 19/200, Iteration: 950/10000 --- Training Loss:1.419487\n",
      "Epoch: 19 finished ! Train Loss: 1.48240, Test Loss: 10.03564\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 20/200, Iteration: 952/10000 --- Training Loss:0.998273\n",
      "Epoch: 20/200, Iteration: 954/10000 --- Training Loss:1.486772\n",
      "Epoch: 20/200, Iteration: 956/10000 --- Training Loss:0.755607\n",
      "Epoch: 20/200, Iteration: 958/10000 --- Training Loss:0.852484\n",
      "Epoch: 20/200, Iteration: 960/10000 --- Training Loss:0.693290\n",
      "Epoch: 20/200, Iteration: 962/10000 --- Training Loss:1.162973\n",
      "Epoch: 20/200, Iteration: 964/10000 --- Training Loss:0.840966\n",
      "Epoch: 20/200, Iteration: 966/10000 --- Training Loss:0.618230\n",
      "Epoch: 20/200, Iteration: 968/10000 --- Training Loss:1.203485\n",
      "Epoch: 20/200, Iteration: 970/10000 --- Training Loss:2.553044\n",
      "Epoch: 20/200, Iteration: 972/10000 --- Training Loss:0.251587\n",
      "Epoch: 20/200, Iteration: 974/10000 --- Training Loss:0.532971\n",
      "Epoch: 20/200, Iteration: 976/10000 --- Training Loss:1.517793\n",
      "Epoch: 20/200, Iteration: 978/10000 --- Training Loss:1.108356\n",
      "Epoch: 20/200, Iteration: 980/10000 --- Training Loss:0.642074\n",
      "Epoch: 20/200, Iteration: 982/10000 --- Training Loss:0.790024\n",
      "Epoch: 20/200, Iteration: 984/10000 --- Training Loss:1.188546\n",
      "Epoch: 20/200, Iteration: 986/10000 --- Training Loss:1.043467\n",
      "Epoch: 20/200, Iteration: 988/10000 --- Training Loss:4.335891\n",
      "Epoch: 20/200, Iteration: 990/10000 --- Training Loss:0.301286\n",
      "Epoch: 20/200, Iteration: 992/10000 --- Training Loss:1.528360\n",
      "Epoch: 20/200, Iteration: 994/10000 --- Training Loss:1.029159\n",
      "Epoch: 20/200, Iteration: 996/10000 --- Training Loss:0.833052\n",
      "Epoch: 20/200, Iteration: 998/10000 --- Training Loss:3.357165\n",
      "Epoch: 20/200, Iteration: 1000/10000 --- Training Loss:0.461814\n",
      "Epoch: 20 finished ! Train Loss: 1.39713, Test Loss: 10.67696\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 21/200, Iteration: 1002/10000 --- Training Loss:0.836981\n",
      "Epoch: 21/200, Iteration: 1004/10000 --- Training Loss:0.267743\n",
      "Epoch: 21/200, Iteration: 1006/10000 --- Training Loss:2.713003\n",
      "Epoch: 21/200, Iteration: 1008/10000 --- Training Loss:1.123979\n",
      "Epoch: 21/200, Iteration: 1010/10000 --- Training Loss:0.388787\n",
      "Epoch: 21/200, Iteration: 1012/10000 --- Training Loss:0.573778\n",
      "Epoch: 21/200, Iteration: 1014/10000 --- Training Loss:0.353946\n",
      "Epoch: 21/200, Iteration: 1016/10000 --- Training Loss:0.704076\n",
      "Epoch: 21/200, Iteration: 1018/10000 --- Training Loss:3.456644\n",
      "Epoch: 21/200, Iteration: 1020/10000 --- Training Loss:0.982724\n",
      "Epoch: 21/200, Iteration: 1022/10000 --- Training Loss:2.276512\n",
      "Epoch: 21/200, Iteration: 1024/10000 --- Training Loss:0.466370\n",
      "Epoch: 21/200, Iteration: 1026/10000 --- Training Loss:2.321826\n",
      "Epoch: 21/200, Iteration: 1028/10000 --- Training Loss:1.877424\n",
      "Epoch: 21/200, Iteration: 1030/10000 --- Training Loss:0.665069\n",
      "Epoch: 21/200, Iteration: 1032/10000 --- Training Loss:2.317490\n",
      "Epoch: 21/200, Iteration: 1034/10000 --- Training Loss:2.130791\n",
      "Epoch: 21/200, Iteration: 1036/10000 --- Training Loss:1.080783\n",
      "Epoch: 21/200, Iteration: 1038/10000 --- Training Loss:1.298168\n",
      "Epoch: 21/200, Iteration: 1040/10000 --- Training Loss:0.934210\n",
      "Epoch: 21/200, Iteration: 1042/10000 --- Training Loss:4.276721\n",
      "Epoch: 21/200, Iteration: 1044/10000 --- Training Loss:0.734092\n",
      "Epoch: 21/200, Iteration: 1046/10000 --- Training Loss:0.989215\n",
      "Epoch: 21/200, Iteration: 1048/10000 --- Training Loss:0.827222\n",
      "Epoch: 21/200, Iteration: 1050/10000 --- Training Loss:0.728204\n",
      "Epoch: 21 finished ! Train Loss: 1.34378, Test Loss: 10.71431\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 22/200, Iteration: 1052/10000 --- Training Loss:1.103892\n",
      "Epoch: 22/200, Iteration: 1054/10000 --- Training Loss:1.615630\n",
      "Epoch: 22/200, Iteration: 1056/10000 --- Training Loss:2.953238\n",
      "Epoch: 22/200, Iteration: 1058/10000 --- Training Loss:0.423499\n",
      "Epoch: 22/200, Iteration: 1060/10000 --- Training Loss:0.456263\n",
      "Epoch: 22/200, Iteration: 1062/10000 --- Training Loss:1.008402\n",
      "Epoch: 22/200, Iteration: 1064/10000 --- Training Loss:1.187481\n",
      "Epoch: 22/200, Iteration: 1066/10000 --- Training Loss:0.262975\n",
      "Epoch: 22/200, Iteration: 1068/10000 --- Training Loss:0.865402\n",
      "Epoch: 22/200, Iteration: 1070/10000 --- Training Loss:1.420037\n",
      "Epoch: 22/200, Iteration: 1072/10000 --- Training Loss:0.368126\n",
      "Epoch: 22/200, Iteration: 1074/10000 --- Training Loss:1.153348\n",
      "Epoch: 22/200, Iteration: 1076/10000 --- Training Loss:3.305120\n",
      "Epoch: 22/200, Iteration: 1078/10000 --- Training Loss:0.410144\n",
      "Epoch: 22/200, Iteration: 1080/10000 --- Training Loss:1.467313\n",
      "Epoch: 22/200, Iteration: 1082/10000 --- Training Loss:0.764268\n",
      "Epoch: 22/200, Iteration: 1084/10000 --- Training Loss:0.294851\n",
      "Epoch: 22/200, Iteration: 1086/10000 --- Training Loss:0.907769\n",
      "Epoch: 22/200, Iteration: 1088/10000 --- Training Loss:0.993528\n",
      "Epoch: 22/200, Iteration: 1090/10000 --- Training Loss:0.663305\n",
      "Epoch: 22/200, Iteration: 1092/10000 --- Training Loss:3.883909\n",
      "Epoch: 22/200, Iteration: 1094/10000 --- Training Loss:1.547377\n",
      "Epoch: 22/200, Iteration: 1096/10000 --- Training Loss:0.785035\n",
      "Epoch: 22/200, Iteration: 1098/10000 --- Training Loss:0.100965\n",
      "Epoch: 22/200, Iteration: 1100/10000 --- Training Loss:1.594455\n",
      "Epoch: 22 finished ! Train Loss: 1.27124, Test Loss: 10.41587\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 23/200, Iteration: 1102/10000 --- Training Loss:1.425510\n",
      "Epoch: 23/200, Iteration: 1104/10000 --- Training Loss:3.234241\n",
      "Epoch: 23/200, Iteration: 1106/10000 --- Training Loss:1.183647\n",
      "Epoch: 23/200, Iteration: 1108/10000 --- Training Loss:1.012578\n",
      "Epoch: 23/200, Iteration: 1110/10000 --- Training Loss:2.055741\n",
      "Epoch: 23/200, Iteration: 1112/10000 --- Training Loss:0.546202\n",
      "Epoch: 23/200, Iteration: 1114/10000 --- Training Loss:2.224653\n",
      "Epoch: 23/200, Iteration: 1116/10000 --- Training Loss:0.155192\n",
      "Epoch: 23/200, Iteration: 1118/10000 --- Training Loss:1.685588\n",
      "Epoch: 23/200, Iteration: 1120/10000 --- Training Loss:0.348563\n",
      "Epoch: 23/200, Iteration: 1122/10000 --- Training Loss:0.885421\n",
      "Epoch: 23/200, Iteration: 1124/10000 --- Training Loss:0.804300\n",
      "Epoch: 23/200, Iteration: 1126/10000 --- Training Loss:1.229373\n",
      "Epoch: 23/200, Iteration: 1128/10000 --- Training Loss:0.371128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/200, Iteration: 1130/10000 --- Training Loss:0.373971\n",
      "Epoch: 23/200, Iteration: 1132/10000 --- Training Loss:0.833696\n",
      "Epoch: 23/200, Iteration: 1134/10000 --- Training Loss:0.428179\n",
      "Epoch: 23/200, Iteration: 1136/10000 --- Training Loss:4.340922\n",
      "Epoch: 23/200, Iteration: 1138/10000 --- Training Loss:0.132194\n",
      "Epoch: 23/200, Iteration: 1140/10000 --- Training Loss:0.848643\n",
      "Epoch: 23/200, Iteration: 1142/10000 --- Training Loss:1.562815\n",
      "Epoch: 23/200, Iteration: 1144/10000 --- Training Loss:1.435887\n",
      "Epoch: 23/200, Iteration: 1146/10000 --- Training Loss:2.305510\n",
      "Epoch: 23/200, Iteration: 1148/10000 --- Training Loss:1.285189\n",
      "Epoch: 23/200, Iteration: 1150/10000 --- Training Loss:0.701215\n",
      "Epoch: 23 finished ! Train Loss: 1.26298, Test Loss: 10.15078\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 24/200, Iteration: 1152/10000 --- Training Loss:4.003129\n",
      "Epoch: 24/200, Iteration: 1154/10000 --- Training Loss:0.226291\n",
      "Epoch: 24/200, Iteration: 1156/10000 --- Training Loss:0.655741\n",
      "Epoch: 24/200, Iteration: 1158/10000 --- Training Loss:5.003156\n",
      "Epoch: 24/200, Iteration: 1160/10000 --- Training Loss:0.720963\n",
      "Epoch: 24/200, Iteration: 1162/10000 --- Training Loss:0.597243\n",
      "Epoch: 24/200, Iteration: 1164/10000 --- Training Loss:0.444934\n",
      "Epoch: 24/200, Iteration: 1166/10000 --- Training Loss:0.939318\n",
      "Epoch: 24/200, Iteration: 1168/10000 --- Training Loss:0.435748\n",
      "Epoch: 24/200, Iteration: 1170/10000 --- Training Loss:0.297365\n",
      "Epoch: 24/200, Iteration: 1172/10000 --- Training Loss:1.221982\n",
      "Epoch: 24/200, Iteration: 1174/10000 --- Training Loss:0.412455\n",
      "Epoch: 24/200, Iteration: 1176/10000 --- Training Loss:0.190535\n",
      "Epoch: 24/200, Iteration: 1178/10000 --- Training Loss:4.502541\n",
      "Epoch: 24/200, Iteration: 1180/10000 --- Training Loss:1.385437\n",
      "Epoch: 24/200, Iteration: 1182/10000 --- Training Loss:0.913771\n",
      "Epoch: 24/200, Iteration: 1184/10000 --- Training Loss:0.752567\n",
      "Epoch: 24/200, Iteration: 1186/10000 --- Training Loss:0.988104\n",
      "Epoch: 24/200, Iteration: 1188/10000 --- Training Loss:0.619721\n",
      "Epoch: 24/200, Iteration: 1190/10000 --- Training Loss:0.397962\n",
      "Epoch: 24/200, Iteration: 1192/10000 --- Training Loss:1.171219\n",
      "Epoch: 24/200, Iteration: 1194/10000 --- Training Loss:1.106689\n",
      "Epoch: 24/200, Iteration: 1196/10000 --- Training Loss:0.386687\n",
      "Epoch: 24/200, Iteration: 1198/10000 --- Training Loss:0.457307\n",
      "Epoch: 24/200, Iteration: 1200/10000 --- Training Loss:3.996489\n",
      "Epoch: 24 finished ! Train Loss: 1.20212, Test Loss: 9.90410\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 25/200, Iteration: 1202/10000 --- Training Loss:0.105082\n",
      "Epoch: 25/200, Iteration: 1204/10000 --- Training Loss:0.290931\n",
      "Epoch: 25/200, Iteration: 1206/10000 --- Training Loss:3.969546\n",
      "Epoch: 25/200, Iteration: 1208/10000 --- Training Loss:0.796226\n",
      "Epoch: 25/200, Iteration: 1210/10000 --- Training Loss:1.081859\n",
      "Epoch: 25/200, Iteration: 1212/10000 --- Training Loss:0.471363\n",
      "Epoch: 25/200, Iteration: 1214/10000 --- Training Loss:0.858007\n",
      "Epoch: 25/200, Iteration: 1216/10000 --- Training Loss:0.277908\n",
      "Epoch: 25/200, Iteration: 1218/10000 --- Training Loss:1.240665\n",
      "Epoch: 25/200, Iteration: 1220/10000 --- Training Loss:2.308227\n",
      "Epoch: 25/200, Iteration: 1222/10000 --- Training Loss:1.675617\n",
      "Epoch: 25/200, Iteration: 1224/10000 --- Training Loss:0.481133\n",
      "Epoch: 25/200, Iteration: 1226/10000 --- Training Loss:1.193596\n",
      "Epoch: 25/200, Iteration: 1228/10000 --- Training Loss:0.372565\n",
      "Epoch: 25/200, Iteration: 1230/10000 --- Training Loss:0.606577\n",
      "Epoch: 25/200, Iteration: 1232/10000 --- Training Loss:0.601241\n",
      "Epoch: 25/200, Iteration: 1234/10000 --- Training Loss:0.284514\n",
      "Epoch: 25/200, Iteration: 1236/10000 --- Training Loss:1.310831\n",
      "Epoch: 25/200, Iteration: 1238/10000 --- Training Loss:3.046124\n",
      "Epoch: 25/200, Iteration: 1240/10000 --- Training Loss:2.845176\n",
      "Epoch: 25/200, Iteration: 1242/10000 --- Training Loss:1.189355\n",
      "Epoch: 25/200, Iteration: 1244/10000 --- Training Loss:1.330531\n",
      "Epoch: 25/200, Iteration: 1246/10000 --- Training Loss:1.409168\n",
      "Epoch: 25/200, Iteration: 1248/10000 --- Training Loss:0.892854\n",
      "Epoch: 25/200, Iteration: 1250/10000 --- Training Loss:4.179453\n",
      "Epoch: 25 finished ! Train Loss: 1.16516, Test Loss: 9.86346\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 26/200, Iteration: 1252/10000 --- Training Loss:0.156483\n",
      "Epoch: 26/200, Iteration: 1254/10000 --- Training Loss:1.098049\n",
      "Epoch: 26/200, Iteration: 1256/10000 --- Training Loss:2.195172\n",
      "Epoch: 26/200, Iteration: 1258/10000 --- Training Loss:0.322954\n",
      "Epoch: 26/200, Iteration: 1260/10000 --- Training Loss:0.415760\n",
      "Epoch: 26/200, Iteration: 1262/10000 --- Training Loss:2.158294\n",
      "Epoch: 26/200, Iteration: 1264/10000 --- Training Loss:1.052608\n",
      "Epoch: 26/200, Iteration: 1266/10000 --- Training Loss:0.349549\n",
      "Epoch: 26/200, Iteration: 1268/10000 --- Training Loss:1.301854\n",
      "Epoch: 26/200, Iteration: 1270/10000 --- Training Loss:0.204502\n",
      "Epoch: 26/200, Iteration: 1272/10000 --- Training Loss:2.863380\n",
      "Epoch: 26/200, Iteration: 1274/10000 --- Training Loss:0.525685\n",
      "Epoch: 26/200, Iteration: 1276/10000 --- Training Loss:0.747640\n",
      "Epoch: 26/200, Iteration: 1278/10000 --- Training Loss:0.627804\n",
      "Epoch: 26/200, Iteration: 1280/10000 --- Training Loss:0.233461\n",
      "Epoch: 26/200, Iteration: 1282/10000 --- Training Loss:0.439793\n",
      "Epoch: 26/200, Iteration: 1284/10000 --- Training Loss:0.992591\n",
      "Epoch: 26/200, Iteration: 1286/10000 --- Training Loss:3.867558\n",
      "Epoch: 26/200, Iteration: 1288/10000 --- Training Loss:0.600938\n",
      "Epoch: 26/200, Iteration: 1290/10000 --- Training Loss:1.758756\n",
      "Epoch: 26/200, Iteration: 1292/10000 --- Training Loss:1.476173\n",
      "Epoch: 26/200, Iteration: 1294/10000 --- Training Loss:0.229249\n",
      "Epoch: 26/200, Iteration: 1296/10000 --- Training Loss:4.482782\n",
      "Epoch: 26/200, Iteration: 1298/10000 --- Training Loss:0.825694\n",
      "Epoch: 26/200, Iteration: 1300/10000 --- Training Loss:0.567477\n",
      "Epoch: 26 finished ! Train Loss: 1.16903, Test Loss: 9.21250\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 27/200, Iteration: 1302/10000 --- Training Loss:0.458988\n",
      "Epoch: 27/200, Iteration: 1304/10000 --- Training Loss:0.856106\n",
      "Epoch: 27/200, Iteration: 1306/10000 --- Training Loss:0.719445\n",
      "Epoch: 27/200, Iteration: 1308/10000 --- Training Loss:1.677877\n",
      "Epoch: 27/200, Iteration: 1310/10000 --- Training Loss:0.391972\n",
      "Epoch: 27/200, Iteration: 1312/10000 --- Training Loss:0.902622\n",
      "Epoch: 27/200, Iteration: 1314/10000 --- Training Loss:0.352201\n",
      "Epoch: 27/200, Iteration: 1316/10000 --- Training Loss:0.852893\n",
      "Epoch: 27/200, Iteration: 1318/10000 --- Training Loss:0.716478\n",
      "Epoch: 27/200, Iteration: 1320/10000 --- Training Loss:2.955208\n",
      "Epoch: 27/200, Iteration: 1322/10000 --- Training Loss:2.397759\n",
      "Epoch: 27/200, Iteration: 1324/10000 --- Training Loss:4.045233\n",
      "Epoch: 27/200, Iteration: 1326/10000 --- Training Loss:0.825313\n",
      "Epoch: 27/200, Iteration: 1328/10000 --- Training Loss:0.321161\n",
      "Epoch: 27/200, Iteration: 1330/10000 --- Training Loss:1.040536\n",
      "Epoch: 27/200, Iteration: 1332/10000 --- Training Loss:0.688695\n",
      "Epoch: 27/200, Iteration: 1334/10000 --- Training Loss:0.559282\n",
      "Epoch: 27/200, Iteration: 1336/10000 --- Training Loss:0.628535\n",
      "Epoch: 27/200, Iteration: 1338/10000 --- Training Loss:0.647813\n",
      "Epoch: 27/200, Iteration: 1340/10000 --- Training Loss:0.885072\n",
      "Epoch: 27/200, Iteration: 1342/10000 --- Training Loss:0.864068\n",
      "Epoch: 27/200, Iteration: 1344/10000 --- Training Loss:0.825041\n",
      "Epoch: 27/200, Iteration: 1346/10000 --- Training Loss:0.425217\n",
      "Epoch: 27/200, Iteration: 1348/10000 --- Training Loss:0.831190\n",
      "Epoch: 27/200, Iteration: 1350/10000 --- Training Loss:0.502350\n",
      "Epoch: 27 finished ! Train Loss: 1.12032, Test Loss: 9.49424\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 28/200, Iteration: 1352/10000 --- Training Loss:0.397283\n",
      "Epoch: 28/200, Iteration: 1354/10000 --- Training Loss:0.554467\n",
      "Epoch: 28/200, Iteration: 1356/10000 --- Training Loss:2.193259\n",
      "Epoch: 28/200, Iteration: 1358/10000 --- Training Loss:1.489153\n",
      "Epoch: 28/200, Iteration: 1360/10000 --- Training Loss:0.367335\n",
      "Epoch: 28/200, Iteration: 1362/10000 --- Training Loss:1.103754\n",
      "Epoch: 28/200, Iteration: 1364/10000 --- Training Loss:1.032704\n",
      "Epoch: 28/200, Iteration: 1366/10000 --- Training Loss:0.760483\n",
      "Epoch: 28/200, Iteration: 1368/10000 --- Training Loss:3.468288\n",
      "Epoch: 28/200, Iteration: 1370/10000 --- Training Loss:3.980782\n",
      "Epoch: 28/200, Iteration: 1372/10000 --- Training Loss:1.588180\n",
      "Epoch: 28/200, Iteration: 1374/10000 --- Training Loss:1.191929\n",
      "Epoch: 28/200, Iteration: 1376/10000 --- Training Loss:0.431373\n",
      "Epoch: 28/200, Iteration: 1378/10000 --- Training Loss:1.174258\n",
      "Epoch: 28/200, Iteration: 1380/10000 --- Training Loss:0.599653\n",
      "Epoch: 28/200, Iteration: 1382/10000 --- Training Loss:0.821233\n",
      "Epoch: 28/200, Iteration: 1384/10000 --- Training Loss:1.037724\n",
      "Epoch: 28/200, Iteration: 1386/10000 --- Training Loss:3.949273\n",
      "Epoch: 28/200, Iteration: 1388/10000 --- Training Loss:0.730549\n",
      "Epoch: 28/200, Iteration: 1390/10000 --- Training Loss:0.158391\n",
      "Epoch: 28/200, Iteration: 1392/10000 --- Training Loss:0.198865\n",
      "Epoch: 28/200, Iteration: 1394/10000 --- Training Loss:0.763265\n",
      "Epoch: 28/200, Iteration: 1396/10000 --- Training Loss:0.425126\n",
      "Epoch: 28/200, Iteration: 1398/10000 --- Training Loss:0.673807\n",
      "Epoch: 28/200, Iteration: 1400/10000 --- Training Loss:0.519062\n",
      "Epoch: 28 finished ! Train Loss: 1.13859, Test Loss: 8.55320\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 29/200, Iteration: 1402/10000 --- Training Loss:1.028311\n",
      "Epoch: 29/200, Iteration: 1404/10000 --- Training Loss:4.662604\n",
      "Epoch: 29/200, Iteration: 1406/10000 --- Training Loss:2.860079\n",
      "Epoch: 29/200, Iteration: 1408/10000 --- Training Loss:0.966659\n",
      "Epoch: 29/200, Iteration: 1410/10000 --- Training Loss:1.159662\n",
      "Epoch: 29/200, Iteration: 1412/10000 --- Training Loss:0.286193\n",
      "Epoch: 29/200, Iteration: 1414/10000 --- Training Loss:0.903338\n",
      "Epoch: 29/200, Iteration: 1416/10000 --- Training Loss:0.970070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/200, Iteration: 1418/10000 --- Training Loss:0.439305\n",
      "Epoch: 29/200, Iteration: 1420/10000 --- Training Loss:0.278216\n",
      "Epoch: 29/200, Iteration: 1422/10000 --- Training Loss:0.697217\n",
      "Epoch: 29/200, Iteration: 1424/10000 --- Training Loss:1.464557\n",
      "Epoch: 29/200, Iteration: 1426/10000 --- Training Loss:0.540410\n",
      "Epoch: 29/200, Iteration: 1428/10000 --- Training Loss:0.789671\n",
      "Epoch: 29/200, Iteration: 1430/10000 --- Training Loss:0.819979\n",
      "Epoch: 29/200, Iteration: 1432/10000 --- Training Loss:0.245553\n",
      "Epoch: 29/200, Iteration: 1434/10000 --- Training Loss:1.843583\n",
      "Epoch: 29/200, Iteration: 1436/10000 --- Training Loss:5.729922\n",
      "Epoch: 29/200, Iteration: 1438/10000 --- Training Loss:0.908425\n",
      "Epoch: 29/200, Iteration: 1440/10000 --- Training Loss:0.744121\n",
      "Epoch: 29/200, Iteration: 1442/10000 --- Training Loss:2.292285\n",
      "Epoch: 29/200, Iteration: 1444/10000 --- Training Loss:2.635939\n",
      "Epoch: 29/200, Iteration: 1446/10000 --- Training Loss:1.835811\n",
      "Epoch: 29/200, Iteration: 1448/10000 --- Training Loss:1.054630\n",
      "Epoch: 29/200, Iteration: 1450/10000 --- Training Loss:1.012168\n",
      "Epoch: 29 finished ! Train Loss: 1.13954, Test Loss: 9.24830\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 30/200, Iteration: 1452/10000 --- Training Loss:0.390310\n",
      "Epoch: 30/200, Iteration: 1454/10000 --- Training Loss:0.401560\n",
      "Epoch: 30/200, Iteration: 1456/10000 --- Training Loss:0.197715\n",
      "Epoch: 30/200, Iteration: 1458/10000 --- Training Loss:0.383244\n",
      "Epoch: 30/200, Iteration: 1460/10000 --- Training Loss:0.664729\n",
      "Epoch: 30/200, Iteration: 1462/10000 --- Training Loss:0.566054\n",
      "Epoch: 30/200, Iteration: 1464/10000 --- Training Loss:1.465323\n",
      "Epoch: 30/200, Iteration: 1466/10000 --- Training Loss:0.263608\n",
      "Epoch: 30/200, Iteration: 1468/10000 --- Training Loss:0.504958\n",
      "Epoch: 30/200, Iteration: 1470/10000 --- Training Loss:1.262506\n",
      "Epoch: 30/200, Iteration: 1472/10000 --- Training Loss:0.892712\n",
      "Epoch: 30/200, Iteration: 1474/10000 --- Training Loss:0.496525\n",
      "Epoch: 30/200, Iteration: 1476/10000 --- Training Loss:5.382633\n",
      "Epoch: 30/200, Iteration: 1478/10000 --- Training Loss:0.903908\n",
      "Epoch: 30/200, Iteration: 1480/10000 --- Training Loss:3.858075\n",
      "Epoch: 30/200, Iteration: 1482/10000 --- Training Loss:0.734900\n",
      "Epoch: 30/200, Iteration: 1484/10000 --- Training Loss:0.559624\n",
      "Epoch: 30/200, Iteration: 1486/10000 --- Training Loss:0.328896\n",
      "Epoch: 30/200, Iteration: 1488/10000 --- Training Loss:0.657988\n",
      "Epoch: 30/200, Iteration: 1490/10000 --- Training Loss:1.079046\n",
      "Epoch: 30/200, Iteration: 1492/10000 --- Training Loss:0.288118\n",
      "Epoch: 30/200, Iteration: 1494/10000 --- Training Loss:0.665068\n",
      "Epoch: 30/200, Iteration: 1496/10000 --- Training Loss:0.906348\n",
      "Epoch: 30/200, Iteration: 1498/10000 --- Training Loss:1.471867\n",
      "Epoch: 30/200, Iteration: 1500/10000 --- Training Loss:1.829713\n",
      "Epoch: 30 finished ! Train Loss: 1.08246, Test Loss: 8.24772\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 31/200, Iteration: 1502/10000 --- Training Loss:0.260413\n",
      "Epoch: 31/200, Iteration: 1504/10000 --- Training Loss:0.420507\n",
      "Epoch: 31/200, Iteration: 1506/10000 --- Training Loss:0.953315\n",
      "Epoch: 31/200, Iteration: 1508/10000 --- Training Loss:1.042535\n",
      "Epoch: 31/200, Iteration: 1510/10000 --- Training Loss:1.004044\n",
      "Epoch: 31/200, Iteration: 1512/10000 --- Training Loss:1.164899\n",
      "Epoch: 31/200, Iteration: 1514/10000 --- Training Loss:1.094020\n",
      "Epoch: 31/200, Iteration: 1516/10000 --- Training Loss:1.471876\n",
      "Epoch: 31/200, Iteration: 1518/10000 --- Training Loss:0.488608\n",
      "Epoch: 31/200, Iteration: 1520/10000 --- Training Loss:1.078752\n",
      "Epoch: 31/200, Iteration: 1522/10000 --- Training Loss:0.659167\n",
      "Epoch: 31/200, Iteration: 1524/10000 --- Training Loss:0.438963\n",
      "Epoch: 31/200, Iteration: 1526/10000 --- Training Loss:0.269538\n",
      "Epoch: 31/200, Iteration: 1528/10000 --- Training Loss:0.843332\n",
      "Epoch: 31/200, Iteration: 1530/10000 --- Training Loss:0.379158\n",
      "Epoch: 31/200, Iteration: 1532/10000 --- Training Loss:0.522980\n",
      "Epoch: 31/200, Iteration: 1534/10000 --- Training Loss:0.873016\n",
      "Epoch: 31/200, Iteration: 1536/10000 --- Training Loss:0.796729\n",
      "Epoch: 31/200, Iteration: 1538/10000 --- Training Loss:1.121751\n",
      "Epoch: 31/200, Iteration: 1540/10000 --- Training Loss:0.331985\n",
      "Epoch: 31/200, Iteration: 1542/10000 --- Training Loss:3.124307\n",
      "Epoch: 31/200, Iteration: 1544/10000 --- Training Loss:1.248043\n",
      "Epoch: 31/200, Iteration: 1546/10000 --- Training Loss:0.443308\n",
      "Epoch: 31/200, Iteration: 1548/10000 --- Training Loss:1.131158\n",
      "Epoch: 31/200, Iteration: 1550/10000 --- Training Loss:2.903642\n",
      "Epoch: 31 finished ! Train Loss: 1.05903, Test Loss: 8.22619\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 32/200, Iteration: 1552/10000 --- Training Loss:0.960202\n",
      "Epoch: 32/200, Iteration: 1554/10000 --- Training Loss:0.398183\n",
      "Epoch: 32/200, Iteration: 1556/10000 --- Training Loss:2.576434\n",
      "Epoch: 32/200, Iteration: 1558/10000 --- Training Loss:0.811185\n",
      "Epoch: 32/200, Iteration: 1560/10000 --- Training Loss:3.383844\n",
      "Epoch: 32/200, Iteration: 1562/10000 --- Training Loss:1.148626\n",
      "Epoch: 32/200, Iteration: 1564/10000 --- Training Loss:0.738506\n",
      "Epoch: 32/200, Iteration: 1566/10000 --- Training Loss:0.946037\n",
      "Epoch: 32/200, Iteration: 1568/10000 --- Training Loss:0.221605\n",
      "Epoch: 32/200, Iteration: 1570/10000 --- Training Loss:0.849323\n",
      "Epoch: 32/200, Iteration: 1572/10000 --- Training Loss:1.096377\n",
      "Epoch: 32/200, Iteration: 1574/10000 --- Training Loss:0.449910\n",
      "Epoch: 32/200, Iteration: 1576/10000 --- Training Loss:0.684223\n",
      "Epoch: 32/200, Iteration: 1578/10000 --- Training Loss:0.574991\n",
      "Epoch: 32/200, Iteration: 1580/10000 --- Training Loss:0.271502\n",
      "Epoch: 32/200, Iteration: 1582/10000 --- Training Loss:2.084594\n",
      "Epoch: 32/200, Iteration: 1584/10000 --- Training Loss:0.332228\n",
      "Epoch: 32/200, Iteration: 1586/10000 --- Training Loss:0.486922\n",
      "Epoch: 32/200, Iteration: 1588/10000 --- Training Loss:0.478398\n",
      "Epoch: 32/200, Iteration: 1590/10000 --- Training Loss:1.124395\n",
      "Epoch: 32/200, Iteration: 1592/10000 --- Training Loss:1.081591\n",
      "Epoch: 32/200, Iteration: 1594/10000 --- Training Loss:3.401752\n",
      "Epoch: 32/200, Iteration: 1596/10000 --- Training Loss:0.689146\n",
      "Epoch: 32/200, Iteration: 1598/10000 --- Training Loss:0.982085\n",
      "Epoch: 32/200, Iteration: 1600/10000 --- Training Loss:2.577533\n",
      "Epoch: 32 finished ! Train Loss: 1.07933, Test Loss: 8.00002\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 33/200, Iteration: 1602/10000 --- Training Loss:0.352758\n",
      "Epoch: 33/200, Iteration: 1604/10000 --- Training Loss:0.300577\n",
      "Epoch: 33/200, Iteration: 1606/10000 --- Training Loss:2.085725\n",
      "Epoch: 33/200, Iteration: 1608/10000 --- Training Loss:0.498753\n",
      "Epoch: 33/200, Iteration: 1610/10000 --- Training Loss:1.073191\n",
      "Epoch: 33/200, Iteration: 1612/10000 --- Training Loss:1.243817\n",
      "Epoch: 33/200, Iteration: 1614/10000 --- Training Loss:0.547817\n",
      "Epoch: 33/200, Iteration: 1616/10000 --- Training Loss:0.772565\n",
      "Epoch: 33/200, Iteration: 1618/10000 --- Training Loss:0.550699\n",
      "Epoch: 33/200, Iteration: 1620/10000 --- Training Loss:1.061379\n",
      "Epoch: 33/200, Iteration: 1622/10000 --- Training Loss:1.259003\n",
      "Epoch: 33/200, Iteration: 1624/10000 --- Training Loss:2.624094\n",
      "Epoch: 33/200, Iteration: 1626/10000 --- Training Loss:1.279086\n",
      "Epoch: 33/200, Iteration: 1628/10000 --- Training Loss:1.358442\n",
      "Epoch: 33/200, Iteration: 1630/10000 --- Training Loss:2.587555\n",
      "Epoch: 33/200, Iteration: 1632/10000 --- Training Loss:0.942211\n",
      "Epoch: 33/200, Iteration: 1634/10000 --- Training Loss:0.285930\n",
      "Epoch: 33/200, Iteration: 1636/10000 --- Training Loss:0.830241\n",
      "Epoch: 33/200, Iteration: 1638/10000 --- Training Loss:0.828624\n",
      "Epoch: 33/200, Iteration: 1640/10000 --- Training Loss:0.699271\n",
      "Epoch: 33/200, Iteration: 1642/10000 --- Training Loss:0.742448\n",
      "Epoch: 33/200, Iteration: 1644/10000 --- Training Loss:0.498898\n",
      "Epoch: 33/200, Iteration: 1646/10000 --- Training Loss:0.230725\n",
      "Epoch: 33/200, Iteration: 1648/10000 --- Training Loss:0.506406\n",
      "Epoch: 33/200, Iteration: 1650/10000 --- Training Loss:3.626947\n",
      "Epoch: 33 finished ! Train Loss: 1.07614, Test Loss: 8.08161\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 34/200, Iteration: 1652/10000 --- Training Loss:0.743638\n",
      "Epoch: 34/200, Iteration: 1654/10000 --- Training Loss:1.184455\n",
      "Epoch: 34/200, Iteration: 1656/10000 --- Training Loss:0.834983\n",
      "Epoch: 34/200, Iteration: 1658/10000 --- Training Loss:0.788079\n",
      "Epoch: 34/200, Iteration: 1660/10000 --- Training Loss:0.551744\n",
      "Epoch: 34/200, Iteration: 1662/10000 --- Training Loss:0.448716\n",
      "Epoch: 34/200, Iteration: 1664/10000 --- Training Loss:0.659691\n",
      "Epoch: 34/200, Iteration: 1666/10000 --- Training Loss:2.438107\n",
      "Epoch: 34/200, Iteration: 1668/10000 --- Training Loss:2.535962\n",
      "Epoch: 34/200, Iteration: 1670/10000 --- Training Loss:0.590175\n",
      "Epoch: 34/200, Iteration: 1672/10000 --- Training Loss:1.279656\n",
      "Epoch: 34/200, Iteration: 1674/10000 --- Training Loss:0.307053\n",
      "Epoch: 34/200, Iteration: 1676/10000 --- Training Loss:0.308832\n",
      "Epoch: 34/200, Iteration: 1678/10000 --- Training Loss:3.985039\n",
      "Epoch: 34/200, Iteration: 1680/10000 --- Training Loss:0.743104\n",
      "Epoch: 34/200, Iteration: 1682/10000 --- Training Loss:1.049819\n",
      "Epoch: 34/200, Iteration: 1684/10000 --- Training Loss:0.369684\n",
      "Epoch: 34/200, Iteration: 1686/10000 --- Training Loss:0.932219\n",
      "Epoch: 34/200, Iteration: 1688/10000 --- Training Loss:1.260269\n",
      "Epoch: 34/200, Iteration: 1690/10000 --- Training Loss:0.942750\n",
      "Epoch: 34/200, Iteration: 1692/10000 --- Training Loss:0.707205\n",
      "Epoch: 34/200, Iteration: 1694/10000 --- Training Loss:0.505490\n",
      "Epoch: 34/200, Iteration: 1696/10000 --- Training Loss:0.828987\n",
      "Epoch: 34/200, Iteration: 1698/10000 --- Training Loss:1.170458\n",
      "Epoch: 34/200, Iteration: 1700/10000 --- Training Loss:0.691593\n",
      "Epoch: 34 finished ! Train Loss: 1.02448, Test Loss: 8.54266\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 35/200, Iteration: 1702/10000 --- Training Loss:2.398173\n",
      "Epoch: 35/200, Iteration: 1704/10000 --- Training Loss:0.909555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35/200, Iteration: 1706/10000 --- Training Loss:0.216276\n",
      "Epoch: 35/200, Iteration: 1708/10000 --- Training Loss:0.242446\n",
      "Epoch: 35/200, Iteration: 1710/10000 --- Training Loss:0.667464\n",
      "Epoch: 35/200, Iteration: 1712/10000 --- Training Loss:0.356729\n",
      "Epoch: 35/200, Iteration: 1714/10000 --- Training Loss:0.786798\n",
      "Epoch: 35/200, Iteration: 1716/10000 --- Training Loss:1.029810\n",
      "Epoch: 35/200, Iteration: 1718/10000 --- Training Loss:2.485157\n",
      "Epoch: 35/200, Iteration: 1720/10000 --- Training Loss:0.564444\n",
      "Epoch: 35/200, Iteration: 1722/10000 --- Training Loss:2.028299\n",
      "Epoch: 35/200, Iteration: 1724/10000 --- Training Loss:1.095498\n",
      "Epoch: 35/200, Iteration: 1726/10000 --- Training Loss:0.055014\n",
      "Epoch: 35/200, Iteration: 1728/10000 --- Training Loss:1.110551\n",
      "Epoch: 35/200, Iteration: 1730/10000 --- Training Loss:1.308696\n",
      "Epoch: 35/200, Iteration: 1732/10000 --- Training Loss:3.436002\n",
      "Epoch: 35/200, Iteration: 1734/10000 --- Training Loss:0.392343\n",
      "Epoch: 35/200, Iteration: 1736/10000 --- Training Loss:0.695433\n",
      "Epoch: 35/200, Iteration: 1738/10000 --- Training Loss:1.097641\n",
      "Epoch: 35/200, Iteration: 1740/10000 --- Training Loss:0.260775\n",
      "Epoch: 35/200, Iteration: 1742/10000 --- Training Loss:0.239521\n",
      "Epoch: 35/200, Iteration: 1744/10000 --- Training Loss:3.085443\n",
      "Epoch: 35/200, Iteration: 1746/10000 --- Training Loss:0.613578\n",
      "Epoch: 35/200, Iteration: 1748/10000 --- Training Loss:0.661350\n",
      "Epoch: 35/200, Iteration: 1750/10000 --- Training Loss:1.363016\n",
      "Epoch: 35 finished ! Train Loss: 1.03823, Test Loss: 7.31860\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 36/200, Iteration: 1752/10000 --- Training Loss:0.234152\n",
      "Epoch: 36/200, Iteration: 1754/10000 --- Training Loss:0.972591\n",
      "Epoch: 36/200, Iteration: 1756/10000 --- Training Loss:0.128495\n",
      "Epoch: 36/200, Iteration: 1758/10000 --- Training Loss:0.550446\n",
      "Epoch: 36/200, Iteration: 1760/10000 --- Training Loss:0.630324\n",
      "Epoch: 36/200, Iteration: 1762/10000 --- Training Loss:0.446584\n",
      "Epoch: 36/200, Iteration: 1764/10000 --- Training Loss:1.123796\n",
      "Epoch: 36/200, Iteration: 1766/10000 --- Training Loss:1.277861\n",
      "Epoch: 36/200, Iteration: 1768/10000 --- Training Loss:1.166611\n",
      "Epoch: 36/200, Iteration: 1770/10000 --- Training Loss:0.805439\n",
      "Epoch: 36/200, Iteration: 1772/10000 --- Training Loss:1.542987\n",
      "Epoch: 36/200, Iteration: 1774/10000 --- Training Loss:1.104525\n",
      "Epoch: 36/200, Iteration: 1776/10000 --- Training Loss:0.924533\n",
      "Epoch: 36/200, Iteration: 1778/10000 --- Training Loss:0.635309\n",
      "Epoch: 36/200, Iteration: 1780/10000 --- Training Loss:2.266462\n",
      "Epoch: 36/200, Iteration: 1782/10000 --- Training Loss:0.295357\n",
      "Epoch: 36/200, Iteration: 1784/10000 --- Training Loss:0.975774\n",
      "Epoch: 36/200, Iteration: 1786/10000 --- Training Loss:0.726141\n",
      "Epoch: 36/200, Iteration: 1788/10000 --- Training Loss:0.982856\n",
      "Epoch: 36/200, Iteration: 1790/10000 --- Training Loss:2.218218\n",
      "Epoch: 36/200, Iteration: 1792/10000 --- Training Loss:0.473010\n",
      "Epoch: 36/200, Iteration: 1794/10000 --- Training Loss:0.513987\n",
      "Epoch: 36/200, Iteration: 1796/10000 --- Training Loss:0.324379\n",
      "Epoch: 36/200, Iteration: 1798/10000 --- Training Loss:0.819723\n",
      "Epoch: 36/200, Iteration: 1800/10000 --- Training Loss:0.800490\n",
      "Epoch: 36 finished ! Train Loss: 0.98257, Test Loss: 7.21430\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 37/200, Iteration: 1802/10000 --- Training Loss:0.951304\n",
      "Epoch: 37/200, Iteration: 1804/10000 --- Training Loss:0.475281\n",
      "Epoch: 37/200, Iteration: 1806/10000 --- Training Loss:0.988430\n",
      "Epoch: 37/200, Iteration: 1808/10000 --- Training Loss:0.963013\n",
      "Epoch: 37/200, Iteration: 1810/10000 --- Training Loss:0.540053\n",
      "Epoch: 37/200, Iteration: 1812/10000 --- Training Loss:0.957708\n",
      "Epoch: 37/200, Iteration: 1814/10000 --- Training Loss:0.166163\n",
      "Epoch: 37/200, Iteration: 1816/10000 --- Training Loss:0.998716\n",
      "Epoch: 37/200, Iteration: 1818/10000 --- Training Loss:1.086253\n",
      "Epoch: 37/200, Iteration: 1820/10000 --- Training Loss:0.265382\n",
      "Epoch: 37/200, Iteration: 1822/10000 --- Training Loss:2.735720\n",
      "Epoch: 37/200, Iteration: 1824/10000 --- Training Loss:0.555893\n",
      "Epoch: 37/200, Iteration: 1826/10000 --- Training Loss:0.865345\n",
      "Epoch: 37/200, Iteration: 1828/10000 --- Training Loss:1.471137\n",
      "Epoch: 37/200, Iteration: 1830/10000 --- Training Loss:0.854450\n",
      "Epoch: 37/200, Iteration: 1832/10000 --- Training Loss:0.420106\n",
      "Epoch: 37/200, Iteration: 1834/10000 --- Training Loss:0.767929\n",
      "Epoch: 37/200, Iteration: 1836/10000 --- Training Loss:0.760186\n",
      "Epoch: 37/200, Iteration: 1838/10000 --- Training Loss:0.496082\n",
      "Epoch: 37/200, Iteration: 1840/10000 --- Training Loss:0.497252\n",
      "Epoch: 37/200, Iteration: 1842/10000 --- Training Loss:0.835984\n",
      "Epoch: 37/200, Iteration: 1844/10000 --- Training Loss:0.635622\n",
      "Epoch: 37/200, Iteration: 1846/10000 --- Training Loss:0.520473\n",
      "Epoch: 37/200, Iteration: 1848/10000 --- Training Loss:0.910938\n",
      "Epoch: 37/200, Iteration: 1850/10000 --- Training Loss:3.083325\n",
      "Epoch: 37 finished ! Train Loss: 0.95015, Test Loss: 7.91195\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 38/200, Iteration: 1852/10000 --- Training Loss:1.795010\n",
      "Epoch: 38/200, Iteration: 1854/10000 --- Training Loss:1.131683\n",
      "Epoch: 38/200, Iteration: 1856/10000 --- Training Loss:0.371717\n",
      "Epoch: 38/200, Iteration: 1858/10000 --- Training Loss:0.771709\n",
      "Epoch: 38/200, Iteration: 1860/10000 --- Training Loss:0.245872\n",
      "Epoch: 38/200, Iteration: 1862/10000 --- Training Loss:0.729591\n",
      "Epoch: 38/200, Iteration: 1864/10000 --- Training Loss:0.657274\n",
      "Epoch: 38/200, Iteration: 1866/10000 --- Training Loss:0.424444\n",
      "Epoch: 38/200, Iteration: 1868/10000 --- Training Loss:0.943918\n",
      "Epoch: 38/200, Iteration: 1870/10000 --- Training Loss:0.143349\n",
      "Epoch: 38/200, Iteration: 1872/10000 --- Training Loss:1.967082\n",
      "Epoch: 38/200, Iteration: 1874/10000 --- Training Loss:0.473630\n",
      "Epoch: 38/200, Iteration: 1876/10000 --- Training Loss:0.825523\n",
      "Epoch: 38/200, Iteration: 1878/10000 --- Training Loss:0.978658\n",
      "Epoch: 38/200, Iteration: 1880/10000 --- Training Loss:1.460389\n",
      "Epoch: 38/200, Iteration: 1882/10000 --- Training Loss:3.309159\n",
      "Epoch: 38/200, Iteration: 1884/10000 --- Training Loss:0.434143\n",
      "Epoch: 38/200, Iteration: 1886/10000 --- Training Loss:1.052524\n",
      "Epoch: 38/200, Iteration: 1888/10000 --- Training Loss:1.939644\n",
      "Epoch: 38/200, Iteration: 1890/10000 --- Training Loss:0.519435\n",
      "Epoch: 38/200, Iteration: 1892/10000 --- Training Loss:0.689126\n",
      "Epoch: 38/200, Iteration: 1894/10000 --- Training Loss:0.564088\n",
      "Epoch: 38/200, Iteration: 1896/10000 --- Training Loss:1.298876\n",
      "Epoch: 38/200, Iteration: 1898/10000 --- Training Loss:0.373169\n",
      "Epoch: 38/200, Iteration: 1900/10000 --- Training Loss:3.486037\n",
      "Epoch: 38 finished ! Train Loss: 0.97496, Test Loss: 7.79836\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 39/200, Iteration: 1902/10000 --- Training Loss:0.968607\n",
      "Epoch: 39/200, Iteration: 1904/10000 --- Training Loss:0.169913\n",
      "Epoch: 39/200, Iteration: 1906/10000 --- Training Loss:0.708797\n",
      "Epoch: 39/200, Iteration: 1908/10000 --- Training Loss:1.551169\n",
      "Epoch: 39/200, Iteration: 1910/10000 --- Training Loss:2.626128\n",
      "Epoch: 39/200, Iteration: 1912/10000 --- Training Loss:2.060238\n",
      "Epoch: 39/200, Iteration: 1914/10000 --- Training Loss:1.268799\n",
      "Epoch: 39/200, Iteration: 1916/10000 --- Training Loss:0.826607\n",
      "Epoch: 39/200, Iteration: 1918/10000 --- Training Loss:1.057568\n",
      "Epoch: 39/200, Iteration: 1920/10000 --- Training Loss:0.729367\n",
      "Epoch: 39/200, Iteration: 1922/10000 --- Training Loss:0.226602\n",
      "Epoch: 39/200, Iteration: 1924/10000 --- Training Loss:0.649787\n",
      "Epoch: 39/200, Iteration: 1926/10000 --- Training Loss:0.990748\n",
      "Epoch: 39/200, Iteration: 1928/10000 --- Training Loss:0.565741\n",
      "Epoch: 39/200, Iteration: 1930/10000 --- Training Loss:0.339741\n",
      "Epoch: 39/200, Iteration: 1932/10000 --- Training Loss:0.837503\n",
      "Epoch: 39/200, Iteration: 1934/10000 --- Training Loss:1.073667\n",
      "Epoch: 39/200, Iteration: 1936/10000 --- Training Loss:0.410545\n",
      "Epoch: 39/200, Iteration: 1938/10000 --- Training Loss:0.498916\n",
      "Epoch: 39/200, Iteration: 1940/10000 --- Training Loss:0.575266\n",
      "Epoch: 39/200, Iteration: 1942/10000 --- Training Loss:1.423611\n",
      "Epoch: 39/200, Iteration: 1944/10000 --- Training Loss:0.543440\n",
      "Epoch: 39/200, Iteration: 1946/10000 --- Training Loss:0.424757\n",
      "Epoch: 39/200, Iteration: 1948/10000 --- Training Loss:0.825212\n",
      "Epoch: 39/200, Iteration: 1950/10000 --- Training Loss:0.950971\n",
      "Epoch: 39 finished ! Train Loss: 1.00484, Test Loss: 7.44761\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 40/200, Iteration: 1952/10000 --- Training Loss:1.549816\n",
      "Epoch: 40/200, Iteration: 1954/10000 --- Training Loss:0.275263\n",
      "Epoch: 40/200, Iteration: 1956/10000 --- Training Loss:0.878867\n",
      "Epoch: 40/200, Iteration: 1958/10000 --- Training Loss:0.636777\n",
      "Epoch: 40/200, Iteration: 1960/10000 --- Training Loss:0.586742\n",
      "Epoch: 40/200, Iteration: 1962/10000 --- Training Loss:0.939132\n",
      "Epoch: 40/200, Iteration: 1964/10000 --- Training Loss:0.696900\n",
      "Epoch: 40/200, Iteration: 1966/10000 --- Training Loss:1.059114\n",
      "Epoch: 40/200, Iteration: 1968/10000 --- Training Loss:0.722692\n",
      "Epoch: 40/200, Iteration: 1970/10000 --- Training Loss:0.687769\n",
      "Epoch: 40/200, Iteration: 1972/10000 --- Training Loss:0.188482\n",
      "Epoch: 40/200, Iteration: 1974/10000 --- Training Loss:0.454070\n",
      "Epoch: 40/200, Iteration: 1976/10000 --- Training Loss:0.695626\n",
      "Epoch: 40/200, Iteration: 1978/10000 --- Training Loss:0.473752\n",
      "Epoch: 40/200, Iteration: 1980/10000 --- Training Loss:0.972264\n",
      "Epoch: 40/200, Iteration: 1982/10000 --- Training Loss:0.439344\n",
      "Epoch: 40/200, Iteration: 1984/10000 --- Training Loss:1.758522\n",
      "Epoch: 40/200, Iteration: 1986/10000 --- Training Loss:1.341133\n",
      "Epoch: 40/200, Iteration: 1988/10000 --- Training Loss:0.682071\n",
      "Epoch: 40/200, Iteration: 1990/10000 --- Training Loss:0.251194\n",
      "Epoch: 40/200, Iteration: 1992/10000 --- Training Loss:0.148164\n",
      "Epoch: 40/200, Iteration: 1994/10000 --- Training Loss:0.608614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/200, Iteration: 1996/10000 --- Training Loss:0.447174\n",
      "Epoch: 40/200, Iteration: 1998/10000 --- Training Loss:0.435339\n",
      "Epoch: 40/200, Iteration: 2000/10000 --- Training Loss:0.178528\n",
      "Epoch: 40 finished ! Train Loss: 0.93514, Test Loss: 7.65530\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 41/200, Iteration: 2002/10000 --- Training Loss:0.630668\n",
      "Epoch: 41/200, Iteration: 2004/10000 --- Training Loss:0.896993\n",
      "Epoch: 41/200, Iteration: 2006/10000 --- Training Loss:0.724803\n",
      "Epoch: 41/200, Iteration: 2008/10000 --- Training Loss:0.596666\n",
      "Epoch: 41/200, Iteration: 2010/10000 --- Training Loss:0.698903\n",
      "Epoch: 41/200, Iteration: 2012/10000 --- Training Loss:0.998904\n",
      "Epoch: 41/200, Iteration: 2014/10000 --- Training Loss:0.966216\n",
      "Epoch: 41/200, Iteration: 2016/10000 --- Training Loss:0.669648\n",
      "Epoch: 41/200, Iteration: 2018/10000 --- Training Loss:0.759969\n",
      "Epoch: 41/200, Iteration: 2020/10000 --- Training Loss:0.980485\n",
      "Epoch: 41/200, Iteration: 2022/10000 --- Training Loss:0.289270\n",
      "Epoch: 41/200, Iteration: 2024/10000 --- Training Loss:0.693596\n",
      "Epoch: 41/200, Iteration: 2026/10000 --- Training Loss:0.619388\n",
      "Epoch: 41/200, Iteration: 2028/10000 --- Training Loss:0.920306\n",
      "Epoch: 41/200, Iteration: 2030/10000 --- Training Loss:0.953621\n",
      "Epoch: 41/200, Iteration: 2032/10000 --- Training Loss:2.628804\n",
      "Epoch: 41/200, Iteration: 2034/10000 --- Training Loss:0.437905\n",
      "Epoch: 41/200, Iteration: 2036/10000 --- Training Loss:0.385460\n",
      "Epoch: 41/200, Iteration: 2038/10000 --- Training Loss:0.733086\n",
      "Epoch: 41/200, Iteration: 2040/10000 --- Training Loss:0.254269\n",
      "Epoch: 41/200, Iteration: 2042/10000 --- Training Loss:0.234231\n",
      "Epoch: 41/200, Iteration: 2044/10000 --- Training Loss:0.188938\n",
      "Epoch: 41/200, Iteration: 2046/10000 --- Training Loss:0.766357\n",
      "Epoch: 41/200, Iteration: 2048/10000 --- Training Loss:2.078194\n",
      "Epoch: 41/200, Iteration: 2050/10000 --- Training Loss:2.856260\n",
      "Epoch: 41 finished ! Train Loss: 0.92932, Test Loss: 8.23915\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 42/200, Iteration: 2052/10000 --- Training Loss:0.597660\n",
      "Epoch: 42/200, Iteration: 2054/10000 --- Training Loss:0.117478\n",
      "Epoch: 42/200, Iteration: 2056/10000 --- Training Loss:0.478970\n",
      "Epoch: 42/200, Iteration: 2058/10000 --- Training Loss:2.343057\n",
      "Epoch: 42/200, Iteration: 2060/10000 --- Training Loss:0.489631\n",
      "Epoch: 42/200, Iteration: 2062/10000 --- Training Loss:0.768044\n",
      "Epoch: 42/200, Iteration: 2064/10000 --- Training Loss:0.347524\n",
      "Epoch: 42/200, Iteration: 2066/10000 --- Training Loss:1.230149\n",
      "Epoch: 42/200, Iteration: 2068/10000 --- Training Loss:0.378461\n",
      "Epoch: 42/200, Iteration: 2070/10000 --- Training Loss:0.975812\n",
      "Epoch: 42/200, Iteration: 2072/10000 --- Training Loss:1.949458\n",
      "Epoch: 42/200, Iteration: 2074/10000 --- Training Loss:0.299977\n",
      "Epoch: 42/200, Iteration: 2076/10000 --- Training Loss:0.785647\n",
      "Epoch: 42/200, Iteration: 2078/10000 --- Training Loss:0.481285\n",
      "Epoch: 42/200, Iteration: 2080/10000 --- Training Loss:3.278556\n",
      "Epoch: 42/200, Iteration: 2082/10000 --- Training Loss:0.658458\n",
      "Epoch: 42/200, Iteration: 2084/10000 --- Training Loss:0.979093\n",
      "Epoch: 42/200, Iteration: 2086/10000 --- Training Loss:2.807909\n",
      "Epoch: 42/200, Iteration: 2088/10000 --- Training Loss:0.966894\n",
      "Epoch: 42/200, Iteration: 2090/10000 --- Training Loss:1.337303\n",
      "Epoch: 42/200, Iteration: 2092/10000 --- Training Loss:0.654475\n",
      "Epoch: 42/200, Iteration: 2094/10000 --- Training Loss:0.342876\n",
      "Epoch: 42/200, Iteration: 2096/10000 --- Training Loss:0.233273\n",
      "Epoch: 42/200, Iteration: 2098/10000 --- Training Loss:0.589197\n",
      "Epoch: 42/200, Iteration: 2100/10000 --- Training Loss:1.842736\n",
      "Epoch: 42 finished ! Train Loss: 0.92738, Test Loss: 7.03897\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 43/200, Iteration: 2102/10000 --- Training Loss:0.788230\n",
      "Epoch: 43/200, Iteration: 2104/10000 --- Training Loss:0.885812\n",
      "Epoch: 43/200, Iteration: 2106/10000 --- Training Loss:0.588329\n",
      "Epoch: 43/200, Iteration: 2108/10000 --- Training Loss:0.993384\n",
      "Epoch: 43/200, Iteration: 2110/10000 --- Training Loss:0.581614\n",
      "Epoch: 43/200, Iteration: 2112/10000 --- Training Loss:1.636736\n",
      "Epoch: 43/200, Iteration: 2114/10000 --- Training Loss:0.584454\n",
      "Epoch: 43/200, Iteration: 2116/10000 --- Training Loss:0.618045\n",
      "Epoch: 43/200, Iteration: 2118/10000 --- Training Loss:0.591217\n",
      "Epoch: 43/200, Iteration: 2120/10000 --- Training Loss:0.514755\n",
      "Epoch: 43/200, Iteration: 2122/10000 --- Training Loss:0.139016\n",
      "Epoch: 43/200, Iteration: 2124/10000 --- Training Loss:1.341978\n",
      "Epoch: 43/200, Iteration: 2126/10000 --- Training Loss:0.535704\n",
      "Epoch: 43/200, Iteration: 2128/10000 --- Training Loss:0.680841\n",
      "Epoch: 43/200, Iteration: 2130/10000 --- Training Loss:0.826286\n",
      "Epoch: 43/200, Iteration: 2132/10000 --- Training Loss:0.200896\n",
      "Epoch: 43/200, Iteration: 2134/10000 --- Training Loss:0.510511\n",
      "Epoch: 43/200, Iteration: 2136/10000 --- Training Loss:0.733829\n",
      "Epoch: 43/200, Iteration: 2138/10000 --- Training Loss:0.722109\n",
      "Epoch: 43/200, Iteration: 2140/10000 --- Training Loss:3.242100\n",
      "Epoch: 43/200, Iteration: 2142/10000 --- Training Loss:0.438882\n",
      "Epoch: 43/200, Iteration: 2144/10000 --- Training Loss:0.839691\n",
      "Epoch: 43/200, Iteration: 2146/10000 --- Training Loss:0.156438\n",
      "Epoch: 43/200, Iteration: 2148/10000 --- Training Loss:0.500736\n",
      "Epoch: 43/200, Iteration: 2150/10000 --- Training Loss:3.043083\n",
      "Epoch: 43 finished ! Train Loss: 0.89606, Test Loss: 6.88708\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 44/200, Iteration: 2152/10000 --- Training Loss:0.829100\n",
      "Epoch: 44/200, Iteration: 2154/10000 --- Training Loss:1.037475\n",
      "Epoch: 44/200, Iteration: 2156/10000 --- Training Loss:1.123078\n",
      "Epoch: 44/200, Iteration: 2158/10000 --- Training Loss:0.703424\n",
      "Epoch: 44/200, Iteration: 2160/10000 --- Training Loss:0.371036\n",
      "Epoch: 44/200, Iteration: 2162/10000 --- Training Loss:0.330057\n",
      "Epoch: 44/200, Iteration: 2164/10000 --- Training Loss:0.415199\n",
      "Epoch: 44/200, Iteration: 2166/10000 --- Training Loss:2.745944\n",
      "Epoch: 44/200, Iteration: 2168/10000 --- Training Loss:1.464422\n",
      "Epoch: 44/200, Iteration: 2170/10000 --- Training Loss:0.911631\n",
      "Epoch: 44/200, Iteration: 2172/10000 --- Training Loss:1.720631\n",
      "Epoch: 44/200, Iteration: 2174/10000 --- Training Loss:1.074401\n",
      "Epoch: 44/200, Iteration: 2176/10000 --- Training Loss:0.636399\n",
      "Epoch: 44/200, Iteration: 2178/10000 --- Training Loss:0.591943\n",
      "Epoch: 44/200, Iteration: 2180/10000 --- Training Loss:0.579005\n",
      "Epoch: 44/200, Iteration: 2182/10000 --- Training Loss:1.206313\n",
      "Epoch: 44/200, Iteration: 2184/10000 --- Training Loss:1.265274\n",
      "Epoch: 44/200, Iteration: 2186/10000 --- Training Loss:1.082678\n",
      "Epoch: 44/200, Iteration: 2188/10000 --- Training Loss:0.915400\n",
      "Epoch: 44/200, Iteration: 2190/10000 --- Training Loss:2.701341\n",
      "Epoch: 44/200, Iteration: 2192/10000 --- Training Loss:0.394346\n",
      "Epoch: 44/200, Iteration: 2194/10000 --- Training Loss:0.076244\n",
      "Epoch: 44/200, Iteration: 2196/10000 --- Training Loss:0.784367\n",
      "Epoch: 44/200, Iteration: 2198/10000 --- Training Loss:0.571531\n",
      "Epoch: 44/200, Iteration: 2200/10000 --- Training Loss:0.398053\n",
      "Epoch: 44 finished ! Train Loss: 0.91557, Test Loss: 6.89554\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 45/200, Iteration: 2202/10000 --- Training Loss:0.686401\n",
      "Epoch: 45/200, Iteration: 2204/10000 --- Training Loss:0.876760\n",
      "Epoch: 45/200, Iteration: 2206/10000 --- Training Loss:0.558781\n",
      "Epoch: 45/200, Iteration: 2208/10000 --- Training Loss:0.817915\n",
      "Epoch: 45/200, Iteration: 2210/10000 --- Training Loss:0.507699\n",
      "Epoch: 45/200, Iteration: 2212/10000 --- Training Loss:0.826466\n",
      "Epoch: 45/200, Iteration: 2214/10000 --- Training Loss:0.479362\n",
      "Epoch: 45/200, Iteration: 2216/10000 --- Training Loss:1.942517\n",
      "Epoch: 45/200, Iteration: 2218/10000 --- Training Loss:0.630984\n",
      "Epoch: 45/200, Iteration: 2220/10000 --- Training Loss:2.944776\n",
      "Epoch: 45/200, Iteration: 2222/10000 --- Training Loss:0.309923\n",
      "Epoch: 45/200, Iteration: 2224/10000 --- Training Loss:0.575473\n",
      "Epoch: 45/200, Iteration: 2226/10000 --- Training Loss:0.817139\n",
      "Epoch: 45/200, Iteration: 2228/10000 --- Training Loss:0.934379\n",
      "Epoch: 45/200, Iteration: 2230/10000 --- Training Loss:0.283027\n",
      "Epoch: 45/200, Iteration: 2232/10000 --- Training Loss:0.659499\n",
      "Epoch: 45/200, Iteration: 2234/10000 --- Training Loss:0.889566\n",
      "Epoch: 45/200, Iteration: 2236/10000 --- Training Loss:1.484877\n",
      "Epoch: 45/200, Iteration: 2238/10000 --- Training Loss:0.924323\n",
      "Epoch: 45/200, Iteration: 2240/10000 --- Training Loss:0.716998\n",
      "Epoch: 45/200, Iteration: 2242/10000 --- Training Loss:0.668945\n",
      "Epoch: 45/200, Iteration: 2244/10000 --- Training Loss:1.948254\n",
      "Epoch: 45/200, Iteration: 2246/10000 --- Training Loss:0.554681\n",
      "Epoch: 45/200, Iteration: 2248/10000 --- Training Loss:0.239875\n",
      "Epoch: 45/200, Iteration: 2250/10000 --- Training Loss:0.902039\n",
      "Epoch: 45 finished ! Train Loss: 0.89904, Test Loss: 6.62254\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 46/200, Iteration: 2252/10000 --- Training Loss:0.463431\n",
      "Epoch: 46/200, Iteration: 2254/10000 --- Training Loss:0.973796\n",
      "Epoch: 46/200, Iteration: 2256/10000 --- Training Loss:0.457049\n",
      "Epoch: 46/200, Iteration: 2258/10000 --- Training Loss:0.272098\n",
      "Epoch: 46/200, Iteration: 2260/10000 --- Training Loss:0.359977\n",
      "Epoch: 46/200, Iteration: 2262/10000 --- Training Loss:0.287994\n",
      "Epoch: 46/200, Iteration: 2264/10000 --- Training Loss:0.523283\n",
      "Epoch: 46/200, Iteration: 2266/10000 --- Training Loss:0.753395\n",
      "Epoch: 46/200, Iteration: 2268/10000 --- Training Loss:0.561982\n",
      "Epoch: 46/200, Iteration: 2270/10000 --- Training Loss:1.049795\n",
      "Epoch: 46/200, Iteration: 2272/10000 --- Training Loss:2.219748\n",
      "Epoch: 46/200, Iteration: 2274/10000 --- Training Loss:0.739861\n",
      "Epoch: 46/200, Iteration: 2276/10000 --- Training Loss:0.494757\n",
      "Epoch: 46/200, Iteration: 2278/10000 --- Training Loss:0.539220\n",
      "Epoch: 46/200, Iteration: 2280/10000 --- Training Loss:0.415133\n",
      "Epoch: 46/200, Iteration: 2282/10000 --- Training Loss:1.118058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46/200, Iteration: 2284/10000 --- Training Loss:0.741030\n",
      "Epoch: 46/200, Iteration: 2286/10000 --- Training Loss:0.706810\n",
      "Epoch: 46/200, Iteration: 2288/10000 --- Training Loss:1.155169\n",
      "Epoch: 46/200, Iteration: 2290/10000 --- Training Loss:1.011392\n",
      "Epoch: 46/200, Iteration: 2292/10000 --- Training Loss:1.171492\n",
      "Epoch: 46/200, Iteration: 2294/10000 --- Training Loss:1.209000\n",
      "Epoch: 46/200, Iteration: 2296/10000 --- Training Loss:0.338816\n",
      "Epoch: 46/200, Iteration: 2298/10000 --- Training Loss:3.531137\n",
      "Epoch: 46/200, Iteration: 2300/10000 --- Training Loss:1.416410\n",
      "Epoch: 46 finished ! Train Loss: 0.88923, Test Loss: 7.44393\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 47/200, Iteration: 2302/10000 --- Training Loss:1.041538\n",
      "Epoch: 47/200, Iteration: 2304/10000 --- Training Loss:0.443503\n",
      "Epoch: 47/200, Iteration: 2306/10000 --- Training Loss:2.019591\n",
      "Epoch: 47/200, Iteration: 2308/10000 --- Training Loss:0.554866\n",
      "Epoch: 47/200, Iteration: 2310/10000 --- Training Loss:0.155683\n",
      "Epoch: 47/200, Iteration: 2312/10000 --- Training Loss:0.745753\n",
      "Epoch: 47/200, Iteration: 2314/10000 --- Training Loss:0.333482\n",
      "Epoch: 47/200, Iteration: 2316/10000 --- Training Loss:0.457719\n",
      "Epoch: 47/200, Iteration: 2318/10000 --- Training Loss:0.771226\n",
      "Epoch: 47/200, Iteration: 2320/10000 --- Training Loss:0.315681\n",
      "Epoch: 47/200, Iteration: 2322/10000 --- Training Loss:0.116186\n",
      "Epoch: 47/200, Iteration: 2324/10000 --- Training Loss:2.040598\n",
      "Epoch: 47/200, Iteration: 2326/10000 --- Training Loss:0.658518\n",
      "Epoch: 47/200, Iteration: 2328/10000 --- Training Loss:0.324407\n",
      "Epoch: 47/200, Iteration: 2330/10000 --- Training Loss:0.725190\n",
      "Epoch: 47/200, Iteration: 2332/10000 --- Training Loss:0.728979\n",
      "Epoch: 47/200, Iteration: 2334/10000 --- Training Loss:0.715614\n",
      "Epoch: 47/200, Iteration: 2336/10000 --- Training Loss:0.420076\n",
      "Epoch: 47/200, Iteration: 2338/10000 --- Training Loss:0.947391\n",
      "Epoch: 47/200, Iteration: 2340/10000 --- Training Loss:0.909310\n",
      "Epoch: 47/200, Iteration: 2342/10000 --- Training Loss:0.890765\n",
      "Epoch: 47/200, Iteration: 2344/10000 --- Training Loss:2.842993\n",
      "Epoch: 47/200, Iteration: 2346/10000 --- Training Loss:0.794358\n",
      "Epoch: 47/200, Iteration: 2348/10000 --- Training Loss:0.158329\n",
      "Epoch: 47/200, Iteration: 2350/10000 --- Training Loss:0.643349\n",
      "Epoch: 47 finished ! Train Loss: 0.87105, Test Loss: 7.50337\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 48/200, Iteration: 2352/10000 --- Training Loss:0.691179\n",
      "Epoch: 48/200, Iteration: 2354/10000 --- Training Loss:1.207029\n",
      "Epoch: 48/200, Iteration: 2356/10000 --- Training Loss:0.746941\n",
      "Epoch: 48/200, Iteration: 2358/10000 --- Training Loss:1.791122\n",
      "Epoch: 48/200, Iteration: 2360/10000 --- Training Loss:0.716192\n",
      "Epoch: 48/200, Iteration: 2362/10000 --- Training Loss:0.210759\n",
      "Epoch: 48/200, Iteration: 2364/10000 --- Training Loss:0.780612\n",
      "Epoch: 48/200, Iteration: 2366/10000 --- Training Loss:0.581219\n",
      "Epoch: 48/200, Iteration: 2368/10000 --- Training Loss:0.346684\n",
      "Epoch: 48/200, Iteration: 2370/10000 --- Training Loss:1.301897\n",
      "Epoch: 48/200, Iteration: 2372/10000 --- Training Loss:0.205650\n",
      "Epoch: 48/200, Iteration: 2374/10000 --- Training Loss:0.805847\n",
      "Epoch: 48/200, Iteration: 2376/10000 --- Training Loss:0.669994\n",
      "Epoch: 48/200, Iteration: 2378/10000 --- Training Loss:1.022860\n",
      "Epoch: 48/200, Iteration: 2380/10000 --- Training Loss:0.630078\n",
      "Epoch: 48/200, Iteration: 2382/10000 --- Training Loss:1.554450\n",
      "Epoch: 48/200, Iteration: 2384/10000 --- Training Loss:0.952195\n",
      "Epoch: 48/200, Iteration: 2386/10000 --- Training Loss:2.912899\n",
      "Epoch: 48/200, Iteration: 2388/10000 --- Training Loss:0.633386\n",
      "Epoch: 48/200, Iteration: 2390/10000 --- Training Loss:0.502139\n",
      "Epoch: 48/200, Iteration: 2392/10000 --- Training Loss:2.305179\n",
      "Epoch: 48/200, Iteration: 2394/10000 --- Training Loss:0.927594\n",
      "Epoch: 48/200, Iteration: 2396/10000 --- Training Loss:0.650291\n",
      "Epoch: 48/200, Iteration: 2398/10000 --- Training Loss:1.071552\n",
      "Epoch: 48/200, Iteration: 2400/10000 --- Training Loss:3.323342\n",
      "Epoch: 48 finished ! Train Loss: 0.83279, Test Loss: 6.86624\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 49/200, Iteration: 2402/10000 --- Training Loss:0.384867\n",
      "Epoch: 49/200, Iteration: 2404/10000 --- Training Loss:1.398151\n",
      "Epoch: 49/200, Iteration: 2406/10000 --- Training Loss:0.648060\n",
      "Epoch: 49/200, Iteration: 2408/10000 --- Training Loss:0.274012\n",
      "Epoch: 49/200, Iteration: 2410/10000 --- Training Loss:0.846444\n",
      "Epoch: 49/200, Iteration: 2412/10000 --- Training Loss:0.356386\n",
      "Epoch: 49/200, Iteration: 2414/10000 --- Training Loss:0.704223\n",
      "Epoch: 49/200, Iteration: 2416/10000 --- Training Loss:1.443581\n",
      "Epoch: 49/200, Iteration: 2418/10000 --- Training Loss:0.401494\n",
      "Epoch: 49/200, Iteration: 2420/10000 --- Training Loss:0.295129\n",
      "Epoch: 49/200, Iteration: 2422/10000 --- Training Loss:0.728606\n",
      "Epoch: 49/200, Iteration: 2424/10000 --- Training Loss:0.661305\n",
      "Epoch: 49/200, Iteration: 2426/10000 --- Training Loss:0.561517\n",
      "Epoch: 49/200, Iteration: 2428/10000 --- Training Loss:0.300916\n",
      "Epoch: 49/200, Iteration: 2430/10000 --- Training Loss:0.628696\n",
      "Epoch: 49/200, Iteration: 2432/10000 --- Training Loss:1.990321\n",
      "Epoch: 49/200, Iteration: 2434/10000 --- Training Loss:0.720077\n",
      "Epoch: 49/200, Iteration: 2436/10000 --- Training Loss:0.701730\n",
      "Epoch: 49/200, Iteration: 2438/10000 --- Training Loss:2.209077\n",
      "Epoch: 49/200, Iteration: 2440/10000 --- Training Loss:1.348102\n",
      "Epoch: 49/200, Iteration: 2442/10000 --- Training Loss:0.924514\n",
      "Epoch: 49/200, Iteration: 2444/10000 --- Training Loss:1.851431\n",
      "Epoch: 49/200, Iteration: 2446/10000 --- Training Loss:2.260020\n",
      "Epoch: 49/200, Iteration: 2448/10000 --- Training Loss:0.966842\n",
      "Epoch: 49/200, Iteration: 2450/10000 --- Training Loss:0.512117\n",
      "Epoch: 49 finished ! Train Loss: 0.91367, Test Loss: 6.62606\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 50/200, Iteration: 2452/10000 --- Training Loss:0.441870\n",
      "Epoch: 50/200, Iteration: 2454/10000 --- Training Loss:0.535636\n",
      "Epoch: 50/200, Iteration: 2456/10000 --- Training Loss:0.637360\n",
      "Epoch: 50/200, Iteration: 2458/10000 --- Training Loss:0.536758\n",
      "Epoch: 50/200, Iteration: 2460/10000 --- Training Loss:0.601115\n",
      "Epoch: 50/200, Iteration: 2462/10000 --- Training Loss:0.266875\n",
      "Epoch: 50/200, Iteration: 2464/10000 --- Training Loss:0.585384\n",
      "Epoch: 50/200, Iteration: 2466/10000 --- Training Loss:0.693255\n",
      "Epoch: 50/200, Iteration: 2468/10000 --- Training Loss:0.553166\n",
      "Epoch: 50/200, Iteration: 2470/10000 --- Training Loss:2.594817\n",
      "Epoch: 50/200, Iteration: 2472/10000 --- Training Loss:0.565790\n",
      "Epoch: 50/200, Iteration: 2474/10000 --- Training Loss:0.936720\n",
      "Epoch: 50/200, Iteration: 2476/10000 --- Training Loss:0.677851\n",
      "Epoch: 50/200, Iteration: 2478/10000 --- Training Loss:1.246014\n",
      "Epoch: 50/200, Iteration: 2480/10000 --- Training Loss:0.463410\n",
      "Epoch: 50/200, Iteration: 2482/10000 --- Training Loss:2.169724\n",
      "Epoch: 50/200, Iteration: 2484/10000 --- Training Loss:1.054335\n",
      "Epoch: 50/200, Iteration: 2486/10000 --- Training Loss:1.251591\n",
      "Epoch: 50/200, Iteration: 2488/10000 --- Training Loss:1.015554\n",
      "Epoch: 50/200, Iteration: 2490/10000 --- Training Loss:0.523173\n",
      "Epoch: 50/200, Iteration: 2492/10000 --- Training Loss:1.008300\n",
      "Epoch: 50/200, Iteration: 2494/10000 --- Training Loss:0.377172\n",
      "Epoch: 50/200, Iteration: 2496/10000 --- Training Loss:1.009215\n",
      "Epoch: 50/200, Iteration: 2498/10000 --- Training Loss:1.045909\n",
      "Epoch: 50/200, Iteration: 2500/10000 --- Training Loss:1.196665\n",
      "Epoch: 50 finished ! Train Loss: 0.90608, Test Loss: 7.04077\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 51/200, Iteration: 2502/10000 --- Training Loss:0.739243\n",
      "Epoch: 51/200, Iteration: 2504/10000 --- Training Loss:1.631161\n",
      "Epoch: 51/200, Iteration: 2506/10000 --- Training Loss:0.726246\n",
      "Epoch: 51/200, Iteration: 2508/10000 --- Training Loss:0.556811\n",
      "Epoch: 51/200, Iteration: 2510/10000 --- Training Loss:0.810859\n",
      "Epoch: 51/200, Iteration: 2512/10000 --- Training Loss:1.200803\n",
      "Epoch: 51/200, Iteration: 2514/10000 --- Training Loss:0.955973\n",
      "Epoch: 51/200, Iteration: 2516/10000 --- Training Loss:0.833624\n",
      "Epoch: 51/200, Iteration: 2518/10000 --- Training Loss:0.436257\n",
      "Epoch: 51/200, Iteration: 2520/10000 --- Training Loss:0.224439\n",
      "Epoch: 51/200, Iteration: 2522/10000 --- Training Loss:0.487821\n",
      "Epoch: 51/200, Iteration: 2524/10000 --- Training Loss:0.469411\n",
      "Epoch: 51/200, Iteration: 2526/10000 --- Training Loss:0.394661\n",
      "Epoch: 51/200, Iteration: 2528/10000 --- Training Loss:1.870982\n",
      "Epoch: 51/200, Iteration: 2530/10000 --- Training Loss:0.818239\n",
      "Epoch: 51/200, Iteration: 2532/10000 --- Training Loss:1.366413\n",
      "Epoch: 51/200, Iteration: 2534/10000 --- Training Loss:0.583462\n",
      "Epoch: 51/200, Iteration: 2536/10000 --- Training Loss:0.314586\n",
      "Epoch: 51/200, Iteration: 2538/10000 --- Training Loss:0.653496\n",
      "Epoch: 51/200, Iteration: 2540/10000 --- Training Loss:2.612505\n",
      "Epoch: 51/200, Iteration: 2542/10000 --- Training Loss:0.777968\n",
      "Epoch: 51/200, Iteration: 2544/10000 --- Training Loss:0.548714\n",
      "Epoch: 51/200, Iteration: 2546/10000 --- Training Loss:1.904527\n",
      "Epoch: 51/200, Iteration: 2548/10000 --- Training Loss:0.430461\n",
      "Epoch: 51/200, Iteration: 2550/10000 --- Training Loss:0.719956\n",
      "Epoch: 51 finished ! Train Loss: 0.87364, Test Loss: 5.62559\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 52/200, Iteration: 2552/10000 --- Training Loss:0.775753\n",
      "Epoch: 52/200, Iteration: 2554/10000 --- Training Loss:3.506683\n",
      "Epoch: 52/200, Iteration: 2556/10000 --- Training Loss:1.338109\n",
      "Epoch: 52/200, Iteration: 2558/10000 --- Training Loss:0.624170\n",
      "Epoch: 52/200, Iteration: 2560/10000 --- Training Loss:0.618371\n",
      "Epoch: 52/200, Iteration: 2562/10000 --- Training Loss:0.142164\n",
      "Epoch: 52/200, Iteration: 2564/10000 --- Training Loss:1.921440\n",
      "Epoch: 52/200, Iteration: 2566/10000 --- Training Loss:0.713288\n",
      "Epoch: 52/200, Iteration: 2568/10000 --- Training Loss:0.757104\n",
      "Epoch: 52/200, Iteration: 2570/10000 --- Training Loss:0.398800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52/200, Iteration: 2572/10000 --- Training Loss:0.281717\n",
      "Epoch: 52/200, Iteration: 2574/10000 --- Training Loss:0.754451\n",
      "Epoch: 52/200, Iteration: 2576/10000 --- Training Loss:0.370650\n",
      "Epoch: 52/200, Iteration: 2578/10000 --- Training Loss:2.079195\n",
      "Epoch: 52/200, Iteration: 2580/10000 --- Training Loss:1.857010\n",
      "Epoch: 52/200, Iteration: 2582/10000 --- Training Loss:0.995958\n",
      "Epoch: 52/200, Iteration: 2584/10000 --- Training Loss:1.608513\n",
      "Epoch: 52/200, Iteration: 2586/10000 --- Training Loss:0.686754\n",
      "Epoch: 52/200, Iteration: 2588/10000 --- Training Loss:0.442314\n",
      "Epoch: 52/200, Iteration: 2590/10000 --- Training Loss:1.153511\n",
      "Epoch: 52/200, Iteration: 2592/10000 --- Training Loss:0.760527\n",
      "Epoch: 52/200, Iteration: 2594/10000 --- Training Loss:0.406467\n",
      "Epoch: 52/200, Iteration: 2596/10000 --- Training Loss:1.386119\n",
      "Epoch: 52/200, Iteration: 2598/10000 --- Training Loss:0.894241\n",
      "Epoch: 52/200, Iteration: 2600/10000 --- Training Loss:0.973285\n",
      "Epoch: 52 finished ! Train Loss: 0.86250, Test Loss: 5.62537\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 53/200, Iteration: 2602/10000 --- Training Loss:0.781686\n",
      "Epoch: 53/200, Iteration: 2604/10000 --- Training Loss:0.707079\n",
      "Epoch: 53/200, Iteration: 2606/10000 --- Training Loss:0.715137\n",
      "Epoch: 53/200, Iteration: 2608/10000 --- Training Loss:0.525650\n",
      "Epoch: 53/200, Iteration: 2610/10000 --- Training Loss:0.772059\n",
      "Epoch: 53/200, Iteration: 2612/10000 --- Training Loss:0.787442\n",
      "Epoch: 53/200, Iteration: 2614/10000 --- Training Loss:0.453555\n",
      "Epoch: 53/200, Iteration: 2616/10000 --- Training Loss:0.367464\n",
      "Epoch: 53/200, Iteration: 2618/10000 --- Training Loss:1.506694\n",
      "Epoch: 53/200, Iteration: 2620/10000 --- Training Loss:0.457185\n",
      "Epoch: 53/200, Iteration: 2622/10000 --- Training Loss:0.436791\n",
      "Epoch: 53/200, Iteration: 2624/10000 --- Training Loss:0.759920\n",
      "Epoch: 53/200, Iteration: 2626/10000 --- Training Loss:0.211217\n",
      "Epoch: 53/200, Iteration: 2628/10000 --- Training Loss:0.635746\n",
      "Epoch: 53/200, Iteration: 2630/10000 --- Training Loss:0.452997\n",
      "Epoch: 53/200, Iteration: 2632/10000 --- Training Loss:3.802912\n",
      "Epoch: 53/200, Iteration: 2634/10000 --- Training Loss:0.341986\n",
      "Epoch: 53/200, Iteration: 2636/10000 --- Training Loss:1.082835\n",
      "Epoch: 53/200, Iteration: 2638/10000 --- Training Loss:0.980545\n",
      "Epoch: 53/200, Iteration: 2640/10000 --- Training Loss:0.220364\n",
      "Epoch: 53/200, Iteration: 2642/10000 --- Training Loss:0.169073\n",
      "Epoch: 53/200, Iteration: 2644/10000 --- Training Loss:0.887289\n",
      "Epoch: 53/200, Iteration: 2646/10000 --- Training Loss:0.602465\n",
      "Epoch: 53/200, Iteration: 2648/10000 --- Training Loss:0.259271\n",
      "Epoch: 53/200, Iteration: 2650/10000 --- Training Loss:0.865821\n",
      "Epoch: 53 finished ! Train Loss: 0.84179, Test Loss: 7.35436\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 54/200, Iteration: 2652/10000 --- Training Loss:1.631204\n",
      "Epoch: 54/200, Iteration: 2654/10000 --- Training Loss:0.659894\n",
      "Epoch: 54/200, Iteration: 2656/10000 --- Training Loss:2.959484\n",
      "Epoch: 54/200, Iteration: 2658/10000 --- Training Loss:0.652025\n",
      "Epoch: 54/200, Iteration: 2660/10000 --- Training Loss:0.773489\n",
      "Epoch: 54/200, Iteration: 2662/10000 --- Training Loss:0.793924\n",
      "Epoch: 54/200, Iteration: 2664/10000 --- Training Loss:0.502582\n",
      "Epoch: 54/200, Iteration: 2666/10000 --- Training Loss:0.530946\n",
      "Epoch: 54/200, Iteration: 2668/10000 --- Training Loss:0.677124\n",
      "Epoch: 54/200, Iteration: 2670/10000 --- Training Loss:0.328044\n",
      "Epoch: 54/200, Iteration: 2672/10000 --- Training Loss:0.357352\n",
      "Epoch: 54/200, Iteration: 2674/10000 --- Training Loss:0.683088\n",
      "Epoch: 54/200, Iteration: 2676/10000 --- Training Loss:0.560581\n",
      "Epoch: 54/200, Iteration: 2678/10000 --- Training Loss:0.910845\n",
      "Epoch: 54/200, Iteration: 2680/10000 --- Training Loss:1.517430\n",
      "Epoch: 54/200, Iteration: 2682/10000 --- Training Loss:0.582460\n",
      "Epoch: 54/200, Iteration: 2684/10000 --- Training Loss:0.811134\n",
      "Epoch: 54/200, Iteration: 2686/10000 --- Training Loss:1.012554\n",
      "Epoch: 54/200, Iteration: 2688/10000 --- Training Loss:1.238075\n",
      "Epoch: 54/200, Iteration: 2690/10000 --- Training Loss:0.546931\n",
      "Epoch: 54/200, Iteration: 2692/10000 --- Training Loss:0.245352\n",
      "Epoch: 54/200, Iteration: 2694/10000 --- Training Loss:0.760896\n",
      "Epoch: 54/200, Iteration: 2696/10000 --- Training Loss:0.856463\n",
      "Epoch: 54/200, Iteration: 2698/10000 --- Training Loss:0.732788\n",
      "Epoch: 54/200, Iteration: 2700/10000 --- Training Loss:1.643217\n",
      "Epoch: 54 finished ! Train Loss: 0.81225, Test Loss: 5.96092\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 55/200, Iteration: 2702/10000 --- Training Loss:0.729627\n",
      "Epoch: 55/200, Iteration: 2704/10000 --- Training Loss:0.761108\n",
      "Epoch: 55/200, Iteration: 2706/10000 --- Training Loss:1.429382\n",
      "Epoch: 55/200, Iteration: 2708/10000 --- Training Loss:0.513839\n",
      "Epoch: 55/200, Iteration: 2710/10000 --- Training Loss:2.352532\n",
      "Epoch: 55/200, Iteration: 2712/10000 --- Training Loss:0.550410\n",
      "Epoch: 55/200, Iteration: 2714/10000 --- Training Loss:0.626278\n",
      "Epoch: 55/200, Iteration: 2716/10000 --- Training Loss:0.300601\n",
      "Epoch: 55/200, Iteration: 2718/10000 --- Training Loss:0.541259\n",
      "Epoch: 55/200, Iteration: 2720/10000 --- Training Loss:0.127927\n",
      "Epoch: 55/200, Iteration: 2722/10000 --- Training Loss:0.986231\n",
      "Epoch: 55/200, Iteration: 2724/10000 --- Training Loss:3.198244\n",
      "Epoch: 55/200, Iteration: 2726/10000 --- Training Loss:1.398806\n",
      "Epoch: 55/200, Iteration: 2728/10000 --- Training Loss:0.629186\n",
      "Epoch: 55/200, Iteration: 2730/10000 --- Training Loss:0.250353\n",
      "Epoch: 55/200, Iteration: 2732/10000 --- Training Loss:0.884718\n",
      "Epoch: 55/200, Iteration: 2734/10000 --- Training Loss:0.880456\n",
      "Epoch: 55/200, Iteration: 2736/10000 --- Training Loss:0.769811\n",
      "Epoch: 55/200, Iteration: 2738/10000 --- Training Loss:0.941474\n",
      "Epoch: 55/200, Iteration: 2740/10000 --- Training Loss:0.686467\n",
      "Epoch: 55/200, Iteration: 2742/10000 --- Training Loss:1.405944\n",
      "Epoch: 55/200, Iteration: 2744/10000 --- Training Loss:0.914161\n",
      "Epoch: 55/200, Iteration: 2746/10000 --- Training Loss:1.333719\n",
      "Epoch: 55/200, Iteration: 2748/10000 --- Training Loss:0.453255\n",
      "Epoch: 55/200, Iteration: 2750/10000 --- Training Loss:0.633472\n",
      "Epoch: 55 finished ! Train Loss: 0.89509, Test Loss: 5.23345\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 56/200, Iteration: 2752/10000 --- Training Loss:0.547450\n",
      "Epoch: 56/200, Iteration: 2754/10000 --- Training Loss:0.357901\n",
      "Epoch: 56/200, Iteration: 2756/10000 --- Training Loss:0.443643\n",
      "Epoch: 56/200, Iteration: 2758/10000 --- Training Loss:0.954239\n",
      "Epoch: 56/200, Iteration: 2760/10000 --- Training Loss:0.959633\n",
      "Epoch: 56/200, Iteration: 2762/10000 --- Training Loss:0.344879\n",
      "Epoch: 56/200, Iteration: 2764/10000 --- Training Loss:0.707163\n",
      "Epoch: 56/200, Iteration: 2766/10000 --- Training Loss:2.101763\n",
      "Epoch: 56/200, Iteration: 2768/10000 --- Training Loss:0.235428\n",
      "Epoch: 56/200, Iteration: 2770/10000 --- Training Loss:0.697459\n",
      "Epoch: 56/200, Iteration: 2772/10000 --- Training Loss:0.599011\n",
      "Epoch: 56/200, Iteration: 2774/10000 --- Training Loss:0.501253\n",
      "Epoch: 56/200, Iteration: 2776/10000 --- Training Loss:0.492317\n",
      "Epoch: 56/200, Iteration: 2778/10000 --- Training Loss:0.450206\n",
      "Epoch: 56/200, Iteration: 2780/10000 --- Training Loss:1.039931\n",
      "Epoch: 56/200, Iteration: 2782/10000 --- Training Loss:0.513324\n",
      "Epoch: 56/200, Iteration: 2784/10000 --- Training Loss:0.449934\n",
      "Epoch: 56/200, Iteration: 2786/10000 --- Training Loss:0.767183\n",
      "Epoch: 56/200, Iteration: 2788/10000 --- Training Loss:1.017574\n",
      "Epoch: 56/200, Iteration: 2790/10000 --- Training Loss:0.707830\n",
      "Epoch: 56/200, Iteration: 2792/10000 --- Training Loss:0.462280\n",
      "Epoch: 56/200, Iteration: 2794/10000 --- Training Loss:0.705865\n",
      "Epoch: 56/200, Iteration: 2796/10000 --- Training Loss:0.439246\n",
      "Epoch: 56/200, Iteration: 2798/10000 --- Training Loss:0.341741\n",
      "Epoch: 56/200, Iteration: 2800/10000 --- Training Loss:1.032774\n",
      "Epoch: 56 finished ! Train Loss: 0.84995, Test Loss: 6.27078\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 57/200, Iteration: 2802/10000 --- Training Loss:0.731656\n",
      "Epoch: 57/200, Iteration: 2804/10000 --- Training Loss:1.036776\n",
      "Epoch: 57/200, Iteration: 2806/10000 --- Training Loss:1.444631\n",
      "Epoch: 57/200, Iteration: 2808/10000 --- Training Loss:0.856214\n",
      "Epoch: 57/200, Iteration: 2810/10000 --- Training Loss:0.546459\n",
      "Epoch: 57/200, Iteration: 2812/10000 --- Training Loss:0.266260\n",
      "Epoch: 57/200, Iteration: 2814/10000 --- Training Loss:0.498861\n",
      "Epoch: 57/200, Iteration: 2816/10000 --- Training Loss:0.856223\n",
      "Epoch: 57/200, Iteration: 2818/10000 --- Training Loss:0.486563\n",
      "Epoch: 57/200, Iteration: 2820/10000 --- Training Loss:0.283890\n",
      "Epoch: 57/200, Iteration: 2822/10000 --- Training Loss:0.375722\n",
      "Epoch: 57/200, Iteration: 2824/10000 --- Training Loss:0.562286\n",
      "Epoch: 57/200, Iteration: 2826/10000 --- Training Loss:0.654103\n",
      "Epoch: 57/200, Iteration: 2828/10000 --- Training Loss:0.421888\n",
      "Epoch: 57/200, Iteration: 2830/10000 --- Training Loss:0.909802\n",
      "Epoch: 57/200, Iteration: 2832/10000 --- Training Loss:0.214749\n",
      "Epoch: 57/200, Iteration: 2834/10000 --- Training Loss:0.916965\n",
      "Epoch: 57/200, Iteration: 2836/10000 --- Training Loss:0.770271\n",
      "Epoch: 57/200, Iteration: 2838/10000 --- Training Loss:1.043417\n",
      "Epoch: 57/200, Iteration: 2840/10000 --- Training Loss:0.252128\n",
      "Epoch: 57/200, Iteration: 2842/10000 --- Training Loss:0.206613\n",
      "Epoch: 57/200, Iteration: 2844/10000 --- Training Loss:0.616639\n",
      "Epoch: 57/200, Iteration: 2846/10000 --- Training Loss:0.267738\n",
      "Epoch: 57/200, Iteration: 2848/10000 --- Training Loss:0.883968\n",
      "Epoch: 57/200, Iteration: 2850/10000 --- Training Loss:0.518765\n",
      "Epoch: 57 finished ! Train Loss: 0.79542, Test Loss: 5.49065\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 58/200, Iteration: 2852/10000 --- Training Loss:1.222190\n",
      "Epoch: 58/200, Iteration: 2854/10000 --- Training Loss:0.978852\n",
      "Epoch: 58/200, Iteration: 2856/10000 --- Training Loss:0.297447\n",
      "Epoch: 58/200, Iteration: 2858/10000 --- Training Loss:2.522893\n",
      "Epoch: 58/200, Iteration: 2860/10000 --- Training Loss:0.986655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58/200, Iteration: 2862/10000 --- Training Loss:0.547878\n",
      "Epoch: 58/200, Iteration: 2864/10000 --- Training Loss:1.748171\n",
      "Epoch: 58/200, Iteration: 2866/10000 --- Training Loss:0.681434\n",
      "Epoch: 58/200, Iteration: 2868/10000 --- Training Loss:0.163240\n",
      "Epoch: 58/200, Iteration: 2870/10000 --- Training Loss:0.774938\n",
      "Epoch: 58/200, Iteration: 2872/10000 --- Training Loss:2.036830\n",
      "Epoch: 58/200, Iteration: 2874/10000 --- Training Loss:0.519130\n",
      "Epoch: 58/200, Iteration: 2876/10000 --- Training Loss:0.611715\n",
      "Epoch: 58/200, Iteration: 2878/10000 --- Training Loss:2.193171\n",
      "Epoch: 58/200, Iteration: 2880/10000 --- Training Loss:0.150273\n",
      "Epoch: 58/200, Iteration: 2882/10000 --- Training Loss:0.575383\n",
      "Epoch: 58/200, Iteration: 2884/10000 --- Training Loss:0.946235\n",
      "Epoch: 58/200, Iteration: 2886/10000 --- Training Loss:0.623685\n",
      "Epoch: 58/200, Iteration: 2888/10000 --- Training Loss:0.103270\n",
      "Epoch: 58/200, Iteration: 2890/10000 --- Training Loss:0.987921\n",
      "Epoch: 58/200, Iteration: 2892/10000 --- Training Loss:0.660944\n",
      "Epoch: 58/200, Iteration: 2894/10000 --- Training Loss:1.238621\n",
      "Epoch: 58/200, Iteration: 2896/10000 --- Training Loss:0.532394\n",
      "Epoch: 58/200, Iteration: 2898/10000 --- Training Loss:0.307983\n",
      "Epoch: 58/200, Iteration: 2900/10000 --- Training Loss:0.351673\n",
      "Epoch: 58 finished ! Train Loss: 0.78009, Test Loss: 6.05741\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 59/200, Iteration: 2902/10000 --- Training Loss:0.851516\n",
      "Epoch: 59/200, Iteration: 2904/10000 --- Training Loss:1.050568\n",
      "Epoch: 59/200, Iteration: 2906/10000 --- Training Loss:0.258354\n",
      "Epoch: 59/200, Iteration: 2908/10000 --- Training Loss:0.882397\n",
      "Epoch: 59/200, Iteration: 2910/10000 --- Training Loss:0.611343\n",
      "Epoch: 59/200, Iteration: 2912/10000 --- Training Loss:0.908758\n",
      "Epoch: 59/200, Iteration: 2914/10000 --- Training Loss:0.171536\n",
      "Epoch: 59/200, Iteration: 2916/10000 --- Training Loss:0.709881\n",
      "Epoch: 59/200, Iteration: 2918/10000 --- Training Loss:0.850898\n",
      "Epoch: 59/200, Iteration: 2920/10000 --- Training Loss:0.714129\n",
      "Epoch: 59/200, Iteration: 2922/10000 --- Training Loss:0.721071\n",
      "Epoch: 59/200, Iteration: 2924/10000 --- Training Loss:0.662874\n",
      "Epoch: 59/200, Iteration: 2926/10000 --- Training Loss:0.135898\n",
      "Epoch: 59/200, Iteration: 2928/10000 --- Training Loss:0.714444\n",
      "Epoch: 59/200, Iteration: 2930/10000 --- Training Loss:0.625899\n",
      "Epoch: 59/200, Iteration: 2932/10000 --- Training Loss:0.428444\n",
      "Epoch: 59/200, Iteration: 2934/10000 --- Training Loss:0.374586\n",
      "Epoch: 59/200, Iteration: 2936/10000 --- Training Loss:0.467770\n",
      "Epoch: 59/200, Iteration: 2938/10000 --- Training Loss:0.753220\n",
      "Epoch: 59/200, Iteration: 2940/10000 --- Training Loss:0.413002\n",
      "Epoch: 59/200, Iteration: 2942/10000 --- Training Loss:1.333702\n",
      "Epoch: 59/200, Iteration: 2944/10000 --- Training Loss:0.761166\n",
      "Epoch: 59/200, Iteration: 2946/10000 --- Training Loss:1.283713\n",
      "Epoch: 59/200, Iteration: 2948/10000 --- Training Loss:1.591143\n",
      "Epoch: 59/200, Iteration: 2950/10000 --- Training Loss:0.371968\n",
      "Epoch: 59 finished ! Train Loss: 0.79028, Test Loss: 5.89977\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 60/200, Iteration: 2952/10000 --- Training Loss:0.612672\n",
      "Epoch: 60/200, Iteration: 2954/10000 --- Training Loss:1.361502\n",
      "Epoch: 60/200, Iteration: 2956/10000 --- Training Loss:0.091452\n",
      "Epoch: 60/200, Iteration: 2958/10000 --- Training Loss:0.270979\n",
      "Epoch: 60/200, Iteration: 2960/10000 --- Training Loss:0.478564\n",
      "Epoch: 60/200, Iteration: 2962/10000 --- Training Loss:1.352455\n",
      "Epoch: 60/200, Iteration: 2964/10000 --- Training Loss:0.442167\n",
      "Epoch: 60/200, Iteration: 2966/10000 --- Training Loss:0.140409\n",
      "Epoch: 60/200, Iteration: 2968/10000 --- Training Loss:0.325593\n",
      "Epoch: 60/200, Iteration: 2970/10000 --- Training Loss:0.513626\n",
      "Epoch: 60/200, Iteration: 2972/10000 --- Training Loss:0.675536\n",
      "Epoch: 60/200, Iteration: 2974/10000 --- Training Loss:0.925167\n",
      "Epoch: 60/200, Iteration: 2976/10000 --- Training Loss:2.104318\n",
      "Epoch: 60/200, Iteration: 2978/10000 --- Training Loss:0.944478\n",
      "Epoch: 60/200, Iteration: 2980/10000 --- Training Loss:0.258668\n",
      "Epoch: 60/200, Iteration: 2982/10000 --- Training Loss:0.926119\n",
      "Epoch: 60/200, Iteration: 2984/10000 --- Training Loss:0.836969\n",
      "Epoch: 60/200, Iteration: 2986/10000 --- Training Loss:0.361457\n",
      "Epoch: 60/200, Iteration: 2988/10000 --- Training Loss:1.046913\n",
      "Epoch: 60/200, Iteration: 2990/10000 --- Training Loss:2.020138\n",
      "Epoch: 60/200, Iteration: 2992/10000 --- Training Loss:0.588314\n",
      "Epoch: 60/200, Iteration: 2994/10000 --- Training Loss:0.686781\n",
      "Epoch: 60/200, Iteration: 2996/10000 --- Training Loss:0.439068\n",
      "Epoch: 60/200, Iteration: 2998/10000 --- Training Loss:0.902737\n",
      "Epoch: 60/200, Iteration: 3000/10000 --- Training Loss:0.879591\n",
      "Epoch: 60 finished ! Train Loss: 0.73725, Test Loss: 5.40259\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 61/200, Iteration: 3002/10000 --- Training Loss:0.596324\n",
      "Epoch: 61/200, Iteration: 3004/10000 --- Training Loss:0.621588\n",
      "Epoch: 61/200, Iteration: 3006/10000 --- Training Loss:0.734335\n",
      "Epoch: 61/200, Iteration: 3008/10000 --- Training Loss:0.845410\n",
      "Epoch: 61/200, Iteration: 3010/10000 --- Training Loss:1.071288\n",
      "Epoch: 61/200, Iteration: 3012/10000 --- Training Loss:0.435771\n",
      "Epoch: 61/200, Iteration: 3014/10000 --- Training Loss:0.316322\n",
      "Epoch: 61/200, Iteration: 3016/10000 --- Training Loss:0.457335\n",
      "Epoch: 61/200, Iteration: 3018/10000 --- Training Loss:0.252052\n",
      "Epoch: 61/200, Iteration: 3020/10000 --- Training Loss:0.444446\n",
      "Epoch: 61/200, Iteration: 3022/10000 --- Training Loss:0.680171\n",
      "Epoch: 61/200, Iteration: 3024/10000 --- Training Loss:0.668886\n",
      "Epoch: 61/200, Iteration: 3026/10000 --- Training Loss:1.204473\n",
      "Epoch: 61/200, Iteration: 3028/10000 --- Training Loss:0.457842\n",
      "Epoch: 61/200, Iteration: 3030/10000 --- Training Loss:0.407063\n",
      "Epoch: 61/200, Iteration: 3032/10000 --- Training Loss:0.324664\n",
      "Epoch: 61/200, Iteration: 3034/10000 --- Training Loss:1.472248\n",
      "Epoch: 61/200, Iteration: 3036/10000 --- Training Loss:1.064908\n",
      "Epoch: 61/200, Iteration: 3038/10000 --- Training Loss:2.344827\n",
      "Epoch: 61/200, Iteration: 3040/10000 --- Training Loss:0.681649\n",
      "Epoch: 61/200, Iteration: 3042/10000 --- Training Loss:0.860082\n",
      "Epoch: 61/200, Iteration: 3044/10000 --- Training Loss:0.998216\n",
      "Epoch: 61/200, Iteration: 3046/10000 --- Training Loss:0.483477\n",
      "Epoch: 61/200, Iteration: 3048/10000 --- Training Loss:0.420864\n",
      "Epoch: 61/200, Iteration: 3050/10000 --- Training Loss:0.566356\n",
      "Epoch: 61 finished ! Train Loss: 0.74077, Test Loss: 6.04789\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 62/200, Iteration: 3052/10000 --- Training Loss:1.058876\n",
      "Epoch: 62/200, Iteration: 3054/10000 --- Training Loss:2.280308\n",
      "Epoch: 62/200, Iteration: 3056/10000 --- Training Loss:0.324275\n",
      "Epoch: 62/200, Iteration: 3058/10000 --- Training Loss:0.517830\n",
      "Epoch: 62/200, Iteration: 3060/10000 --- Training Loss:0.523658\n",
      "Epoch: 62/200, Iteration: 3062/10000 --- Training Loss:0.385128\n",
      "Epoch: 62/200, Iteration: 3064/10000 --- Training Loss:0.342111\n",
      "Epoch: 62/200, Iteration: 3066/10000 --- Training Loss:0.232680\n",
      "Epoch: 62/200, Iteration: 3068/10000 --- Training Loss:0.400751\n",
      "Epoch: 62/200, Iteration: 3070/10000 --- Training Loss:0.508879\n",
      "Epoch: 62/200, Iteration: 3072/10000 --- Training Loss:0.366074\n",
      "Epoch: 62/200, Iteration: 3074/10000 --- Training Loss:0.729267\n",
      "Epoch: 62/200, Iteration: 3076/10000 --- Training Loss:0.356480\n",
      "Epoch: 62/200, Iteration: 3078/10000 --- Training Loss:1.078711\n",
      "Epoch: 62/200, Iteration: 3080/10000 --- Training Loss:0.632991\n",
      "Epoch: 62/200, Iteration: 3082/10000 --- Training Loss:0.120724\n",
      "Epoch: 62/200, Iteration: 3084/10000 --- Training Loss:0.362955\n",
      "Epoch: 62/200, Iteration: 3086/10000 --- Training Loss:0.899916\n",
      "Epoch: 62/200, Iteration: 3088/10000 --- Training Loss:0.192373\n",
      "Epoch: 62/200, Iteration: 3090/10000 --- Training Loss:0.306259\n",
      "Epoch: 62/200, Iteration: 3092/10000 --- Training Loss:1.696371\n",
      "Epoch: 62/200, Iteration: 3094/10000 --- Training Loss:0.649879\n",
      "Epoch: 62/200, Iteration: 3096/10000 --- Training Loss:0.233369\n",
      "Epoch: 62/200, Iteration: 3098/10000 --- Training Loss:1.129917\n",
      "Epoch: 62/200, Iteration: 3100/10000 --- Training Loss:0.386932\n",
      "Epoch: 62 finished ! Train Loss: 0.75662, Test Loss: 4.98996\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 63/200, Iteration: 3102/10000 --- Training Loss:0.393444\n",
      "Epoch: 63/200, Iteration: 3104/10000 --- Training Loss:0.452470\n",
      "Epoch: 63/200, Iteration: 3106/10000 --- Training Loss:1.157225\n",
      "Epoch: 63/200, Iteration: 3108/10000 --- Training Loss:0.870517\n",
      "Epoch: 63/200, Iteration: 3110/10000 --- Training Loss:2.442393\n",
      "Epoch: 63/200, Iteration: 3112/10000 --- Training Loss:0.741224\n",
      "Epoch: 63/200, Iteration: 3114/10000 --- Training Loss:0.420421\n",
      "Epoch: 63/200, Iteration: 3116/10000 --- Training Loss:0.369331\n",
      "Epoch: 63/200, Iteration: 3118/10000 --- Training Loss:0.481868\n",
      "Epoch: 63/200, Iteration: 3120/10000 --- Training Loss:0.185756\n",
      "Epoch: 63/200, Iteration: 3122/10000 --- Training Loss:0.081015\n",
      "Epoch: 63/200, Iteration: 3124/10000 --- Training Loss:0.089672\n",
      "Epoch: 63/200, Iteration: 3126/10000 --- Training Loss:0.651639\n",
      "Epoch: 63/200, Iteration: 3128/10000 --- Training Loss:0.674330\n",
      "Epoch: 63/200, Iteration: 3130/10000 --- Training Loss:2.779285\n",
      "Epoch: 63/200, Iteration: 3132/10000 --- Training Loss:0.382412\n",
      "Epoch: 63/200, Iteration: 3134/10000 --- Training Loss:0.361005\n",
      "Epoch: 63/200, Iteration: 3136/10000 --- Training Loss:0.443323\n",
      "Epoch: 63/200, Iteration: 3138/10000 --- Training Loss:0.603374\n",
      "Epoch: 63/200, Iteration: 3140/10000 --- Training Loss:0.688366\n",
      "Epoch: 63/200, Iteration: 3142/10000 --- Training Loss:0.413552\n",
      "Epoch: 63/200, Iteration: 3144/10000 --- Training Loss:1.723745\n",
      "Epoch: 63/200, Iteration: 3146/10000 --- Training Loss:0.354973\n",
      "Epoch: 63/200, Iteration: 3148/10000 --- Training Loss:0.410494\n",
      "Epoch: 63/200, Iteration: 3150/10000 --- Training Loss:0.772928\n",
      "Epoch: 63 finished ! Train Loss: 0.73445, Test Loss: 5.35874\n",
      "Epoch consuming time: 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64/200, Iteration: 3152/10000 --- Training Loss:0.282505\n",
      "Epoch: 64/200, Iteration: 3154/10000 --- Training Loss:0.296425\n",
      "Epoch: 64/200, Iteration: 3156/10000 --- Training Loss:1.078597\n",
      "Epoch: 64/200, Iteration: 3158/10000 --- Training Loss:0.336167\n",
      "Epoch: 64/200, Iteration: 3160/10000 --- Training Loss:0.207671\n",
      "Epoch: 64/200, Iteration: 3162/10000 --- Training Loss:0.852211\n",
      "Epoch: 64/200, Iteration: 3164/10000 --- Training Loss:0.751515\n",
      "Epoch: 64/200, Iteration: 3166/10000 --- Training Loss:0.874832\n",
      "Epoch: 64/200, Iteration: 3168/10000 --- Training Loss:0.543195\n",
      "Epoch: 64/200, Iteration: 3170/10000 --- Training Loss:1.796916\n",
      "Epoch: 64/200, Iteration: 3172/10000 --- Training Loss:1.667032\n",
      "Epoch: 64/200, Iteration: 3174/10000 --- Training Loss:1.058726\n",
      "Epoch: 64/200, Iteration: 3176/10000 --- Training Loss:0.603250\n",
      "Epoch: 64/200, Iteration: 3178/10000 --- Training Loss:0.573630\n",
      "Epoch: 64/200, Iteration: 3180/10000 --- Training Loss:1.195530\n",
      "Epoch: 64/200, Iteration: 3182/10000 --- Training Loss:0.534186\n",
      "Epoch: 64/200, Iteration: 3184/10000 --- Training Loss:0.431834\n",
      "Epoch: 64/200, Iteration: 3186/10000 --- Training Loss:1.541740\n",
      "Epoch: 64/200, Iteration: 3188/10000 --- Training Loss:0.460260\n",
      "Epoch: 64/200, Iteration: 3190/10000 --- Training Loss:0.550320\n",
      "Epoch: 64/200, Iteration: 3192/10000 --- Training Loss:0.223326\n",
      "Epoch: 64/200, Iteration: 3194/10000 --- Training Loss:0.386877\n",
      "Epoch: 64/200, Iteration: 3196/10000 --- Training Loss:1.101283\n",
      "Epoch: 64/200, Iteration: 3198/10000 --- Training Loss:0.519965\n",
      "Epoch: 64/200, Iteration: 3200/10000 --- Training Loss:0.392353\n",
      "Epoch: 64 finished ! Train Loss: 0.84201, Test Loss: 5.26478\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 65/200, Iteration: 3202/10000 --- Training Loss:1.011291\n",
      "Epoch: 65/200, Iteration: 3204/10000 --- Training Loss:0.497322\n",
      "Epoch: 65/200, Iteration: 3206/10000 --- Training Loss:0.343680\n",
      "Epoch: 65/200, Iteration: 3208/10000 --- Training Loss:0.256878\n",
      "Epoch: 65/200, Iteration: 3210/10000 --- Training Loss:0.922837\n",
      "Epoch: 65/200, Iteration: 3212/10000 --- Training Loss:0.775035\n",
      "Epoch: 65/200, Iteration: 3214/10000 --- Training Loss:0.254211\n",
      "Epoch: 65/200, Iteration: 3216/10000 --- Training Loss:0.690297\n",
      "Epoch: 65/200, Iteration: 3218/10000 --- Training Loss:2.652545\n",
      "Epoch: 65/200, Iteration: 3220/10000 --- Training Loss:0.539636\n",
      "Epoch: 65/200, Iteration: 3222/10000 --- Training Loss:1.079446\n",
      "Epoch: 65/200, Iteration: 3224/10000 --- Training Loss:1.532637\n",
      "Epoch: 65/200, Iteration: 3226/10000 --- Training Loss:0.505661\n",
      "Epoch: 65/200, Iteration: 3228/10000 --- Training Loss:0.513853\n",
      "Epoch: 65/200, Iteration: 3230/10000 --- Training Loss:0.527298\n",
      "Epoch: 65/200, Iteration: 3232/10000 --- Training Loss:0.452372\n",
      "Epoch: 65/200, Iteration: 3234/10000 --- Training Loss:0.519585\n",
      "Epoch: 65/200, Iteration: 3236/10000 --- Training Loss:0.617056\n",
      "Epoch: 65/200, Iteration: 3238/10000 --- Training Loss:0.579063\n",
      "Epoch: 65/200, Iteration: 3240/10000 --- Training Loss:0.651906\n",
      "Epoch: 65/200, Iteration: 3242/10000 --- Training Loss:0.994254\n",
      "Epoch: 65/200, Iteration: 3244/10000 --- Training Loss:1.329685\n",
      "Epoch: 65/200, Iteration: 3246/10000 --- Training Loss:0.297788\n",
      "Epoch: 65/200, Iteration: 3248/10000 --- Training Loss:0.768052\n",
      "Epoch: 65/200, Iteration: 3250/10000 --- Training Loss:0.784960\n",
      "Epoch: 65 finished ! Train Loss: 0.75847, Test Loss: 5.00530\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 66/200, Iteration: 3252/10000 --- Training Loss:0.986545\n",
      "Epoch: 66/200, Iteration: 3254/10000 --- Training Loss:0.413573\n",
      "Epoch: 66/200, Iteration: 3256/10000 --- Training Loss:1.740299\n",
      "Epoch: 66/200, Iteration: 3258/10000 --- Training Loss:0.529273\n",
      "Epoch: 66/200, Iteration: 3260/10000 --- Training Loss:0.965650\n",
      "Epoch: 66/200, Iteration: 3262/10000 --- Training Loss:0.787706\n",
      "Epoch: 66/200, Iteration: 3264/10000 --- Training Loss:0.768565\n",
      "Epoch: 66/200, Iteration: 3266/10000 --- Training Loss:0.567053\n",
      "Epoch: 66/200, Iteration: 3268/10000 --- Training Loss:0.485110\n",
      "Epoch: 66/200, Iteration: 3270/10000 --- Training Loss:0.566860\n",
      "Epoch: 66/200, Iteration: 3272/10000 --- Training Loss:1.011302\n",
      "Epoch: 66/200, Iteration: 3274/10000 --- Training Loss:0.268116\n",
      "Epoch: 66/200, Iteration: 3276/10000 --- Training Loss:0.773348\n",
      "Epoch: 66/200, Iteration: 3278/10000 --- Training Loss:0.962028\n",
      "Epoch: 66/200, Iteration: 3280/10000 --- Training Loss:0.659120\n",
      "Epoch: 66/200, Iteration: 3282/10000 --- Training Loss:0.742723\n",
      "Epoch: 66/200, Iteration: 3284/10000 --- Training Loss:1.675265\n",
      "Epoch: 66/200, Iteration: 3286/10000 --- Training Loss:1.191552\n",
      "Epoch: 66/200, Iteration: 3288/10000 --- Training Loss:1.188487\n",
      "Epoch: 66/200, Iteration: 3290/10000 --- Training Loss:0.549125\n",
      "Epoch: 66/200, Iteration: 3292/10000 --- Training Loss:2.536311\n",
      "Epoch: 66/200, Iteration: 3294/10000 --- Training Loss:0.599573\n",
      "Epoch: 66/200, Iteration: 3296/10000 --- Training Loss:0.279835\n",
      "Epoch: 66/200, Iteration: 3298/10000 --- Training Loss:0.420521\n",
      "Epoch: 66/200, Iteration: 3300/10000 --- Training Loss:0.503434\n",
      "Epoch: 66 finished ! Train Loss: 0.86784, Test Loss: 4.62562\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 67/200, Iteration: 3302/10000 --- Training Loss:0.686466\n",
      "Epoch: 67/200, Iteration: 3304/10000 --- Training Loss:0.731769\n",
      "Epoch: 67/200, Iteration: 3306/10000 --- Training Loss:0.500564\n",
      "Epoch: 67/200, Iteration: 3308/10000 --- Training Loss:0.334843\n",
      "Epoch: 67/200, Iteration: 3310/10000 --- Training Loss:1.055151\n",
      "Epoch: 67/200, Iteration: 3312/10000 --- Training Loss:0.656228\n",
      "Epoch: 67/200, Iteration: 3314/10000 --- Training Loss:1.379792\n",
      "Epoch: 67/200, Iteration: 3316/10000 --- Training Loss:0.427934\n",
      "Epoch: 67/200, Iteration: 3318/10000 --- Training Loss:0.322211\n",
      "Epoch: 67/200, Iteration: 3320/10000 --- Training Loss:1.018496\n",
      "Epoch: 67/200, Iteration: 3322/10000 --- Training Loss:0.722329\n",
      "Epoch: 67/200, Iteration: 3324/10000 --- Training Loss:0.609914\n",
      "Epoch: 67/200, Iteration: 3326/10000 --- Training Loss:0.369182\n",
      "Epoch: 67/200, Iteration: 3328/10000 --- Training Loss:0.589321\n",
      "Epoch: 67/200, Iteration: 3330/10000 --- Training Loss:2.042084\n",
      "Epoch: 67/200, Iteration: 3332/10000 --- Training Loss:0.282793\n",
      "Epoch: 67/200, Iteration: 3334/10000 --- Training Loss:0.891704\n",
      "Epoch: 67/200, Iteration: 3336/10000 --- Training Loss:0.418576\n",
      "Epoch: 67/200, Iteration: 3338/10000 --- Training Loss:0.322714\n",
      "Epoch: 67/200, Iteration: 3340/10000 --- Training Loss:0.346904\n",
      "Epoch: 67/200, Iteration: 3342/10000 --- Training Loss:0.116089\n",
      "Epoch: 67/200, Iteration: 3344/10000 --- Training Loss:1.066472\n",
      "Epoch: 67/200, Iteration: 3346/10000 --- Training Loss:0.405483\n",
      "Epoch: 67/200, Iteration: 3348/10000 --- Training Loss:0.824997\n",
      "Epoch: 67/200, Iteration: 3350/10000 --- Training Loss:0.510139\n",
      "Epoch: 67 finished ! Train Loss: 0.73160, Test Loss: 4.95030\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 68/200, Iteration: 3352/10000 --- Training Loss:0.304284\n",
      "Epoch: 68/200, Iteration: 3354/10000 --- Training Loss:0.499047\n",
      "Epoch: 68/200, Iteration: 3356/10000 --- Training Loss:0.758571\n",
      "Epoch: 68/200, Iteration: 3358/10000 --- Training Loss:0.558749\n",
      "Epoch: 68/200, Iteration: 3360/10000 --- Training Loss:0.227533\n",
      "Epoch: 68/200, Iteration: 3362/10000 --- Training Loss:0.963085\n",
      "Epoch: 68/200, Iteration: 3364/10000 --- Training Loss:0.535655\n",
      "Epoch: 68/200, Iteration: 3366/10000 --- Training Loss:0.648164\n",
      "Epoch: 68/200, Iteration: 3368/10000 --- Training Loss:0.725805\n",
      "Epoch: 68/200, Iteration: 3370/10000 --- Training Loss:0.614917\n",
      "Epoch: 68/200, Iteration: 3372/10000 --- Training Loss:1.121667\n",
      "Epoch: 68/200, Iteration: 3374/10000 --- Training Loss:0.577421\n",
      "Epoch: 68/200, Iteration: 3376/10000 --- Training Loss:0.326275\n",
      "Epoch: 68/200, Iteration: 3378/10000 --- Training Loss:0.843364\n",
      "Epoch: 68/200, Iteration: 3380/10000 --- Training Loss:0.469119\n",
      "Epoch: 68/200, Iteration: 3382/10000 --- Training Loss:0.380031\n",
      "Epoch: 68/200, Iteration: 3384/10000 --- Training Loss:0.247489\n",
      "Epoch: 68/200, Iteration: 3386/10000 --- Training Loss:0.943616\n",
      "Epoch: 68/200, Iteration: 3388/10000 --- Training Loss:0.606931\n",
      "Epoch: 68/200, Iteration: 3390/10000 --- Training Loss:0.498875\n",
      "Epoch: 68/200, Iteration: 3392/10000 --- Training Loss:1.511107\n",
      "Epoch: 68/200, Iteration: 3394/10000 --- Training Loss:0.438585\n",
      "Epoch: 68/200, Iteration: 3396/10000 --- Training Loss:0.811546\n",
      "Epoch: 68/200, Iteration: 3398/10000 --- Training Loss:1.777682\n",
      "Epoch: 68/200, Iteration: 3400/10000 --- Training Loss:0.423199\n",
      "Epoch: 68 finished ! Train Loss: 0.71141, Test Loss: 4.80504\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 69/200, Iteration: 3402/10000 --- Training Loss:1.225752\n",
      "Epoch: 69/200, Iteration: 3404/10000 --- Training Loss:0.548101\n",
      "Epoch: 69/200, Iteration: 3406/10000 --- Training Loss:0.707208\n",
      "Epoch: 69/200, Iteration: 3408/10000 --- Training Loss:1.009666\n",
      "Epoch: 69/200, Iteration: 3410/10000 --- Training Loss:0.544564\n",
      "Epoch: 69/200, Iteration: 3412/10000 --- Training Loss:0.559603\n",
      "Epoch: 69/200, Iteration: 3414/10000 --- Training Loss:0.637041\n",
      "Epoch: 69/200, Iteration: 3416/10000 --- Training Loss:0.472320\n",
      "Epoch: 69/200, Iteration: 3418/10000 --- Training Loss:0.414496\n",
      "Epoch: 69/200, Iteration: 3420/10000 --- Training Loss:0.501429\n",
      "Epoch: 69/200, Iteration: 3422/10000 --- Training Loss:1.267117\n",
      "Epoch: 69/200, Iteration: 3424/10000 --- Training Loss:2.439830\n",
      "Epoch: 69/200, Iteration: 3426/10000 --- Training Loss:0.916377\n",
      "Epoch: 69/200, Iteration: 3428/10000 --- Training Loss:1.049285\n",
      "Epoch: 69/200, Iteration: 3430/10000 --- Training Loss:1.214936\n",
      "Epoch: 69/200, Iteration: 3432/10000 --- Training Loss:1.030071\n",
      "Epoch: 69/200, Iteration: 3434/10000 --- Training Loss:0.963898\n",
      "Epoch: 69/200, Iteration: 3436/10000 --- Training Loss:0.301774\n",
      "Epoch: 69/200, Iteration: 3438/10000 --- Training Loss:0.488823\n",
      "Epoch: 69/200, Iteration: 3440/10000 --- Training Loss:0.576324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69/200, Iteration: 3442/10000 --- Training Loss:0.357259\n",
      "Epoch: 69/200, Iteration: 3444/10000 --- Training Loss:0.619267\n",
      "Epoch: 69/200, Iteration: 3446/10000 --- Training Loss:1.268334\n",
      "Epoch: 69/200, Iteration: 3448/10000 --- Training Loss:1.017748\n",
      "Epoch: 69/200, Iteration: 3450/10000 --- Training Loss:1.490415\n",
      "Epoch: 69 finished ! Train Loss: 0.78993, Test Loss: 4.02552\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 70/200, Iteration: 3452/10000 --- Training Loss:0.532878\n",
      "Epoch: 70/200, Iteration: 3454/10000 --- Training Loss:0.890036\n",
      "Epoch: 70/200, Iteration: 3456/10000 --- Training Loss:0.516463\n",
      "Epoch: 70/200, Iteration: 3458/10000 --- Training Loss:0.952421\n",
      "Epoch: 70/200, Iteration: 3460/10000 --- Training Loss:0.313989\n",
      "Epoch: 70/200, Iteration: 3462/10000 --- Training Loss:1.099001\n",
      "Epoch: 70/200, Iteration: 3464/10000 --- Training Loss:0.997697\n",
      "Epoch: 70/200, Iteration: 3466/10000 --- Training Loss:1.940674\n",
      "Epoch: 70/200, Iteration: 3468/10000 --- Training Loss:1.210042\n",
      "Epoch: 70/200, Iteration: 3470/10000 --- Training Loss:0.576481\n",
      "Epoch: 70/200, Iteration: 3472/10000 --- Training Loss:1.262518\n",
      "Epoch: 70/200, Iteration: 3474/10000 --- Training Loss:0.328305\n",
      "Epoch: 70/200, Iteration: 3476/10000 --- Training Loss:0.361973\n",
      "Epoch: 70/200, Iteration: 3478/10000 --- Training Loss:0.581669\n",
      "Epoch: 70/200, Iteration: 3480/10000 --- Training Loss:1.843566\n",
      "Epoch: 70/200, Iteration: 3482/10000 --- Training Loss:0.328988\n",
      "Epoch: 70/200, Iteration: 3484/10000 --- Training Loss:0.456030\n",
      "Epoch: 70/200, Iteration: 3486/10000 --- Training Loss:0.567156\n",
      "Epoch: 70/200, Iteration: 3488/10000 --- Training Loss:0.272475\n",
      "Epoch: 70/200, Iteration: 3490/10000 --- Training Loss:0.728714\n",
      "Epoch: 70/200, Iteration: 3492/10000 --- Training Loss:0.634885\n",
      "Epoch: 70/200, Iteration: 3494/10000 --- Training Loss:0.286385\n",
      "Epoch: 70/200, Iteration: 3496/10000 --- Training Loss:0.344773\n",
      "Epoch: 70/200, Iteration: 3498/10000 --- Training Loss:0.662394\n",
      "Epoch: 70/200, Iteration: 3500/10000 --- Training Loss:0.513715\n",
      "Epoch: 70 finished ! Train Loss: 0.76344, Test Loss: 4.61885\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 71/200, Iteration: 3502/10000 --- Training Loss:1.431514\n",
      "Epoch: 71/200, Iteration: 3504/10000 --- Training Loss:0.463742\n",
      "Epoch: 71/200, Iteration: 3506/10000 --- Training Loss:0.243220\n",
      "Epoch: 71/200, Iteration: 3508/10000 --- Training Loss:0.346449\n",
      "Epoch: 71/200, Iteration: 3510/10000 --- Training Loss:0.628010\n",
      "Epoch: 71/200, Iteration: 3512/10000 --- Training Loss:0.391696\n",
      "Epoch: 71/200, Iteration: 3514/10000 --- Training Loss:0.370801\n",
      "Epoch: 71/200, Iteration: 3516/10000 --- Training Loss:0.436714\n",
      "Epoch: 71/200, Iteration: 3518/10000 --- Training Loss:0.327585\n",
      "Epoch: 71/200, Iteration: 3520/10000 --- Training Loss:0.589203\n",
      "Epoch: 71/200, Iteration: 3522/10000 --- Training Loss:0.180100\n",
      "Epoch: 71/200, Iteration: 3524/10000 --- Training Loss:0.325080\n",
      "Epoch: 71/200, Iteration: 3526/10000 --- Training Loss:1.008545\n",
      "Epoch: 71/200, Iteration: 3528/10000 --- Training Loss:0.625135\n",
      "Epoch: 71/200, Iteration: 3530/10000 --- Training Loss:0.418501\n",
      "Epoch: 71/200, Iteration: 3532/10000 --- Training Loss:0.617966\n",
      "Epoch: 71/200, Iteration: 3534/10000 --- Training Loss:0.982048\n",
      "Epoch: 71/200, Iteration: 3536/10000 --- Training Loss:1.716622\n",
      "Epoch: 71/200, Iteration: 3538/10000 --- Training Loss:0.705649\n",
      "Epoch: 71/200, Iteration: 3540/10000 --- Training Loss:0.917891\n",
      "Epoch: 71/200, Iteration: 3542/10000 --- Training Loss:0.849429\n",
      "Epoch: 71/200, Iteration: 3544/10000 --- Training Loss:0.726057\n",
      "Epoch: 71/200, Iteration: 3546/10000 --- Training Loss:1.151063\n",
      "Epoch: 71/200, Iteration: 3548/10000 --- Training Loss:0.381160\n",
      "Epoch: 71/200, Iteration: 3550/10000 --- Training Loss:0.413091\n",
      "Epoch: 71 finished ! Train Loss: 0.73561, Test Loss: 4.15008\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 72/200, Iteration: 3552/10000 --- Training Loss:0.701149\n",
      "Epoch: 72/200, Iteration: 3554/10000 --- Training Loss:0.280898\n",
      "Epoch: 72/200, Iteration: 3556/10000 --- Training Loss:0.348015\n",
      "Epoch: 72/200, Iteration: 3558/10000 --- Training Loss:0.700829\n",
      "Epoch: 72/200, Iteration: 3560/10000 --- Training Loss:0.613100\n",
      "Epoch: 72/200, Iteration: 3562/10000 --- Training Loss:0.864767\n",
      "Epoch: 72/200, Iteration: 3564/10000 --- Training Loss:0.200637\n",
      "Epoch: 72/200, Iteration: 3566/10000 --- Training Loss:0.371472\n",
      "Epoch: 72/200, Iteration: 3568/10000 --- Training Loss:0.637936\n",
      "Epoch: 72/200, Iteration: 3570/10000 --- Training Loss:0.908022\n",
      "Epoch: 72/200, Iteration: 3572/10000 --- Training Loss:0.522300\n",
      "Epoch: 72/200, Iteration: 3574/10000 --- Training Loss:0.666163\n",
      "Epoch: 72/200, Iteration: 3576/10000 --- Training Loss:0.818782\n",
      "Epoch: 72/200, Iteration: 3578/10000 --- Training Loss:0.752077\n",
      "Epoch: 72/200, Iteration: 3580/10000 --- Training Loss:1.474375\n",
      "Epoch: 72/200, Iteration: 3582/10000 --- Training Loss:0.651612\n",
      "Epoch: 72/200, Iteration: 3584/10000 --- Training Loss:0.593395\n",
      "Epoch: 72/200, Iteration: 3586/10000 --- Training Loss:0.526892\n",
      "Epoch: 72/200, Iteration: 3588/10000 --- Training Loss:0.807158\n",
      "Epoch: 72/200, Iteration: 3590/10000 --- Training Loss:1.012241\n",
      "Epoch: 72/200, Iteration: 3592/10000 --- Training Loss:2.507890\n",
      "Epoch: 72/200, Iteration: 3594/10000 --- Training Loss:0.268867\n",
      "Epoch: 72/200, Iteration: 3596/10000 --- Training Loss:0.474277\n",
      "Epoch: 72/200, Iteration: 3598/10000 --- Training Loss:0.327471\n",
      "Epoch: 72/200, Iteration: 3600/10000 --- Training Loss:1.200381\n",
      "Epoch: 72 finished ! Train Loss: 0.75410, Test Loss: 4.67204\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 73/200, Iteration: 3602/10000 --- Training Loss:0.582339\n",
      "Epoch: 73/200, Iteration: 3604/10000 --- Training Loss:0.317836\n",
      "Epoch: 73/200, Iteration: 3606/10000 --- Training Loss:0.722822\n",
      "Epoch: 73/200, Iteration: 3608/10000 --- Training Loss:0.772244\n",
      "Epoch: 73/200, Iteration: 3610/10000 --- Training Loss:0.702870\n",
      "Epoch: 73/200, Iteration: 3612/10000 --- Training Loss:0.405507\n",
      "Epoch: 73/200, Iteration: 3614/10000 --- Training Loss:0.650670\n",
      "Epoch: 73/200, Iteration: 3616/10000 --- Training Loss:0.238982\n",
      "Epoch: 73/200, Iteration: 3618/10000 --- Training Loss:0.182971\n",
      "Epoch: 73/200, Iteration: 3620/10000 --- Training Loss:0.738783\n",
      "Epoch: 73/200, Iteration: 3622/10000 --- Training Loss:0.357525\n",
      "Epoch: 73/200, Iteration: 3624/10000 --- Training Loss:3.033513\n",
      "Epoch: 73/200, Iteration: 3626/10000 --- Training Loss:2.610992\n",
      "Epoch: 73/200, Iteration: 3628/10000 --- Training Loss:0.473229\n",
      "Epoch: 73/200, Iteration: 3630/10000 --- Training Loss:0.776419\n",
      "Epoch: 73/200, Iteration: 3632/10000 --- Training Loss:0.619683\n",
      "Epoch: 73/200, Iteration: 3634/10000 --- Training Loss:0.419264\n",
      "Epoch: 73/200, Iteration: 3636/10000 --- Training Loss:0.925278\n",
      "Epoch: 73/200, Iteration: 3638/10000 --- Training Loss:0.654296\n",
      "Epoch: 73/200, Iteration: 3640/10000 --- Training Loss:1.562564\n",
      "Epoch: 73/200, Iteration: 3642/10000 --- Training Loss:0.292004\n",
      "Epoch: 73/200, Iteration: 3644/10000 --- Training Loss:0.261655\n",
      "Epoch: 73/200, Iteration: 3646/10000 --- Training Loss:1.313400\n",
      "Epoch: 73/200, Iteration: 3648/10000 --- Training Loss:1.005802\n",
      "Epoch: 73/200, Iteration: 3650/10000 --- Training Loss:0.561705\n",
      "Epoch: 73 finished ! Train Loss: 0.69726, Test Loss: 4.93846\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 74/200, Iteration: 3652/10000 --- Training Loss:1.067436\n",
      "Epoch: 74/200, Iteration: 3654/10000 --- Training Loss:0.541585\n",
      "Epoch: 74/200, Iteration: 3656/10000 --- Training Loss:0.676871\n",
      "Epoch: 74/200, Iteration: 3658/10000 --- Training Loss:0.548474\n",
      "Epoch: 74/200, Iteration: 3660/10000 --- Training Loss:0.966620\n",
      "Epoch: 74/200, Iteration: 3662/10000 --- Training Loss:0.680709\n",
      "Epoch: 74/200, Iteration: 3664/10000 --- Training Loss:0.423589\n",
      "Epoch: 74/200, Iteration: 3666/10000 --- Training Loss:0.358849\n",
      "Epoch: 74/200, Iteration: 3668/10000 --- Training Loss:0.214395\n",
      "Epoch: 74/200, Iteration: 3670/10000 --- Training Loss:0.500097\n",
      "Epoch: 74/200, Iteration: 3672/10000 --- Training Loss:0.527024\n",
      "Epoch: 74/200, Iteration: 3674/10000 --- Training Loss:0.433130\n",
      "Epoch: 74/200, Iteration: 3676/10000 --- Training Loss:0.561535\n",
      "Epoch: 74/200, Iteration: 3678/10000 --- Training Loss:0.566502\n",
      "Epoch: 74/200, Iteration: 3680/10000 --- Training Loss:0.664906\n",
      "Epoch: 74/200, Iteration: 3682/10000 --- Training Loss:0.970451\n",
      "Epoch: 74/200, Iteration: 3684/10000 --- Training Loss:0.448053\n",
      "Epoch: 74/200, Iteration: 3686/10000 --- Training Loss:0.351291\n",
      "Epoch: 74/200, Iteration: 3688/10000 --- Training Loss:1.995962\n",
      "Epoch: 74/200, Iteration: 3690/10000 --- Training Loss:0.848128\n",
      "Epoch: 74/200, Iteration: 3692/10000 --- Training Loss:0.428817\n",
      "Epoch: 74/200, Iteration: 3694/10000 --- Training Loss:0.484059\n",
      "Epoch: 74/200, Iteration: 3696/10000 --- Training Loss:0.143334\n",
      "Epoch: 74/200, Iteration: 3698/10000 --- Training Loss:0.166896\n",
      "Epoch: 74/200, Iteration: 3700/10000 --- Training Loss:0.287867\n",
      "Epoch: 74 finished ! Train Loss: 0.68896, Test Loss: 4.33679\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 75/200, Iteration: 3702/10000 --- Training Loss:0.581906\n",
      "Epoch: 75/200, Iteration: 3704/10000 --- Training Loss:0.548138\n",
      "Epoch: 75/200, Iteration: 3706/10000 --- Training Loss:0.189044\n",
      "Epoch: 75/200, Iteration: 3708/10000 --- Training Loss:0.370869\n",
      "Epoch: 75/200, Iteration: 3710/10000 --- Training Loss:0.651715\n",
      "Epoch: 75/200, Iteration: 3712/10000 --- Training Loss:0.417319\n",
      "Epoch: 75/200, Iteration: 3714/10000 --- Training Loss:2.120580\n",
      "Epoch: 75/200, Iteration: 3716/10000 --- Training Loss:0.341051\n",
      "Epoch: 75/200, Iteration: 3718/10000 --- Training Loss:0.687769\n",
      "Epoch: 75/200, Iteration: 3720/10000 --- Training Loss:1.008236\n",
      "Epoch: 75/200, Iteration: 3722/10000 --- Training Loss:0.680274\n",
      "Epoch: 75/200, Iteration: 3724/10000 --- Training Loss:0.438958\n",
      "Epoch: 75/200, Iteration: 3726/10000 --- Training Loss:0.908749\n",
      "Epoch: 75/200, Iteration: 3728/10000 --- Training Loss:0.653719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75/200, Iteration: 3730/10000 --- Training Loss:0.176972\n",
      "Epoch: 75/200, Iteration: 3732/10000 --- Training Loss:1.801177\n",
      "Epoch: 75/200, Iteration: 3734/10000 --- Training Loss:0.356448\n",
      "Epoch: 75/200, Iteration: 3736/10000 --- Training Loss:0.824742\n",
      "Epoch: 75/200, Iteration: 3738/10000 --- Training Loss:0.500330\n",
      "Epoch: 75/200, Iteration: 3740/10000 --- Training Loss:0.307587\n",
      "Epoch: 75/200, Iteration: 3742/10000 --- Training Loss:0.636547\n",
      "Epoch: 75/200, Iteration: 3744/10000 --- Training Loss:0.491644\n",
      "Epoch: 75/200, Iteration: 3746/10000 --- Training Loss:0.551242\n",
      "Epoch: 75/200, Iteration: 3748/10000 --- Training Loss:0.951486\n",
      "Epoch: 75/200, Iteration: 3750/10000 --- Training Loss:1.134879\n",
      "Epoch: 75 finished ! Train Loss: 0.66323, Test Loss: 4.73559\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 76/200, Iteration: 3752/10000 --- Training Loss:0.489775\n",
      "Epoch: 76/200, Iteration: 3754/10000 --- Training Loss:1.075764\n",
      "Epoch: 76/200, Iteration: 3756/10000 --- Training Loss:0.565224\n",
      "Epoch: 76/200, Iteration: 3758/10000 --- Training Loss:0.614658\n",
      "Epoch: 76/200, Iteration: 3760/10000 --- Training Loss:0.401847\n",
      "Epoch: 76/200, Iteration: 3762/10000 --- Training Loss:0.646155\n",
      "Epoch: 76/200, Iteration: 3764/10000 --- Training Loss:0.553558\n",
      "Epoch: 76/200, Iteration: 3766/10000 --- Training Loss:0.483437\n",
      "Epoch: 76/200, Iteration: 3768/10000 --- Training Loss:0.699557\n",
      "Epoch: 76/200, Iteration: 3770/10000 --- Training Loss:1.289921\n",
      "Epoch: 76/200, Iteration: 3772/10000 --- Training Loss:0.269099\n",
      "Epoch: 76/200, Iteration: 3774/10000 --- Training Loss:0.589310\n",
      "Epoch: 76/200, Iteration: 3776/10000 --- Training Loss:0.187077\n",
      "Epoch: 76/200, Iteration: 3778/10000 --- Training Loss:1.133742\n",
      "Epoch: 76/200, Iteration: 3780/10000 --- Training Loss:0.460530\n",
      "Epoch: 76/200, Iteration: 3782/10000 --- Training Loss:0.748516\n",
      "Epoch: 76/200, Iteration: 3784/10000 --- Training Loss:0.402669\n",
      "Epoch: 76/200, Iteration: 3786/10000 --- Training Loss:0.506634\n",
      "Epoch: 76/200, Iteration: 3788/10000 --- Training Loss:1.124224\n",
      "Epoch: 76/200, Iteration: 3790/10000 --- Training Loss:0.463822\n",
      "Epoch: 76/200, Iteration: 3792/10000 --- Training Loss:0.703734\n",
      "Epoch: 76/200, Iteration: 3794/10000 --- Training Loss:0.403756\n",
      "Epoch: 76/200, Iteration: 3796/10000 --- Training Loss:0.490518\n",
      "Epoch: 76/200, Iteration: 3798/10000 --- Training Loss:1.860903\n",
      "Epoch: 76/200, Iteration: 3800/10000 --- Training Loss:0.227562\n",
      "Epoch: 76 finished ! Train Loss: 0.67222, Test Loss: 4.44001\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 77/200, Iteration: 3802/10000 --- Training Loss:1.153493\n",
      "Epoch: 77/200, Iteration: 3804/10000 --- Training Loss:0.882718\n",
      "Epoch: 77/200, Iteration: 3806/10000 --- Training Loss:0.605294\n",
      "Epoch: 77/200, Iteration: 3808/10000 --- Training Loss:0.562838\n",
      "Epoch: 77/200, Iteration: 3810/10000 --- Training Loss:0.337774\n",
      "Epoch: 77/200, Iteration: 3812/10000 --- Training Loss:0.109789\n",
      "Epoch: 77/200, Iteration: 3814/10000 --- Training Loss:0.395697\n",
      "Epoch: 77/200, Iteration: 3816/10000 --- Training Loss:0.772002\n",
      "Epoch: 77/200, Iteration: 3818/10000 --- Training Loss:0.680476\n",
      "Epoch: 77/200, Iteration: 3820/10000 --- Training Loss:0.330430\n",
      "Epoch: 77/200, Iteration: 3822/10000 --- Training Loss:0.359256\n",
      "Epoch: 77/200, Iteration: 3824/10000 --- Training Loss:0.974218\n",
      "Epoch: 77/200, Iteration: 3826/10000 --- Training Loss:0.414770\n",
      "Epoch: 77/200, Iteration: 3828/10000 --- Training Loss:0.569580\n",
      "Epoch: 77/200, Iteration: 3830/10000 --- Training Loss:0.653870\n",
      "Epoch: 77/200, Iteration: 3832/10000 --- Training Loss:0.714146\n",
      "Epoch: 77/200, Iteration: 3834/10000 --- Training Loss:0.383983\n",
      "Epoch: 77/200, Iteration: 3836/10000 --- Training Loss:0.599045\n",
      "Epoch: 77/200, Iteration: 3838/10000 --- Training Loss:0.726847\n",
      "Epoch: 77/200, Iteration: 3840/10000 --- Training Loss:0.494310\n",
      "Epoch: 77/200, Iteration: 3842/10000 --- Training Loss:0.393809\n",
      "Epoch: 77/200, Iteration: 3844/10000 --- Training Loss:1.140386\n",
      "Epoch: 77/200, Iteration: 3846/10000 --- Training Loss:0.246736\n",
      "Epoch: 77/200, Iteration: 3848/10000 --- Training Loss:0.494184\n",
      "Epoch: 77/200, Iteration: 3850/10000 --- Training Loss:0.334870\n",
      "Epoch: 77 finished ! Train Loss: 0.67561, Test Loss: 5.41314\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 78/200, Iteration: 3852/10000 --- Training Loss:0.594715\n",
      "Epoch: 78/200, Iteration: 3854/10000 --- Training Loss:1.325470\n",
      "Epoch: 78/200, Iteration: 3856/10000 --- Training Loss:0.766333\n",
      "Epoch: 78/200, Iteration: 3858/10000 --- Training Loss:0.771264\n",
      "Epoch: 78/200, Iteration: 3860/10000 --- Training Loss:0.381141\n",
      "Epoch: 78/200, Iteration: 3862/10000 --- Training Loss:0.777317\n",
      "Epoch: 78/200, Iteration: 3864/10000 --- Training Loss:0.338623\n",
      "Epoch: 78/200, Iteration: 3866/10000 --- Training Loss:0.678135\n",
      "Epoch: 78/200, Iteration: 3868/10000 --- Training Loss:2.110672\n",
      "Epoch: 78/200, Iteration: 3870/10000 --- Training Loss:0.404881\n",
      "Epoch: 78/200, Iteration: 3872/10000 --- Training Loss:0.614828\n",
      "Epoch: 78/200, Iteration: 3874/10000 --- Training Loss:0.381663\n",
      "Epoch: 78/200, Iteration: 3876/10000 --- Training Loss:0.432773\n",
      "Epoch: 78/200, Iteration: 3878/10000 --- Training Loss:0.994229\n",
      "Epoch: 78/200, Iteration: 3880/10000 --- Training Loss:0.793967\n",
      "Epoch: 78/200, Iteration: 3882/10000 --- Training Loss:0.517014\n",
      "Epoch: 78/200, Iteration: 3884/10000 --- Training Loss:0.990734\n",
      "Epoch: 78/200, Iteration: 3886/10000 --- Training Loss:0.783813\n",
      "Epoch: 78/200, Iteration: 3888/10000 --- Training Loss:0.583656\n",
      "Epoch: 78/200, Iteration: 3890/10000 --- Training Loss:1.455029\n",
      "Epoch: 78/200, Iteration: 3892/10000 --- Training Loss:0.379610\n",
      "Epoch: 78/200, Iteration: 3894/10000 --- Training Loss:1.554641\n",
      "Epoch: 78/200, Iteration: 3896/10000 --- Training Loss:0.513537\n",
      "Epoch: 78/200, Iteration: 3898/10000 --- Training Loss:0.166188\n",
      "Epoch: 78/200, Iteration: 3900/10000 --- Training Loss:0.574266\n",
      "Epoch: 78 finished ! Train Loss: 0.73946, Test Loss: 4.55745\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 79/200, Iteration: 3902/10000 --- Training Loss:0.598895\n",
      "Epoch: 79/200, Iteration: 3904/10000 --- Training Loss:0.560662\n",
      "Epoch: 79/200, Iteration: 3906/10000 --- Training Loss:0.571592\n",
      "Epoch: 79/200, Iteration: 3908/10000 --- Training Loss:0.536940\n",
      "Epoch: 79/200, Iteration: 3910/10000 --- Training Loss:0.567481\n",
      "Epoch: 79/200, Iteration: 3912/10000 --- Training Loss:1.219262\n",
      "Epoch: 79/200, Iteration: 3914/10000 --- Training Loss:0.611248\n",
      "Epoch: 79/200, Iteration: 3916/10000 --- Training Loss:0.671848\n",
      "Epoch: 79/200, Iteration: 3918/10000 --- Training Loss:1.068550\n",
      "Epoch: 79/200, Iteration: 3920/10000 --- Training Loss:0.517648\n",
      "Epoch: 79/200, Iteration: 3922/10000 --- Training Loss:0.272601\n",
      "Epoch: 79/200, Iteration: 3924/10000 --- Training Loss:0.712072\n",
      "Epoch: 79/200, Iteration: 3926/10000 --- Training Loss:0.689769\n",
      "Epoch: 79/200, Iteration: 3928/10000 --- Training Loss:0.318565\n",
      "Epoch: 79/200, Iteration: 3930/10000 --- Training Loss:0.899956\n",
      "Epoch: 79/200, Iteration: 3932/10000 --- Training Loss:0.683804\n",
      "Epoch: 79/200, Iteration: 3934/10000 --- Training Loss:0.504189\n",
      "Epoch: 79/200, Iteration: 3936/10000 --- Training Loss:0.307647\n",
      "Epoch: 79/200, Iteration: 3938/10000 --- Training Loss:0.227680\n",
      "Epoch: 79/200, Iteration: 3940/10000 --- Training Loss:0.439647\n",
      "Epoch: 79/200, Iteration: 3942/10000 --- Training Loss:0.547286\n",
      "Epoch: 79/200, Iteration: 3944/10000 --- Training Loss:0.423295\n",
      "Epoch: 79/200, Iteration: 3946/10000 --- Training Loss:2.358069\n",
      "Epoch: 79/200, Iteration: 3948/10000 --- Training Loss:0.933880\n",
      "Epoch: 79/200, Iteration: 3950/10000 --- Training Loss:0.762700\n",
      "Epoch: 79 finished ! Train Loss: 0.68455, Test Loss: 4.18519\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 80/200, Iteration: 3952/10000 --- Training Loss:0.385738\n",
      "Epoch: 80/200, Iteration: 3954/10000 --- Training Loss:0.857595\n",
      "Epoch: 80/200, Iteration: 3956/10000 --- Training Loss:0.763340\n",
      "Epoch: 80/200, Iteration: 3958/10000 --- Training Loss:0.123014\n",
      "Epoch: 80/200, Iteration: 3960/10000 --- Training Loss:0.437151\n",
      "Epoch: 80/200, Iteration: 3962/10000 --- Training Loss:0.349065\n",
      "Epoch: 80/200, Iteration: 3964/10000 --- Training Loss:0.450424\n",
      "Epoch: 80/200, Iteration: 3966/10000 --- Training Loss:0.214116\n",
      "Epoch: 80/200, Iteration: 3968/10000 --- Training Loss:0.529484\n",
      "Epoch: 80/200, Iteration: 3970/10000 --- Training Loss:0.850585\n",
      "Epoch: 80/200, Iteration: 3972/10000 --- Training Loss:0.792388\n",
      "Epoch: 80/200, Iteration: 3974/10000 --- Training Loss:0.236389\n",
      "Epoch: 80/200, Iteration: 3976/10000 --- Training Loss:0.228938\n",
      "Epoch: 80/200, Iteration: 3978/10000 --- Training Loss:0.675673\n",
      "Epoch: 80/200, Iteration: 3980/10000 --- Training Loss:0.263818\n",
      "Epoch: 80/200, Iteration: 3982/10000 --- Training Loss:0.508199\n",
      "Epoch: 80/200, Iteration: 3984/10000 --- Training Loss:0.656570\n",
      "Epoch: 80/200, Iteration: 3986/10000 --- Training Loss:1.031433\n",
      "Epoch: 80/200, Iteration: 3988/10000 --- Training Loss:0.594670\n",
      "Epoch: 80/200, Iteration: 3990/10000 --- Training Loss:0.756294\n",
      "Epoch: 80/200, Iteration: 3992/10000 --- Training Loss:1.193331\n",
      "Epoch: 80/200, Iteration: 3994/10000 --- Training Loss:0.568748\n",
      "Epoch: 80/200, Iteration: 3996/10000 --- Training Loss:0.293168\n",
      "Epoch: 80/200, Iteration: 3998/10000 --- Training Loss:0.492122\n",
      "Epoch: 80/200, Iteration: 4000/10000 --- Training Loss:0.690800\n",
      "Epoch: 80 finished ! Train Loss: 0.69408, Test Loss: 3.92353\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 81/200, Iteration: 4002/10000 --- Training Loss:0.489412\n",
      "Epoch: 81/200, Iteration: 4004/10000 --- Training Loss:1.156412\n",
      "Epoch: 81/200, Iteration: 4006/10000 --- Training Loss:1.293971\n",
      "Epoch: 81/200, Iteration: 4008/10000 --- Training Loss:0.724967\n",
      "Epoch: 81/200, Iteration: 4010/10000 --- Training Loss:0.983000\n",
      "Epoch: 81/200, Iteration: 4012/10000 --- Training Loss:1.793231\n",
      "Epoch: 81/200, Iteration: 4014/10000 --- Training Loss:1.049807\n",
      "Epoch: 81/200, Iteration: 4016/10000 --- Training Loss:0.321880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81/200, Iteration: 4018/10000 --- Training Loss:0.415737\n",
      "Epoch: 81/200, Iteration: 4020/10000 --- Training Loss:0.480082\n",
      "Epoch: 81/200, Iteration: 4022/10000 --- Training Loss:1.007716\n",
      "Epoch: 81/200, Iteration: 4024/10000 --- Training Loss:0.572056\n",
      "Epoch: 81/200, Iteration: 4026/10000 --- Training Loss:0.644221\n",
      "Epoch: 81/200, Iteration: 4028/10000 --- Training Loss:0.349186\n",
      "Epoch: 81/200, Iteration: 4030/10000 --- Training Loss:0.538204\n",
      "Epoch: 81/200, Iteration: 4032/10000 --- Training Loss:0.596371\n",
      "Epoch: 81/200, Iteration: 4034/10000 --- Training Loss:0.827683\n",
      "Epoch: 81/200, Iteration: 4036/10000 --- Training Loss:0.404968\n",
      "Epoch: 81/200, Iteration: 4038/10000 --- Training Loss:0.759997\n",
      "Epoch: 81/200, Iteration: 4040/10000 --- Training Loss:0.470570\n",
      "Epoch: 81/200, Iteration: 4042/10000 --- Training Loss:0.811937\n",
      "Epoch: 81/200, Iteration: 4044/10000 --- Training Loss:0.808832\n",
      "Epoch: 81/200, Iteration: 4046/10000 --- Training Loss:0.724657\n",
      "Epoch: 81/200, Iteration: 4048/10000 --- Training Loss:0.432210\n",
      "Epoch: 81/200, Iteration: 4050/10000 --- Training Loss:0.625096\n",
      "Epoch: 81 finished ! Train Loss: 0.69056, Test Loss: 3.69154\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 82/200, Iteration: 4052/10000 --- Training Loss:0.442003\n",
      "Epoch: 82/200, Iteration: 4054/10000 --- Training Loss:0.392355\n",
      "Epoch: 82/200, Iteration: 4056/10000 --- Training Loss:0.475210\n",
      "Epoch: 82/200, Iteration: 4058/10000 --- Training Loss:0.582830\n",
      "Epoch: 82/200, Iteration: 4060/10000 --- Training Loss:0.689999\n",
      "Epoch: 82/200, Iteration: 4062/10000 --- Training Loss:1.021486\n",
      "Epoch: 82/200, Iteration: 4064/10000 --- Training Loss:0.935458\n",
      "Epoch: 82/200, Iteration: 4066/10000 --- Training Loss:0.412883\n",
      "Epoch: 82/200, Iteration: 4068/10000 --- Training Loss:1.416902\n",
      "Epoch: 82/200, Iteration: 4070/10000 --- Training Loss:0.632599\n",
      "Epoch: 82/200, Iteration: 4072/10000 --- Training Loss:0.365973\n",
      "Epoch: 82/200, Iteration: 4074/10000 --- Training Loss:0.714178\n",
      "Epoch: 82/200, Iteration: 4076/10000 --- Training Loss:0.596477\n",
      "Epoch: 82/200, Iteration: 4078/10000 --- Training Loss:0.661538\n",
      "Epoch: 82/200, Iteration: 4080/10000 --- Training Loss:0.551839\n",
      "Epoch: 82/200, Iteration: 4082/10000 --- Training Loss:0.423587\n",
      "Epoch: 82/200, Iteration: 4084/10000 --- Training Loss:0.372243\n",
      "Epoch: 82/200, Iteration: 4086/10000 --- Training Loss:0.576193\n",
      "Epoch: 82/200, Iteration: 4088/10000 --- Training Loss:1.192881\n",
      "Epoch: 82/200, Iteration: 4090/10000 --- Training Loss:0.194079\n",
      "Epoch: 82/200, Iteration: 4092/10000 --- Training Loss:0.940953\n",
      "Epoch: 82/200, Iteration: 4094/10000 --- Training Loss:0.443056\n",
      "Epoch: 82/200, Iteration: 4096/10000 --- Training Loss:1.303139\n",
      "Epoch: 82/200, Iteration: 4098/10000 --- Training Loss:1.628608\n",
      "Epoch: 82/200, Iteration: 4100/10000 --- Training Loss:0.343584\n",
      "Epoch: 82 finished ! Train Loss: 0.74057, Test Loss: 5.43761\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 83/200, Iteration: 4102/10000 --- Training Loss:1.219180\n",
      "Epoch: 83/200, Iteration: 4104/10000 --- Training Loss:0.774976\n",
      "Epoch: 83/200, Iteration: 4106/10000 --- Training Loss:1.490834\n",
      "Epoch: 83/200, Iteration: 4108/10000 --- Training Loss:0.733797\n",
      "Epoch: 83/200, Iteration: 4110/10000 --- Training Loss:0.653789\n",
      "Epoch: 83/200, Iteration: 4112/10000 --- Training Loss:1.548136\n",
      "Epoch: 83/200, Iteration: 4114/10000 --- Training Loss:0.416383\n",
      "Epoch: 83/200, Iteration: 4116/10000 --- Training Loss:0.524779\n",
      "Epoch: 83/200, Iteration: 4118/10000 --- Training Loss:1.116489\n",
      "Epoch: 83/200, Iteration: 4120/10000 --- Training Loss:1.532931\n",
      "Epoch: 83/200, Iteration: 4122/10000 --- Training Loss:1.704533\n",
      "Epoch: 83/200, Iteration: 4124/10000 --- Training Loss:1.031470\n",
      "Epoch: 83/200, Iteration: 4126/10000 --- Training Loss:0.399714\n",
      "Epoch: 83/200, Iteration: 4128/10000 --- Training Loss:0.614043\n",
      "Epoch: 83/200, Iteration: 4130/10000 --- Training Loss:1.116550\n",
      "Epoch: 83/200, Iteration: 4132/10000 --- Training Loss:0.557931\n",
      "Epoch: 83/200, Iteration: 4134/10000 --- Training Loss:0.626333\n",
      "Epoch: 83/200, Iteration: 4136/10000 --- Training Loss:0.884522\n",
      "Epoch: 83/200, Iteration: 4138/10000 --- Training Loss:0.849458\n",
      "Epoch: 83/200, Iteration: 4140/10000 --- Training Loss:0.475673\n",
      "Epoch: 83/200, Iteration: 4142/10000 --- Training Loss:0.293174\n",
      "Epoch: 83/200, Iteration: 4144/10000 --- Training Loss:0.231582\n",
      "Epoch: 83/200, Iteration: 4146/10000 --- Training Loss:0.980711\n",
      "Epoch: 83/200, Iteration: 4148/10000 --- Training Loss:0.997306\n",
      "Epoch: 83/200, Iteration: 4150/10000 --- Training Loss:2.455982\n",
      "Epoch: 83 finished ! Train Loss: 0.89773, Test Loss: 3.46696\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 84/200, Iteration: 4152/10000 --- Training Loss:0.520449\n",
      "Epoch: 84/200, Iteration: 4154/10000 --- Training Loss:0.498302\n",
      "Epoch: 84/200, Iteration: 4156/10000 --- Training Loss:1.474257\n",
      "Epoch: 84/200, Iteration: 4158/10000 --- Training Loss:0.837063\n",
      "Epoch: 84/200, Iteration: 4160/10000 --- Training Loss:1.590744\n",
      "Epoch: 84/200, Iteration: 4162/10000 --- Training Loss:0.278429\n",
      "Epoch: 84/200, Iteration: 4164/10000 --- Training Loss:0.858004\n",
      "Epoch: 84/200, Iteration: 4166/10000 --- Training Loss:0.595150\n",
      "Epoch: 84/200, Iteration: 4168/10000 --- Training Loss:0.375038\n",
      "Epoch: 84/200, Iteration: 4170/10000 --- Training Loss:0.691572\n",
      "Epoch: 84/200, Iteration: 4172/10000 --- Training Loss:0.749915\n",
      "Epoch: 84/200, Iteration: 4174/10000 --- Training Loss:1.075224\n",
      "Epoch: 84/200, Iteration: 4176/10000 --- Training Loss:0.357863\n",
      "Epoch: 84/200, Iteration: 4178/10000 --- Training Loss:0.329338\n",
      "Epoch: 84/200, Iteration: 4180/10000 --- Training Loss:0.470387\n",
      "Epoch: 84/200, Iteration: 4182/10000 --- Training Loss:0.520304\n",
      "Epoch: 84/200, Iteration: 4184/10000 --- Training Loss:0.387488\n",
      "Epoch: 84/200, Iteration: 4186/10000 --- Training Loss:0.861480\n",
      "Epoch: 84/200, Iteration: 4188/10000 --- Training Loss:1.861346\n",
      "Epoch: 84/200, Iteration: 4190/10000 --- Training Loss:1.805650\n",
      "Epoch: 84/200, Iteration: 4192/10000 --- Training Loss:0.132258\n",
      "Epoch: 84/200, Iteration: 4194/10000 --- Training Loss:0.679451\n",
      "Epoch: 84/200, Iteration: 4196/10000 --- Training Loss:0.804504\n",
      "Epoch: 84/200, Iteration: 4198/10000 --- Training Loss:0.839839\n",
      "Epoch: 84/200, Iteration: 4200/10000 --- Training Loss:0.878401\n",
      "Epoch: 84 finished ! Train Loss: 0.77080, Test Loss: 4.31865\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 85/200, Iteration: 4202/10000 --- Training Loss:0.439564\n",
      "Epoch: 85/200, Iteration: 4204/10000 --- Training Loss:1.837553\n",
      "Epoch: 85/200, Iteration: 4206/10000 --- Training Loss:0.646316\n",
      "Epoch: 85/200, Iteration: 4208/10000 --- Training Loss:0.950491\n",
      "Epoch: 85/200, Iteration: 4210/10000 --- Training Loss:1.715266\n",
      "Epoch: 85/200, Iteration: 4212/10000 --- Training Loss:0.383755\n",
      "Epoch: 85/200, Iteration: 4214/10000 --- Training Loss:2.232253\n",
      "Epoch: 85/200, Iteration: 4216/10000 --- Training Loss:1.247940\n",
      "Epoch: 85/200, Iteration: 4218/10000 --- Training Loss:1.618083\n",
      "Epoch: 85/200, Iteration: 4220/10000 --- Training Loss:1.118213\n",
      "Epoch: 85/200, Iteration: 4222/10000 --- Training Loss:0.431922\n",
      "Epoch: 85/200, Iteration: 4224/10000 --- Training Loss:0.912704\n",
      "Epoch: 85/200, Iteration: 4226/10000 --- Training Loss:0.317164\n",
      "Epoch: 85/200, Iteration: 4228/10000 --- Training Loss:2.359141\n",
      "Epoch: 85/200, Iteration: 4230/10000 --- Training Loss:0.346171\n",
      "Epoch: 85/200, Iteration: 4232/10000 --- Training Loss:0.595335\n",
      "Epoch: 85/200, Iteration: 4234/10000 --- Training Loss:0.876952\n",
      "Epoch: 85/200, Iteration: 4236/10000 --- Training Loss:0.724508\n",
      "Epoch: 85/200, Iteration: 4238/10000 --- Training Loss:0.287398\n",
      "Epoch: 85/200, Iteration: 4240/10000 --- Training Loss:0.957924\n",
      "Epoch: 85/200, Iteration: 4242/10000 --- Training Loss:0.308510\n",
      "Epoch: 85/200, Iteration: 4244/10000 --- Training Loss:0.808513\n",
      "Epoch: 85/200, Iteration: 4246/10000 --- Training Loss:0.582756\n",
      "Epoch: 85/200, Iteration: 4248/10000 --- Training Loss:0.805779\n",
      "Epoch: 85/200, Iteration: 4250/10000 --- Training Loss:0.381818\n",
      "Epoch: 85 finished ! Train Loss: 0.87243, Test Loss: 3.80878\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 86/200, Iteration: 4252/10000 --- Training Loss:0.530975\n",
      "Epoch: 86/200, Iteration: 4254/10000 --- Training Loss:0.632478\n",
      "Epoch: 86/200, Iteration: 4256/10000 --- Training Loss:0.183369\n",
      "Epoch: 86/200, Iteration: 4258/10000 --- Training Loss:0.496962\n",
      "Epoch: 86/200, Iteration: 4260/10000 --- Training Loss:0.341394\n",
      "Epoch: 86/200, Iteration: 4262/10000 --- Training Loss:0.690858\n",
      "Epoch: 86/200, Iteration: 4264/10000 --- Training Loss:1.132584\n",
      "Epoch: 86/200, Iteration: 4266/10000 --- Training Loss:0.570183\n",
      "Epoch: 86/200, Iteration: 4268/10000 --- Training Loss:0.188273\n",
      "Epoch: 86/200, Iteration: 4270/10000 --- Training Loss:1.511738\n",
      "Epoch: 86/200, Iteration: 4272/10000 --- Training Loss:0.503634\n",
      "Epoch: 86/200, Iteration: 4274/10000 --- Training Loss:1.031620\n",
      "Epoch: 86/200, Iteration: 4276/10000 --- Training Loss:0.209049\n",
      "Epoch: 86/200, Iteration: 4278/10000 --- Training Loss:0.807375\n",
      "Epoch: 86/200, Iteration: 4280/10000 --- Training Loss:1.888908\n",
      "Epoch: 86/200, Iteration: 4282/10000 --- Training Loss:0.925351\n",
      "Epoch: 86/200, Iteration: 4284/10000 --- Training Loss:1.101824\n",
      "Epoch: 86/200, Iteration: 4286/10000 --- Training Loss:0.707544\n",
      "Epoch: 86/200, Iteration: 4288/10000 --- Training Loss:1.398022\n",
      "Epoch: 86/200, Iteration: 4290/10000 --- Training Loss:0.822533\n",
      "Epoch: 86/200, Iteration: 4292/10000 --- Training Loss:0.582841\n",
      "Epoch: 86/200, Iteration: 4294/10000 --- Training Loss:1.768965\n",
      "Epoch: 86/200, Iteration: 4296/10000 --- Training Loss:0.299052\n",
      "Epoch: 86/200, Iteration: 4298/10000 --- Training Loss:1.330447\n",
      "Epoch: 86/200, Iteration: 4300/10000 --- Training Loss:1.278819\n",
      "Epoch: 86 finished ! Train Loss: 0.78573, Test Loss: 3.47286\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 87/200, Iteration: 4302/10000 --- Training Loss:0.980623\n",
      "Epoch: 87/200, Iteration: 4304/10000 --- Training Loss:0.984001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87/200, Iteration: 4306/10000 --- Training Loss:0.379074\n",
      "Epoch: 87/200, Iteration: 4308/10000 --- Training Loss:0.182980\n",
      "Epoch: 87/200, Iteration: 4310/10000 --- Training Loss:0.358595\n",
      "Epoch: 87/200, Iteration: 4312/10000 --- Training Loss:0.167241\n",
      "Epoch: 87/200, Iteration: 4314/10000 --- Training Loss:0.919150\n",
      "Epoch: 87/200, Iteration: 4316/10000 --- Training Loss:0.431371\n",
      "Epoch: 87/200, Iteration: 4318/10000 --- Training Loss:1.588842\n",
      "Epoch: 87/200, Iteration: 4320/10000 --- Training Loss:1.060571\n",
      "Epoch: 87/200, Iteration: 4322/10000 --- Training Loss:0.584323\n",
      "Epoch: 87/200, Iteration: 4324/10000 --- Training Loss:0.145488\n",
      "Epoch: 87/200, Iteration: 4326/10000 --- Training Loss:1.382711\n",
      "Epoch: 87/200, Iteration: 4328/10000 --- Training Loss:1.039119\n",
      "Epoch: 87/200, Iteration: 4330/10000 --- Training Loss:0.527698\n",
      "Epoch: 87/200, Iteration: 4332/10000 --- Training Loss:1.497671\n",
      "Epoch: 87/200, Iteration: 4334/10000 --- Training Loss:0.373237\n",
      "Epoch: 87/200, Iteration: 4336/10000 --- Training Loss:0.464563\n",
      "Epoch: 87/200, Iteration: 4338/10000 --- Training Loss:0.487037\n",
      "Epoch: 87/200, Iteration: 4340/10000 --- Training Loss:0.276134\n",
      "Epoch: 87/200, Iteration: 4342/10000 --- Training Loss:0.293330\n",
      "Epoch: 87/200, Iteration: 4344/10000 --- Training Loss:0.692184\n",
      "Epoch: 87/200, Iteration: 4346/10000 --- Training Loss:1.734598\n",
      "Epoch: 87/200, Iteration: 4348/10000 --- Training Loss:0.817753\n",
      "Epoch: 87/200, Iteration: 4350/10000 --- Training Loss:0.587451\n",
      "Epoch: 87 finished ! Train Loss: 0.68336, Test Loss: 3.79961\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 88/200, Iteration: 4352/10000 --- Training Loss:0.451470\n",
      "Epoch: 88/200, Iteration: 4354/10000 --- Training Loss:0.654700\n",
      "Epoch: 88/200, Iteration: 4356/10000 --- Training Loss:0.419209\n",
      "Epoch: 88/200, Iteration: 4358/10000 --- Training Loss:0.513666\n",
      "Epoch: 88/200, Iteration: 4360/10000 --- Training Loss:1.527010\n",
      "Epoch: 88/200, Iteration: 4362/10000 --- Training Loss:0.466204\n",
      "Epoch: 88/200, Iteration: 4364/10000 --- Training Loss:0.360211\n",
      "Epoch: 88/200, Iteration: 4366/10000 --- Training Loss:0.528801\n",
      "Epoch: 88/200, Iteration: 4368/10000 --- Training Loss:0.446877\n",
      "Epoch: 88/200, Iteration: 4370/10000 --- Training Loss:0.471652\n",
      "Epoch: 88/200, Iteration: 4372/10000 --- Training Loss:0.758553\n",
      "Epoch: 88/200, Iteration: 4374/10000 --- Training Loss:0.460109\n",
      "Epoch: 88/200, Iteration: 4376/10000 --- Training Loss:0.304982\n",
      "Epoch: 88/200, Iteration: 4378/10000 --- Training Loss:0.641923\n",
      "Epoch: 88/200, Iteration: 4380/10000 --- Training Loss:0.827991\n",
      "Epoch: 88/200, Iteration: 4382/10000 --- Training Loss:0.345095\n",
      "Epoch: 88/200, Iteration: 4384/10000 --- Training Loss:0.290419\n",
      "Epoch: 88/200, Iteration: 4386/10000 --- Training Loss:0.984246\n",
      "Epoch: 88/200, Iteration: 4388/10000 --- Training Loss:0.242800\n",
      "Epoch: 88/200, Iteration: 4390/10000 --- Training Loss:2.076932\n",
      "Epoch: 88/200, Iteration: 4392/10000 --- Training Loss:0.720722\n",
      "Epoch: 88/200, Iteration: 4394/10000 --- Training Loss:0.319795\n",
      "Epoch: 88/200, Iteration: 4396/10000 --- Training Loss:0.292566\n",
      "Epoch: 88/200, Iteration: 4398/10000 --- Training Loss:0.478663\n",
      "Epoch: 88/200, Iteration: 4400/10000 --- Training Loss:0.412792\n",
      "Epoch: 88 finished ! Train Loss: 0.63250, Test Loss: 3.29545\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 89/200, Iteration: 4402/10000 --- Training Loss:0.387692\n",
      "Epoch: 89/200, Iteration: 4404/10000 --- Training Loss:0.371039\n",
      "Epoch: 89/200, Iteration: 4406/10000 --- Training Loss:1.284639\n",
      "Epoch: 89/200, Iteration: 4408/10000 --- Training Loss:0.490280\n",
      "Epoch: 89/200, Iteration: 4410/10000 --- Training Loss:1.968858\n",
      "Epoch: 89/200, Iteration: 4412/10000 --- Training Loss:0.460981\n",
      "Epoch: 89/200, Iteration: 4414/10000 --- Training Loss:0.363868\n",
      "Epoch: 89/200, Iteration: 4416/10000 --- Training Loss:1.087715\n",
      "Epoch: 89/200, Iteration: 4418/10000 --- Training Loss:0.781519\n",
      "Epoch: 89/200, Iteration: 4420/10000 --- Training Loss:0.946279\n",
      "Epoch: 89/200, Iteration: 4422/10000 --- Training Loss:0.627114\n",
      "Epoch: 89/200, Iteration: 4424/10000 --- Training Loss:1.065862\n",
      "Epoch: 89/200, Iteration: 4426/10000 --- Training Loss:1.140558\n",
      "Epoch: 89/200, Iteration: 4428/10000 --- Training Loss:0.267091\n",
      "Epoch: 89/200, Iteration: 4430/10000 --- Training Loss:0.675066\n",
      "Epoch: 89/200, Iteration: 4432/10000 --- Training Loss:0.454193\n",
      "Epoch: 89/200, Iteration: 4434/10000 --- Training Loss:0.937223\n",
      "Epoch: 89/200, Iteration: 4436/10000 --- Training Loss:0.696480\n",
      "Epoch: 89/200, Iteration: 4438/10000 --- Training Loss:1.105915\n",
      "Epoch: 89/200, Iteration: 4440/10000 --- Training Loss:0.764535\n",
      "Epoch: 89/200, Iteration: 4442/10000 --- Training Loss:0.247707\n",
      "Epoch: 89/200, Iteration: 4444/10000 --- Training Loss:0.979691\n",
      "Epoch: 89/200, Iteration: 4446/10000 --- Training Loss:0.722619\n",
      "Epoch: 89/200, Iteration: 4448/10000 --- Training Loss:0.434826\n",
      "Epoch: 89/200, Iteration: 4450/10000 --- Training Loss:0.483371\n",
      "Epoch: 89 finished ! Train Loss: 0.73667, Test Loss: 3.25300\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 90/200, Iteration: 4452/10000 --- Training Loss:0.527812\n",
      "Epoch: 90/200, Iteration: 4454/10000 --- Training Loss:0.596381\n",
      "Epoch: 90/200, Iteration: 4456/10000 --- Training Loss:1.004767\n",
      "Epoch: 90/200, Iteration: 4458/10000 --- Training Loss:0.135155\n",
      "Epoch: 90/200, Iteration: 4460/10000 --- Training Loss:1.075696\n",
      "Epoch: 90/200, Iteration: 4462/10000 --- Training Loss:0.181189\n",
      "Epoch: 90/200, Iteration: 4464/10000 --- Training Loss:0.195089\n",
      "Epoch: 90/200, Iteration: 4466/10000 --- Training Loss:0.399196\n",
      "Epoch: 90/200, Iteration: 4468/10000 --- Training Loss:0.653948\n",
      "Epoch: 90/200, Iteration: 4470/10000 --- Training Loss:0.202332\n",
      "Epoch: 90/200, Iteration: 4472/10000 --- Training Loss:0.217415\n",
      "Epoch: 90/200, Iteration: 4474/10000 --- Training Loss:0.962545\n",
      "Epoch: 90/200, Iteration: 4476/10000 --- Training Loss:0.850713\n",
      "Epoch: 90/200, Iteration: 4478/10000 --- Training Loss:3.190212\n",
      "Epoch: 90/200, Iteration: 4480/10000 --- Training Loss:1.412836\n",
      "Epoch: 90/200, Iteration: 4482/10000 --- Training Loss:0.864412\n",
      "Epoch: 90/200, Iteration: 4484/10000 --- Training Loss:0.844010\n",
      "Epoch: 90/200, Iteration: 4486/10000 --- Training Loss:0.937728\n",
      "Epoch: 90/200, Iteration: 4488/10000 --- Training Loss:0.259578\n",
      "Epoch: 90/200, Iteration: 4490/10000 --- Training Loss:0.247469\n",
      "Epoch: 90/200, Iteration: 4492/10000 --- Training Loss:0.545258\n",
      "Epoch: 90/200, Iteration: 4494/10000 --- Training Loss:0.477880\n",
      "Epoch: 90/200, Iteration: 4496/10000 --- Training Loss:0.921985\n",
      "Epoch: 90/200, Iteration: 4498/10000 --- Training Loss:0.419194\n",
      "Epoch: 90/200, Iteration: 4500/10000 --- Training Loss:1.161309\n",
      "Epoch: 90 finished ! Train Loss: 0.68158, Test Loss: 3.06327\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 91/200, Iteration: 4502/10000 --- Training Loss:0.651699\n",
      "Epoch: 91/200, Iteration: 4504/10000 --- Training Loss:0.854276\n",
      "Epoch: 91/200, Iteration: 4506/10000 --- Training Loss:0.713138\n",
      "Epoch: 91/200, Iteration: 4508/10000 --- Training Loss:0.774088\n",
      "Epoch: 91/200, Iteration: 4510/10000 --- Training Loss:1.536335\n",
      "Epoch: 91/200, Iteration: 4512/10000 --- Training Loss:1.040777\n",
      "Epoch: 91/200, Iteration: 4514/10000 --- Training Loss:1.244218\n",
      "Epoch: 91/200, Iteration: 4516/10000 --- Training Loss:0.504468\n",
      "Epoch: 91/200, Iteration: 4518/10000 --- Training Loss:0.817489\n",
      "Epoch: 91/200, Iteration: 4520/10000 --- Training Loss:1.100756\n",
      "Epoch: 91/200, Iteration: 4522/10000 --- Training Loss:1.486963\n",
      "Epoch: 91/200, Iteration: 4524/10000 --- Training Loss:0.561853\n",
      "Epoch: 91/200, Iteration: 4526/10000 --- Training Loss:1.387603\n",
      "Epoch: 91/200, Iteration: 4528/10000 --- Training Loss:1.758769\n",
      "Epoch: 91/200, Iteration: 4530/10000 --- Training Loss:0.496699\n",
      "Epoch: 91/200, Iteration: 4532/10000 --- Training Loss:0.788216\n",
      "Epoch: 91/200, Iteration: 4534/10000 --- Training Loss:0.367078\n",
      "Epoch: 91/200, Iteration: 4536/10000 --- Training Loss:0.735081\n",
      "Epoch: 91/200, Iteration: 4538/10000 --- Training Loss:0.274946\n",
      "Epoch: 91/200, Iteration: 4540/10000 --- Training Loss:0.214034\n",
      "Epoch: 91/200, Iteration: 4542/10000 --- Training Loss:0.285428\n",
      "Epoch: 91/200, Iteration: 4544/10000 --- Training Loss:1.648703\n",
      "Epoch: 91/200, Iteration: 4546/10000 --- Training Loss:0.691891\n",
      "Epoch: 91/200, Iteration: 4548/10000 --- Training Loss:0.289192\n",
      "Epoch: 91/200, Iteration: 4550/10000 --- Training Loss:0.380927\n",
      "Epoch: 91 finished ! Train Loss: 0.82479, Test Loss: 4.67356\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 92/200, Iteration: 4552/10000 --- Training Loss:0.523167\n",
      "Epoch: 92/200, Iteration: 4554/10000 --- Training Loss:0.222601\n",
      "Epoch: 92/200, Iteration: 4556/10000 --- Training Loss:0.405125\n",
      "Epoch: 92/200, Iteration: 4558/10000 --- Training Loss:0.214387\n",
      "Epoch: 92/200, Iteration: 4560/10000 --- Training Loss:0.579641\n",
      "Epoch: 92/200, Iteration: 4562/10000 --- Training Loss:1.202669\n",
      "Epoch: 92/200, Iteration: 4564/10000 --- Training Loss:0.183135\n",
      "Epoch: 92/200, Iteration: 4566/10000 --- Training Loss:0.929359\n",
      "Epoch: 92/200, Iteration: 4568/10000 --- Training Loss:0.464800\n",
      "Epoch: 92/200, Iteration: 4570/10000 --- Training Loss:0.911917\n",
      "Epoch: 92/200, Iteration: 4572/10000 --- Training Loss:1.014779\n",
      "Epoch: 92/200, Iteration: 4574/10000 --- Training Loss:0.520251\n",
      "Epoch: 92/200, Iteration: 4576/10000 --- Training Loss:0.760884\n",
      "Epoch: 92/200, Iteration: 4578/10000 --- Training Loss:1.195002\n",
      "Epoch: 92/200, Iteration: 4580/10000 --- Training Loss:0.807247\n",
      "Epoch: 92/200, Iteration: 4582/10000 --- Training Loss:2.841121\n",
      "Epoch: 92/200, Iteration: 4584/10000 --- Training Loss:0.369200\n",
      "Epoch: 92/200, Iteration: 4586/10000 --- Training Loss:0.512715\n",
      "Epoch: 92/200, Iteration: 4588/10000 --- Training Loss:1.075607\n",
      "Epoch: 92/200, Iteration: 4590/10000 --- Training Loss:0.894730\n",
      "Epoch: 92/200, Iteration: 4592/10000 --- Training Loss:0.370129\n",
      "Epoch: 92/200, Iteration: 4594/10000 --- Training Loss:1.318528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92/200, Iteration: 4596/10000 --- Training Loss:0.876603\n",
      "Epoch: 92/200, Iteration: 4598/10000 --- Training Loss:0.587752\n",
      "Epoch: 92/200, Iteration: 4600/10000 --- Training Loss:0.264655\n",
      "Epoch: 92 finished ! Train Loss: 0.71326, Test Loss: 3.72721\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 93/200, Iteration: 4602/10000 --- Training Loss:0.845044\n",
      "Epoch: 93/200, Iteration: 4604/10000 --- Training Loss:0.539690\n",
      "Epoch: 93/200, Iteration: 4606/10000 --- Training Loss:1.039121\n",
      "Epoch: 93/200, Iteration: 4608/10000 --- Training Loss:0.559350\n",
      "Epoch: 93/200, Iteration: 4610/10000 --- Training Loss:0.406719\n",
      "Epoch: 93/200, Iteration: 4612/10000 --- Training Loss:1.037072\n",
      "Epoch: 93/200, Iteration: 4614/10000 --- Training Loss:0.365460\n",
      "Epoch: 93/200, Iteration: 4616/10000 --- Training Loss:0.424283\n",
      "Epoch: 93/200, Iteration: 4618/10000 --- Training Loss:0.354464\n",
      "Epoch: 93/200, Iteration: 4620/10000 --- Training Loss:0.406760\n",
      "Epoch: 93/200, Iteration: 4622/10000 --- Training Loss:0.663786\n",
      "Epoch: 93/200, Iteration: 4624/10000 --- Training Loss:0.126716\n",
      "Epoch: 93/200, Iteration: 4626/10000 --- Training Loss:0.564272\n",
      "Epoch: 93/200, Iteration: 4628/10000 --- Training Loss:1.200961\n",
      "Epoch: 93/200, Iteration: 4630/10000 --- Training Loss:0.842184\n",
      "Epoch: 93/200, Iteration: 4632/10000 --- Training Loss:0.245812\n",
      "Epoch: 93/200, Iteration: 4634/10000 --- Training Loss:0.969451\n",
      "Epoch: 93/200, Iteration: 4636/10000 --- Training Loss:0.675664\n",
      "Epoch: 93/200, Iteration: 4638/10000 --- Training Loss:0.948923\n",
      "Epoch: 93/200, Iteration: 4640/10000 --- Training Loss:0.282624\n",
      "Epoch: 93/200, Iteration: 4642/10000 --- Training Loss:0.809703\n",
      "Epoch: 93/200, Iteration: 4644/10000 --- Training Loss:0.607736\n",
      "Epoch: 93/200, Iteration: 4646/10000 --- Training Loss:0.446302\n",
      "Epoch: 93/200, Iteration: 4648/10000 --- Training Loss:0.423207\n",
      "Epoch: 93/200, Iteration: 4650/10000 --- Training Loss:1.807240\n",
      "Epoch: 93 finished ! Train Loss: 0.63749, Test Loss: 3.24403\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 94/200, Iteration: 4652/10000 --- Training Loss:0.334847\n",
      "Epoch: 94/200, Iteration: 4654/10000 --- Training Loss:0.266073\n",
      "Epoch: 94/200, Iteration: 4656/10000 --- Training Loss:0.493623\n",
      "Epoch: 94/200, Iteration: 4658/10000 --- Training Loss:0.375549\n",
      "Epoch: 94/200, Iteration: 4660/10000 --- Training Loss:1.314989\n",
      "Epoch: 94/200, Iteration: 4662/10000 --- Training Loss:0.387794\n",
      "Epoch: 94/200, Iteration: 4664/10000 --- Training Loss:0.500268\n",
      "Epoch: 94/200, Iteration: 4666/10000 --- Training Loss:0.433391\n",
      "Epoch: 94/200, Iteration: 4668/10000 --- Training Loss:0.806533\n",
      "Epoch: 94/200, Iteration: 4670/10000 --- Training Loss:0.839251\n",
      "Epoch: 94/200, Iteration: 4672/10000 --- Training Loss:0.504019\n",
      "Epoch: 94/200, Iteration: 4674/10000 --- Training Loss:0.530688\n",
      "Epoch: 94/200, Iteration: 4676/10000 --- Training Loss:0.712204\n",
      "Epoch: 94/200, Iteration: 4678/10000 --- Training Loss:0.493752\n",
      "Epoch: 94/200, Iteration: 4680/10000 --- Training Loss:0.390236\n",
      "Epoch: 94/200, Iteration: 4682/10000 --- Training Loss:0.228085\n",
      "Epoch: 94/200, Iteration: 4684/10000 --- Training Loss:0.515925\n",
      "Epoch: 94/200, Iteration: 4686/10000 --- Training Loss:0.320911\n",
      "Epoch: 94/200, Iteration: 4688/10000 --- Training Loss:0.429634\n",
      "Epoch: 94/200, Iteration: 4690/10000 --- Training Loss:0.595185\n",
      "Epoch: 94/200, Iteration: 4692/10000 --- Training Loss:0.334268\n",
      "Epoch: 94/200, Iteration: 4694/10000 --- Training Loss:0.579714\n",
      "Epoch: 94/200, Iteration: 4696/10000 --- Training Loss:1.048099\n",
      "Epoch: 94/200, Iteration: 4698/10000 --- Training Loss:0.378262\n",
      "Epoch: 94/200, Iteration: 4700/10000 --- Training Loss:0.142185\n",
      "Epoch: 94 finished ! Train Loss: 0.61596, Test Loss: 3.31680\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 95/200, Iteration: 4702/10000 --- Training Loss:0.563839\n",
      "Epoch: 95/200, Iteration: 4704/10000 --- Training Loss:0.826520\n",
      "Epoch: 95/200, Iteration: 4706/10000 --- Training Loss:0.277865\n",
      "Epoch: 95/200, Iteration: 4708/10000 --- Training Loss:0.465433\n",
      "Epoch: 95/200, Iteration: 4710/10000 --- Training Loss:0.264549\n",
      "Epoch: 95/200, Iteration: 4712/10000 --- Training Loss:0.374314\n",
      "Epoch: 95/200, Iteration: 4714/10000 --- Training Loss:0.418735\n",
      "Epoch: 95/200, Iteration: 4716/10000 --- Training Loss:0.552328\n",
      "Epoch: 95/200, Iteration: 4718/10000 --- Training Loss:0.278275\n",
      "Epoch: 95/200, Iteration: 4720/10000 --- Training Loss:0.740389\n",
      "Epoch: 95/200, Iteration: 4722/10000 --- Training Loss:0.254397\n",
      "Epoch: 95/200, Iteration: 4724/10000 --- Training Loss:1.268224\n",
      "Epoch: 95/200, Iteration: 4726/10000 --- Training Loss:0.382567\n",
      "Epoch: 95/200, Iteration: 4728/10000 --- Training Loss:0.923615\n",
      "Epoch: 95/200, Iteration: 4730/10000 --- Training Loss:0.324965\n",
      "Epoch: 95/200, Iteration: 4732/10000 --- Training Loss:0.799318\n",
      "Epoch: 95/200, Iteration: 4734/10000 --- Training Loss:0.369541\n",
      "Epoch: 95/200, Iteration: 4736/10000 --- Training Loss:0.472826\n",
      "Epoch: 95/200, Iteration: 4738/10000 --- Training Loss:0.484553\n",
      "Epoch: 95/200, Iteration: 4740/10000 --- Training Loss:1.272382\n",
      "Epoch: 95/200, Iteration: 4742/10000 --- Training Loss:0.212085\n",
      "Epoch: 95/200, Iteration: 4744/10000 --- Training Loss:1.664308\n",
      "Epoch: 95/200, Iteration: 4746/10000 --- Training Loss:2.522723\n",
      "Epoch: 95/200, Iteration: 4748/10000 --- Training Loss:0.566457\n",
      "Epoch: 95/200, Iteration: 4750/10000 --- Training Loss:2.274032\n",
      "Epoch: 95 finished ! Train Loss: 0.69989, Test Loss: 3.22679\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 96/200, Iteration: 4752/10000 --- Training Loss:0.551580\n",
      "Epoch: 96/200, Iteration: 4754/10000 --- Training Loss:1.158254\n",
      "Epoch: 96/200, Iteration: 4756/10000 --- Training Loss:0.493014\n",
      "Epoch: 96/200, Iteration: 4758/10000 --- Training Loss:0.336774\n",
      "Epoch: 96/200, Iteration: 4760/10000 --- Training Loss:0.631099\n",
      "Epoch: 96/200, Iteration: 4762/10000 --- Training Loss:0.776644\n",
      "Epoch: 96/200, Iteration: 4764/10000 --- Training Loss:0.873666\n",
      "Epoch: 96/200, Iteration: 4766/10000 --- Training Loss:0.469979\n",
      "Epoch: 96/200, Iteration: 4768/10000 --- Training Loss:0.174602\n",
      "Epoch: 96/200, Iteration: 4770/10000 --- Training Loss:0.461930\n",
      "Epoch: 96/200, Iteration: 4772/10000 --- Training Loss:0.280480\n",
      "Epoch: 96/200, Iteration: 4774/10000 --- Training Loss:0.310352\n",
      "Epoch: 96/200, Iteration: 4776/10000 --- Training Loss:0.473641\n",
      "Epoch: 96/200, Iteration: 4778/10000 --- Training Loss:1.299832\n",
      "Epoch: 96/200, Iteration: 4780/10000 --- Training Loss:0.360783\n",
      "Epoch: 96/200, Iteration: 4782/10000 --- Training Loss:1.045193\n",
      "Epoch: 96/200, Iteration: 4784/10000 --- Training Loss:0.364020\n",
      "Epoch: 96/200, Iteration: 4786/10000 --- Training Loss:0.816791\n",
      "Epoch: 96/200, Iteration: 4788/10000 --- Training Loss:0.239454\n",
      "Epoch: 96/200, Iteration: 4790/10000 --- Training Loss:0.514180\n",
      "Epoch: 96/200, Iteration: 4792/10000 --- Training Loss:0.226143\n",
      "Epoch: 96/200, Iteration: 4794/10000 --- Training Loss:0.486919\n",
      "Epoch: 96/200, Iteration: 4796/10000 --- Training Loss:0.738760\n",
      "Epoch: 96/200, Iteration: 4798/10000 --- Training Loss:0.287915\n",
      "Epoch: 96/200, Iteration: 4800/10000 --- Training Loss:0.478636\n",
      "Epoch: 96 finished ! Train Loss: 0.67095, Test Loss: 2.90583\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 97/200, Iteration: 4802/10000 --- Training Loss:0.121878\n",
      "Epoch: 97/200, Iteration: 4804/10000 --- Training Loss:0.639817\n",
      "Epoch: 97/200, Iteration: 4806/10000 --- Training Loss:0.615407\n",
      "Epoch: 97/200, Iteration: 4808/10000 --- Training Loss:1.543812\n",
      "Epoch: 97/200, Iteration: 4810/10000 --- Training Loss:0.194287\n",
      "Epoch: 97/200, Iteration: 4812/10000 --- Training Loss:0.763716\n",
      "Epoch: 97/200, Iteration: 4814/10000 --- Training Loss:0.423844\n",
      "Epoch: 97/200, Iteration: 4816/10000 --- Training Loss:1.712393\n",
      "Epoch: 97/200, Iteration: 4818/10000 --- Training Loss:0.643742\n",
      "Epoch: 97/200, Iteration: 4820/10000 --- Training Loss:1.886250\n",
      "Epoch: 97/200, Iteration: 4822/10000 --- Training Loss:0.756496\n",
      "Epoch: 97/200, Iteration: 4824/10000 --- Training Loss:0.554374\n",
      "Epoch: 97/200, Iteration: 4826/10000 --- Training Loss:1.336571\n",
      "Epoch: 97/200, Iteration: 4828/10000 --- Training Loss:0.406980\n",
      "Epoch: 97/200, Iteration: 4830/10000 --- Training Loss:2.391041\n",
      "Epoch: 97/200, Iteration: 4832/10000 --- Training Loss:0.846244\n",
      "Epoch: 97/200, Iteration: 4834/10000 --- Training Loss:0.794188\n",
      "Epoch: 97/200, Iteration: 4836/10000 --- Training Loss:0.325382\n",
      "Epoch: 97/200, Iteration: 4838/10000 --- Training Loss:0.843444\n",
      "Epoch: 97/200, Iteration: 4840/10000 --- Training Loss:0.456951\n",
      "Epoch: 97/200, Iteration: 4842/10000 --- Training Loss:1.712537\n",
      "Epoch: 97/200, Iteration: 4844/10000 --- Training Loss:1.896960\n",
      "Epoch: 97/200, Iteration: 4846/10000 --- Training Loss:0.373615\n",
      "Epoch: 97/200, Iteration: 4848/10000 --- Training Loss:1.790748\n",
      "Epoch: 97/200, Iteration: 4850/10000 --- Training Loss:1.759361\n",
      "Epoch: 97 finished ! Train Loss: 1.02066, Test Loss: 3.59451\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 98/200, Iteration: 4852/10000 --- Training Loss:1.772745\n",
      "Epoch: 98/200, Iteration: 4854/10000 --- Training Loss:0.827138\n",
      "Epoch: 98/200, Iteration: 4856/10000 --- Training Loss:0.674297\n",
      "Epoch: 98/200, Iteration: 4858/10000 --- Training Loss:1.657887\n",
      "Epoch: 98/200, Iteration: 4860/10000 --- Training Loss:0.610982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98/200, Iteration: 4862/10000 --- Training Loss:1.118143\n",
      "Epoch: 98/200, Iteration: 4864/10000 --- Training Loss:0.235227\n",
      "Epoch: 98/200, Iteration: 4866/10000 --- Training Loss:1.725521\n",
      "Epoch: 98/200, Iteration: 4868/10000 --- Training Loss:0.324983\n",
      "Epoch: 98/200, Iteration: 4870/10000 --- Training Loss:1.038089\n",
      "Epoch: 98/200, Iteration: 4872/10000 --- Training Loss:0.552039\n",
      "Epoch: 98/200, Iteration: 4874/10000 --- Training Loss:1.365698\n",
      "Epoch: 98/200, Iteration: 4876/10000 --- Training Loss:0.250921\n",
      "Epoch: 98/200, Iteration: 4878/10000 --- Training Loss:3.043817\n",
      "Epoch: 98/200, Iteration: 4880/10000 --- Training Loss:0.689805\n",
      "Epoch: 98/200, Iteration: 4882/10000 --- Training Loss:0.857955\n",
      "Epoch: 98/200, Iteration: 4884/10000 --- Training Loss:2.540752\n",
      "Epoch: 98/200, Iteration: 4886/10000 --- Training Loss:1.320504\n",
      "Epoch: 98/200, Iteration: 4888/10000 --- Training Loss:1.667697\n",
      "Epoch: 98/200, Iteration: 4890/10000 --- Training Loss:0.544399\n",
      "Epoch: 98/200, Iteration: 4892/10000 --- Training Loss:0.976378\n",
      "Epoch: 98/200, Iteration: 4894/10000 --- Training Loss:0.997994\n",
      "Epoch: 98/200, Iteration: 4896/10000 --- Training Loss:0.707168\n",
      "Epoch: 98/200, Iteration: 4898/10000 --- Training Loss:0.726146\n",
      "Epoch: 98/200, Iteration: 4900/10000 --- Training Loss:0.744300\n",
      "Epoch: 98 finished ! Train Loss: 1.04236, Test Loss: 2.97449\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 99/200, Iteration: 4902/10000 --- Training Loss:0.736017\n",
      "Epoch: 99/200, Iteration: 4904/10000 --- Training Loss:0.429567\n",
      "Epoch: 99/200, Iteration: 4906/10000 --- Training Loss:1.531139\n",
      "Epoch: 99/200, Iteration: 4908/10000 --- Training Loss:0.680379\n",
      "Epoch: 99/200, Iteration: 4910/10000 --- Training Loss:0.350124\n",
      "Epoch: 99/200, Iteration: 4912/10000 --- Training Loss:0.511802\n",
      "Epoch: 99/200, Iteration: 4914/10000 --- Training Loss:0.336618\n",
      "Epoch: 99/200, Iteration: 4916/10000 --- Training Loss:1.171193\n",
      "Epoch: 99/200, Iteration: 4918/10000 --- Training Loss:0.702467\n",
      "Epoch: 99/200, Iteration: 4920/10000 --- Training Loss:0.525700\n",
      "Epoch: 99/200, Iteration: 4922/10000 --- Training Loss:0.572230\n",
      "Epoch: 99/200, Iteration: 4924/10000 --- Training Loss:0.318708\n",
      "Epoch: 99/200, Iteration: 4926/10000 --- Training Loss:0.776693\n",
      "Epoch: 99/200, Iteration: 4928/10000 --- Training Loss:1.040544\n",
      "Epoch: 99/200, Iteration: 4930/10000 --- Training Loss:0.535673\n",
      "Epoch: 99/200, Iteration: 4932/10000 --- Training Loss:0.709931\n",
      "Epoch: 99/200, Iteration: 4934/10000 --- Training Loss:0.308486\n",
      "Epoch: 99/200, Iteration: 4936/10000 --- Training Loss:1.103944\n",
      "Epoch: 99/200, Iteration: 4938/10000 --- Training Loss:0.587282\n",
      "Epoch: 99/200, Iteration: 4940/10000 --- Training Loss:0.608138\n",
      "Epoch: 99/200, Iteration: 4942/10000 --- Training Loss:0.294137\n",
      "Epoch: 99/200, Iteration: 4944/10000 --- Training Loss:0.961923\n",
      "Epoch: 99/200, Iteration: 4946/10000 --- Training Loss:0.059099\n",
      "Epoch: 99/200, Iteration: 4948/10000 --- Training Loss:1.051460\n",
      "Epoch: 99/200, Iteration: 4950/10000 --- Training Loss:0.389660\n",
      "Epoch: 99 finished ! Train Loss: 0.66974, Test Loss: 3.34990\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 100/200, Iteration: 4952/10000 --- Training Loss:1.225072\n",
      "Epoch: 100/200, Iteration: 4954/10000 --- Training Loss:0.755406\n",
      "Epoch: 100/200, Iteration: 4956/10000 --- Training Loss:0.222809\n",
      "Epoch: 100/200, Iteration: 4958/10000 --- Training Loss:0.479858\n",
      "Epoch: 100/200, Iteration: 4960/10000 --- Training Loss:0.320850\n",
      "Epoch: 100/200, Iteration: 4962/10000 --- Training Loss:0.607618\n",
      "Epoch: 100/200, Iteration: 4964/10000 --- Training Loss:2.412207\n",
      "Epoch: 100/200, Iteration: 4966/10000 --- Training Loss:0.251049\n",
      "Epoch: 100/200, Iteration: 4968/10000 --- Training Loss:0.339305\n",
      "Epoch: 100/200, Iteration: 4970/10000 --- Training Loss:0.525564\n",
      "Epoch: 100/200, Iteration: 4972/10000 --- Training Loss:0.608853\n",
      "Epoch: 100/200, Iteration: 4974/10000 --- Training Loss:0.860622\n",
      "Epoch: 100/200, Iteration: 4976/10000 --- Training Loss:0.473100\n",
      "Epoch: 100/200, Iteration: 4978/10000 --- Training Loss:0.398579\n",
      "Epoch: 100/200, Iteration: 4980/10000 --- Training Loss:0.430788\n",
      "Epoch: 100/200, Iteration: 4982/10000 --- Training Loss:1.570000\n",
      "Epoch: 100/200, Iteration: 4984/10000 --- Training Loss:0.961777\n",
      "Epoch: 100/200, Iteration: 4986/10000 --- Training Loss:0.506796\n",
      "Epoch: 100/200, Iteration: 4988/10000 --- Training Loss:0.766953\n",
      "Epoch: 100/200, Iteration: 4990/10000 --- Training Loss:0.441470\n",
      "Epoch: 100/200, Iteration: 4992/10000 --- Training Loss:0.508438\n",
      "Epoch: 100/200, Iteration: 4994/10000 --- Training Loss:0.228529\n",
      "Epoch: 100/200, Iteration: 4996/10000 --- Training Loss:0.180970\n",
      "Epoch: 100/200, Iteration: 4998/10000 --- Training Loss:0.555907\n",
      "Epoch: 100/200, Iteration: 5000/10000 --- Training Loss:0.134969\n",
      "Epoch: 100 finished ! Train Loss: 0.60031, Test Loss: 2.90253\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 101/200, Iteration: 5002/10000 --- Training Loss:0.387662\n",
      "Epoch: 101/200, Iteration: 5004/10000 --- Training Loss:0.475319\n",
      "Epoch: 101/200, Iteration: 5006/10000 --- Training Loss:0.403450\n",
      "Epoch: 101/200, Iteration: 5008/10000 --- Training Loss:0.124636\n",
      "Epoch: 101/200, Iteration: 5010/10000 --- Training Loss:0.473565\n",
      "Epoch: 101/200, Iteration: 5012/10000 --- Training Loss:0.681198\n",
      "Epoch: 101/200, Iteration: 5014/10000 --- Training Loss:1.393665\n",
      "Epoch: 101/200, Iteration: 5016/10000 --- Training Loss:0.727957\n",
      "Epoch: 101/200, Iteration: 5018/10000 --- Training Loss:0.111162\n",
      "Epoch: 101/200, Iteration: 5020/10000 --- Training Loss:0.552888\n",
      "Epoch: 101/200, Iteration: 5022/10000 --- Training Loss:0.484324\n",
      "Epoch: 101/200, Iteration: 5024/10000 --- Training Loss:0.616689\n",
      "Epoch: 101/200, Iteration: 5026/10000 --- Training Loss:1.204442\n",
      "Epoch: 101/200, Iteration: 5028/10000 --- Training Loss:0.830585\n",
      "Epoch: 101/200, Iteration: 5030/10000 --- Training Loss:0.490593\n",
      "Epoch: 101/200, Iteration: 5032/10000 --- Training Loss:0.453427\n",
      "Epoch: 101/200, Iteration: 5034/10000 --- Training Loss:0.575392\n",
      "Epoch: 101/200, Iteration: 5036/10000 --- Training Loss:0.310939\n",
      "Epoch: 101/200, Iteration: 5038/10000 --- Training Loss:0.468970\n",
      "Epoch: 101/200, Iteration: 5040/10000 --- Training Loss:0.361243\n",
      "Epoch: 101/200, Iteration: 5042/10000 --- Training Loss:0.133103\n",
      "Epoch: 101/200, Iteration: 5044/10000 --- Training Loss:0.770547\n",
      "Epoch: 101/200, Iteration: 5046/10000 --- Training Loss:0.174443\n",
      "Epoch: 101/200, Iteration: 5048/10000 --- Training Loss:0.279200\n",
      "Epoch: 101/200, Iteration: 5050/10000 --- Training Loss:0.767877\n",
      "Epoch: 101 finished ! Train Loss: 0.64198, Test Loss: 2.67524\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 102/200, Iteration: 5052/10000 --- Training Loss:0.285738\n",
      "Epoch: 102/200, Iteration: 5054/10000 --- Training Loss:0.216156\n",
      "Epoch: 102/200, Iteration: 5056/10000 --- Training Loss:1.285663\n",
      "Epoch: 102/200, Iteration: 5058/10000 --- Training Loss:0.469232\n",
      "Epoch: 102/200, Iteration: 5060/10000 --- Training Loss:2.072009\n",
      "Epoch: 102/200, Iteration: 5062/10000 --- Training Loss:0.354841\n",
      "Epoch: 102/200, Iteration: 5064/10000 --- Training Loss:0.328142\n",
      "Epoch: 102/200, Iteration: 5066/10000 --- Training Loss:0.182903\n",
      "Epoch: 102/200, Iteration: 5068/10000 --- Training Loss:1.569977\n",
      "Epoch: 102/200, Iteration: 5070/10000 --- Training Loss:0.345984\n",
      "Epoch: 102/200, Iteration: 5072/10000 --- Training Loss:0.355995\n",
      "Epoch: 102/200, Iteration: 5074/10000 --- Training Loss:0.388575\n",
      "Epoch: 102/200, Iteration: 5076/10000 --- Training Loss:0.314679\n",
      "Epoch: 102/200, Iteration: 5078/10000 --- Training Loss:0.401365\n",
      "Epoch: 102/200, Iteration: 5080/10000 --- Training Loss:0.700048\n",
      "Epoch: 102/200, Iteration: 5082/10000 --- Training Loss:0.581743\n",
      "Epoch: 102/200, Iteration: 5084/10000 --- Training Loss:1.262464\n",
      "Epoch: 102/200, Iteration: 5086/10000 --- Training Loss:0.693927\n",
      "Epoch: 102/200, Iteration: 5088/10000 --- Training Loss:0.363108\n",
      "Epoch: 102/200, Iteration: 5090/10000 --- Training Loss:0.157359\n",
      "Epoch: 102/200, Iteration: 5092/10000 --- Training Loss:0.451261\n",
      "Epoch: 102/200, Iteration: 5094/10000 --- Training Loss:0.458152\n",
      "Epoch: 102/200, Iteration: 5096/10000 --- Training Loss:0.599471\n",
      "Epoch: 102/200, Iteration: 5098/10000 --- Training Loss:0.613693\n",
      "Epoch: 102/200, Iteration: 5100/10000 --- Training Loss:0.556090\n",
      "Epoch: 102 finished ! Train Loss: 0.62536, Test Loss: 2.74080\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 103/200, Iteration: 5102/10000 --- Training Loss:0.587580\n",
      "Epoch: 103/200, Iteration: 5104/10000 --- Training Loss:0.131692\n",
      "Epoch: 103/200, Iteration: 5106/10000 --- Training Loss:0.870166\n",
      "Epoch: 103/200, Iteration: 5108/10000 --- Training Loss:0.387912\n",
      "Epoch: 103/200, Iteration: 5110/10000 --- Training Loss:0.308114\n",
      "Epoch: 103/200, Iteration: 5112/10000 --- Training Loss:0.572430\n",
      "Epoch: 103/200, Iteration: 5114/10000 --- Training Loss:0.661053\n",
      "Epoch: 103/200, Iteration: 5116/10000 --- Training Loss:0.801770\n",
      "Epoch: 103/200, Iteration: 5118/10000 --- Training Loss:0.947631\n",
      "Epoch: 103/200, Iteration: 5120/10000 --- Training Loss:0.554756\n",
      "Epoch: 103/200, Iteration: 5122/10000 --- Training Loss:0.237729\n",
      "Epoch: 103/200, Iteration: 5124/10000 --- Training Loss:0.725861\n",
      "Epoch: 103/200, Iteration: 5126/10000 --- Training Loss:0.732418\n",
      "Epoch: 103/200, Iteration: 5128/10000 --- Training Loss:0.841508\n",
      "Epoch: 103/200, Iteration: 5130/10000 --- Training Loss:0.447927\n",
      "Epoch: 103/200, Iteration: 5132/10000 --- Training Loss:0.620272\n",
      "Epoch: 103/200, Iteration: 5134/10000 --- Training Loss:0.487534\n",
      "Epoch: 103/200, Iteration: 5136/10000 --- Training Loss:0.216438\n",
      "Epoch: 103/200, Iteration: 5138/10000 --- Training Loss:0.968222\n",
      "Epoch: 103/200, Iteration: 5140/10000 --- Training Loss:0.891955\n",
      "Epoch: 103/200, Iteration: 5142/10000 --- Training Loss:0.636633\n",
      "Epoch: 103/200, Iteration: 5144/10000 --- Training Loss:0.357760\n",
      "Epoch: 103/200, Iteration: 5146/10000 --- Training Loss:0.369798\n",
      "Epoch: 103/200, Iteration: 5148/10000 --- Training Loss:0.985081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103/200, Iteration: 5150/10000 --- Training Loss:0.656028\n",
      "Epoch: 103 finished ! Train Loss: 0.65194, Test Loss: 2.52350\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 104/200, Iteration: 5152/10000 --- Training Loss:0.709429\n",
      "Epoch: 104/200, Iteration: 5154/10000 --- Training Loss:0.462924\n",
      "Epoch: 104/200, Iteration: 5156/10000 --- Training Loss:0.654855\n",
      "Epoch: 104/200, Iteration: 5158/10000 --- Training Loss:0.599482\n",
      "Epoch: 104/200, Iteration: 5160/10000 --- Training Loss:0.955248\n",
      "Epoch: 104/200, Iteration: 5162/10000 --- Training Loss:1.119626\n",
      "Epoch: 104/200, Iteration: 5164/10000 --- Training Loss:0.275499\n",
      "Epoch: 104/200, Iteration: 5166/10000 --- Training Loss:1.457834\n",
      "Epoch: 104/200, Iteration: 5168/10000 --- Training Loss:0.944146\n",
      "Epoch: 104/200, Iteration: 5170/10000 --- Training Loss:0.340782\n",
      "Epoch: 104/200, Iteration: 5172/10000 --- Training Loss:0.490607\n",
      "Epoch: 104/200, Iteration: 5174/10000 --- Training Loss:0.938198\n",
      "Epoch: 104/200, Iteration: 5176/10000 --- Training Loss:0.840351\n",
      "Epoch: 104/200, Iteration: 5178/10000 --- Training Loss:0.251572\n",
      "Epoch: 104/200, Iteration: 5180/10000 --- Training Loss:0.533461\n",
      "Epoch: 104/200, Iteration: 5182/10000 --- Training Loss:0.778607\n",
      "Epoch: 104/200, Iteration: 5184/10000 --- Training Loss:1.389214\n",
      "Epoch: 104/200, Iteration: 5186/10000 --- Training Loss:1.089324\n",
      "Epoch: 104/200, Iteration: 5188/10000 --- Training Loss:0.717999\n",
      "Epoch: 104/200, Iteration: 5190/10000 --- Training Loss:0.637995\n",
      "Epoch: 104/200, Iteration: 5192/10000 --- Training Loss:0.490696\n",
      "Epoch: 104/200, Iteration: 5194/10000 --- Training Loss:0.553653\n",
      "Epoch: 104/200, Iteration: 5196/10000 --- Training Loss:2.479424\n",
      "Epoch: 104/200, Iteration: 5198/10000 --- Training Loss:0.648597\n",
      "Epoch: 104/200, Iteration: 5200/10000 --- Training Loss:0.480988\n",
      "Epoch: 104 finished ! Train Loss: 0.78233, Test Loss: 2.93079\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 105/200, Iteration: 5202/10000 --- Training Loss:0.562257\n",
      "Epoch: 105/200, Iteration: 5204/10000 --- Training Loss:1.056724\n",
      "Epoch: 105/200, Iteration: 5206/10000 --- Training Loss:0.317346\n",
      "Epoch: 105/200, Iteration: 5208/10000 --- Training Loss:1.381093\n",
      "Epoch: 105/200, Iteration: 5210/10000 --- Training Loss:1.676506\n",
      "Epoch: 105/200, Iteration: 5212/10000 --- Training Loss:0.682943\n",
      "Epoch: 105/200, Iteration: 5214/10000 --- Training Loss:0.296199\n",
      "Epoch: 105/200, Iteration: 5216/10000 --- Training Loss:0.474014\n",
      "Epoch: 105/200, Iteration: 5218/10000 --- Training Loss:0.570524\n",
      "Epoch: 105/200, Iteration: 5220/10000 --- Training Loss:0.652144\n",
      "Epoch: 105/200, Iteration: 5222/10000 --- Training Loss:0.112143\n",
      "Epoch: 105/200, Iteration: 5224/10000 --- Training Loss:0.278666\n",
      "Epoch: 105/200, Iteration: 5226/10000 --- Training Loss:0.601534\n",
      "Epoch: 105/200, Iteration: 5228/10000 --- Training Loss:0.556330\n",
      "Epoch: 105/200, Iteration: 5230/10000 --- Training Loss:1.032842\n",
      "Epoch: 105/200, Iteration: 5232/10000 --- Training Loss:0.899046\n",
      "Epoch: 105/200, Iteration: 5234/10000 --- Training Loss:0.474745\n",
      "Epoch: 105/200, Iteration: 5236/10000 --- Training Loss:0.355520\n",
      "Epoch: 105/200, Iteration: 5238/10000 --- Training Loss:0.497332\n",
      "Epoch: 105/200, Iteration: 5240/10000 --- Training Loss:0.724029\n",
      "Epoch: 105/200, Iteration: 5242/10000 --- Training Loss:0.668109\n",
      "Epoch: 105/200, Iteration: 5244/10000 --- Training Loss:0.642359\n",
      "Epoch: 105/200, Iteration: 5246/10000 --- Training Loss:0.174945\n",
      "Epoch: 105/200, Iteration: 5248/10000 --- Training Loss:0.380091\n",
      "Epoch: 105/200, Iteration: 5250/10000 --- Training Loss:2.259745\n",
      "Epoch: 105 finished ! Train Loss: 0.68314, Test Loss: 3.17629\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 106/200, Iteration: 5252/10000 --- Training Loss:0.613164\n",
      "Epoch: 106/200, Iteration: 5254/10000 --- Training Loss:0.673829\n",
      "Epoch: 106/200, Iteration: 5256/10000 --- Training Loss:0.880782\n",
      "Epoch: 106/200, Iteration: 5258/10000 --- Training Loss:0.278323\n",
      "Epoch: 106/200, Iteration: 5260/10000 --- Training Loss:2.262372\n",
      "Epoch: 106/200, Iteration: 5262/10000 --- Training Loss:0.533811\n",
      "Epoch: 106/200, Iteration: 5264/10000 --- Training Loss:0.517873\n",
      "Epoch: 106/200, Iteration: 5266/10000 --- Training Loss:0.583986\n",
      "Epoch: 106/200, Iteration: 5268/10000 --- Training Loss:0.925130\n",
      "Epoch: 106/200, Iteration: 5270/10000 --- Training Loss:0.768588\n",
      "Epoch: 106/200, Iteration: 5272/10000 --- Training Loss:2.382707\n",
      "Epoch: 106/200, Iteration: 5274/10000 --- Training Loss:1.603524\n",
      "Epoch: 106/200, Iteration: 5276/10000 --- Training Loss:1.146336\n",
      "Epoch: 106/200, Iteration: 5278/10000 --- Training Loss:0.344667\n",
      "Epoch: 106/200, Iteration: 5280/10000 --- Training Loss:1.812270\n",
      "Epoch: 106/200, Iteration: 5282/10000 --- Training Loss:0.795565\n",
      "Epoch: 106/200, Iteration: 5284/10000 --- Training Loss:0.853941\n",
      "Epoch: 106/200, Iteration: 5286/10000 --- Training Loss:0.974735\n",
      "Epoch: 106/200, Iteration: 5288/10000 --- Training Loss:0.466961\n",
      "Epoch: 106/200, Iteration: 5290/10000 --- Training Loss:0.824769\n",
      "Epoch: 106/200, Iteration: 5292/10000 --- Training Loss:0.618055\n",
      "Epoch: 106/200, Iteration: 5294/10000 --- Training Loss:0.524250\n",
      "Epoch: 106/200, Iteration: 5296/10000 --- Training Loss:0.319383\n",
      "Epoch: 106/200, Iteration: 5298/10000 --- Training Loss:0.669535\n",
      "Epoch: 106/200, Iteration: 5300/10000 --- Training Loss:0.359928\n",
      "Epoch: 106 finished ! Train Loss: 0.75377, Test Loss: 3.44492\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 107/200, Iteration: 5302/10000 --- Training Loss:0.880409\n",
      "Epoch: 107/200, Iteration: 5304/10000 --- Training Loss:1.061411\n",
      "Epoch: 107/200, Iteration: 5306/10000 --- Training Loss:0.177088\n",
      "Epoch: 107/200, Iteration: 5308/10000 --- Training Loss:0.592500\n",
      "Epoch: 107/200, Iteration: 5310/10000 --- Training Loss:0.241394\n",
      "Epoch: 107/200, Iteration: 5312/10000 --- Training Loss:0.551009\n",
      "Epoch: 107/200, Iteration: 5314/10000 --- Training Loss:0.458571\n",
      "Epoch: 107/200, Iteration: 5316/10000 --- Training Loss:0.518308\n",
      "Epoch: 107/200, Iteration: 5318/10000 --- Training Loss:0.484633\n",
      "Epoch: 107/200, Iteration: 5320/10000 --- Training Loss:0.303802\n",
      "Epoch: 107/200, Iteration: 5322/10000 --- Training Loss:0.923836\n",
      "Epoch: 107/200, Iteration: 5324/10000 --- Training Loss:0.493713\n",
      "Epoch: 107/200, Iteration: 5326/10000 --- Training Loss:0.730535\n",
      "Epoch: 107/200, Iteration: 5328/10000 --- Training Loss:0.388948\n",
      "Epoch: 107/200, Iteration: 5330/10000 --- Training Loss:0.122200\n",
      "Epoch: 107/200, Iteration: 5332/10000 --- Training Loss:0.222957\n",
      "Epoch: 107/200, Iteration: 5334/10000 --- Training Loss:0.274590\n",
      "Epoch: 107/200, Iteration: 5336/10000 --- Training Loss:0.841855\n",
      "Epoch: 107/200, Iteration: 5338/10000 --- Training Loss:0.692076\n",
      "Epoch: 107/200, Iteration: 5340/10000 --- Training Loss:1.133873\n",
      "Epoch: 107/200, Iteration: 5342/10000 --- Training Loss:0.594357\n",
      "Epoch: 107/200, Iteration: 5344/10000 --- Training Loss:0.288594\n",
      "Epoch: 107/200, Iteration: 5346/10000 --- Training Loss:0.293685\n",
      "Epoch: 107/200, Iteration: 5348/10000 --- Training Loss:0.434568\n",
      "Epoch: 107/200, Iteration: 5350/10000 --- Training Loss:0.517789\n",
      "Epoch: 107 finished ! Train Loss: 0.62499, Test Loss: 2.97022\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 108/200, Iteration: 5352/10000 --- Training Loss:1.381482\n",
      "Epoch: 108/200, Iteration: 5354/10000 --- Training Loss:0.908425\n",
      "Epoch: 108/200, Iteration: 5356/10000 --- Training Loss:0.308595\n",
      "Epoch: 108/200, Iteration: 5358/10000 --- Training Loss:0.838365\n",
      "Epoch: 108/200, Iteration: 5360/10000 --- Training Loss:0.696455\n",
      "Epoch: 108/200, Iteration: 5362/10000 --- Training Loss:0.463287\n",
      "Epoch: 108/200, Iteration: 5364/10000 --- Training Loss:0.232585\n",
      "Epoch: 108/200, Iteration: 5366/10000 --- Training Loss:0.187939\n",
      "Epoch: 108/200, Iteration: 5368/10000 --- Training Loss:0.468079\n",
      "Epoch: 108/200, Iteration: 5370/10000 --- Training Loss:0.208040\n",
      "Epoch: 108/200, Iteration: 5372/10000 --- Training Loss:0.475879\n",
      "Epoch: 108/200, Iteration: 5374/10000 --- Training Loss:0.695617\n",
      "Epoch: 108/200, Iteration: 5376/10000 --- Training Loss:0.287685\n",
      "Epoch: 108/200, Iteration: 5378/10000 --- Training Loss:0.582462\n",
      "Epoch: 108/200, Iteration: 5380/10000 --- Training Loss:0.719793\n",
      "Epoch: 108/200, Iteration: 5382/10000 --- Training Loss:0.430318\n",
      "Epoch: 108/200, Iteration: 5384/10000 --- Training Loss:0.448166\n",
      "Epoch: 108/200, Iteration: 5386/10000 --- Training Loss:0.202443\n",
      "Epoch: 108/200, Iteration: 5388/10000 --- Training Loss:0.330419\n",
      "Epoch: 108/200, Iteration: 5390/10000 --- Training Loss:0.838707\n",
      "Epoch: 108/200, Iteration: 5392/10000 --- Training Loss:1.292320\n",
      "Epoch: 108/200, Iteration: 5394/10000 --- Training Loss:0.873539\n",
      "Epoch: 108/200, Iteration: 5396/10000 --- Training Loss:0.480565\n",
      "Epoch: 108/200, Iteration: 5398/10000 --- Training Loss:0.270655\n",
      "Epoch: 108/200, Iteration: 5400/10000 --- Training Loss:0.539029\n",
      "Epoch: 108 finished ! Train Loss: 0.56846, Test Loss: 3.08460\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 109/200, Iteration: 5402/10000 --- Training Loss:0.554973\n",
      "Epoch: 109/200, Iteration: 5404/10000 --- Training Loss:0.363970\n",
      "Epoch: 109/200, Iteration: 5406/10000 --- Training Loss:0.213194\n",
      "Epoch: 109/200, Iteration: 5408/10000 --- Training Loss:0.585151\n",
      "Epoch: 109/200, Iteration: 5410/10000 --- Training Loss:0.186911\n",
      "Epoch: 109/200, Iteration: 5412/10000 --- Training Loss:0.246116\n",
      "Epoch: 109/200, Iteration: 5414/10000 --- Training Loss:0.212954\n",
      "Epoch: 109/200, Iteration: 5416/10000 --- Training Loss:0.442517\n",
      "Epoch: 109/200, Iteration: 5418/10000 --- Training Loss:0.516877\n",
      "Epoch: 109/200, Iteration: 5420/10000 --- Training Loss:0.421988\n",
      "Epoch: 109/200, Iteration: 5422/10000 --- Training Loss:0.400399\n",
      "Epoch: 109/200, Iteration: 5424/10000 --- Training Loss:0.348716\n",
      "Epoch: 109/200, Iteration: 5426/10000 --- Training Loss:0.325768\n",
      "Epoch: 109/200, Iteration: 5428/10000 --- Training Loss:0.564320\n",
      "Epoch: 109/200, Iteration: 5430/10000 --- Training Loss:1.540376\n",
      "Epoch: 109/200, Iteration: 5432/10000 --- Training Loss:0.895845\n",
      "Epoch: 109/200, Iteration: 5434/10000 --- Training Loss:1.377207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109/200, Iteration: 5436/10000 --- Training Loss:0.848962\n",
      "Epoch: 109/200, Iteration: 5438/10000 --- Training Loss:0.361709\n",
      "Epoch: 109/200, Iteration: 5440/10000 --- Training Loss:0.793318\n",
      "Epoch: 109/200, Iteration: 5442/10000 --- Training Loss:0.895550\n",
      "Epoch: 109/200, Iteration: 5444/10000 --- Training Loss:0.613231\n",
      "Epoch: 109/200, Iteration: 5446/10000 --- Training Loss:0.357155\n",
      "Epoch: 109/200, Iteration: 5448/10000 --- Training Loss:0.394975\n",
      "Epoch: 109/200, Iteration: 5450/10000 --- Training Loss:1.838097\n",
      "Epoch: 109 finished ! Train Loss: 0.60284, Test Loss: 2.38830\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 110/200, Iteration: 5452/10000 --- Training Loss:0.366197\n",
      "Epoch: 110/200, Iteration: 5454/10000 --- Training Loss:0.342540\n",
      "Epoch: 110/200, Iteration: 5456/10000 --- Training Loss:0.529340\n",
      "Epoch: 110/200, Iteration: 5458/10000 --- Training Loss:0.504828\n",
      "Epoch: 110/200, Iteration: 5460/10000 --- Training Loss:0.573826\n",
      "Epoch: 110/200, Iteration: 5462/10000 --- Training Loss:0.240933\n",
      "Epoch: 110/200, Iteration: 5464/10000 --- Training Loss:0.703344\n",
      "Epoch: 110/200, Iteration: 5466/10000 --- Training Loss:0.351536\n",
      "Epoch: 110/200, Iteration: 5468/10000 --- Training Loss:0.428122\n",
      "Epoch: 110/200, Iteration: 5470/10000 --- Training Loss:0.480885\n",
      "Epoch: 110/200, Iteration: 5472/10000 --- Training Loss:0.760960\n",
      "Epoch: 110/200, Iteration: 5474/10000 --- Training Loss:0.218766\n",
      "Epoch: 110/200, Iteration: 5476/10000 --- Training Loss:1.066206\n",
      "Epoch: 110/200, Iteration: 5478/10000 --- Training Loss:0.343109\n",
      "Epoch: 110/200, Iteration: 5480/10000 --- Training Loss:0.417877\n",
      "Epoch: 110/200, Iteration: 5482/10000 --- Training Loss:3.388648\n",
      "Epoch: 110/200, Iteration: 5484/10000 --- Training Loss:1.765530\n",
      "Epoch: 110/200, Iteration: 5486/10000 --- Training Loss:1.124296\n",
      "Epoch: 110/200, Iteration: 5488/10000 --- Training Loss:0.462754\n",
      "Epoch: 110/200, Iteration: 5490/10000 --- Training Loss:1.474720\n",
      "Epoch: 110/200, Iteration: 5492/10000 --- Training Loss:0.205845\n",
      "Epoch: 110/200, Iteration: 5494/10000 --- Training Loss:0.471474\n",
      "Epoch: 110/200, Iteration: 5496/10000 --- Training Loss:1.162280\n",
      "Epoch: 110/200, Iteration: 5498/10000 --- Training Loss:0.572272\n",
      "Epoch: 110/200, Iteration: 5500/10000 --- Training Loss:0.736528\n",
      "Epoch: 110 finished ! Train Loss: 0.70053, Test Loss: 3.20585\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 111/200, Iteration: 5502/10000 --- Training Loss:0.376183\n",
      "Epoch: 111/200, Iteration: 5504/10000 --- Training Loss:1.012841\n",
      "Epoch: 111/200, Iteration: 5506/10000 --- Training Loss:0.537600\n",
      "Epoch: 111/200, Iteration: 5508/10000 --- Training Loss:0.826951\n",
      "Epoch: 111/200, Iteration: 5510/10000 --- Training Loss:0.299345\n",
      "Epoch: 111/200, Iteration: 5512/10000 --- Training Loss:0.736655\n",
      "Epoch: 111/200, Iteration: 5514/10000 --- Training Loss:0.129115\n",
      "Epoch: 111/200, Iteration: 5516/10000 --- Training Loss:1.026085\n",
      "Epoch: 111/200, Iteration: 5518/10000 --- Training Loss:0.632609\n",
      "Epoch: 111/200, Iteration: 5520/10000 --- Training Loss:0.688717\n",
      "Epoch: 111/200, Iteration: 5522/10000 --- Training Loss:0.501803\n",
      "Epoch: 111/200, Iteration: 5524/10000 --- Training Loss:0.459082\n",
      "Epoch: 111/200, Iteration: 5526/10000 --- Training Loss:0.511080\n",
      "Epoch: 111/200, Iteration: 5528/10000 --- Training Loss:0.491821\n",
      "Epoch: 111/200, Iteration: 5530/10000 --- Training Loss:0.513114\n",
      "Epoch: 111/200, Iteration: 5532/10000 --- Training Loss:0.461985\n",
      "Epoch: 111/200, Iteration: 5534/10000 --- Training Loss:0.310632\n",
      "Epoch: 111/200, Iteration: 5536/10000 --- Training Loss:1.924647\n",
      "Epoch: 111/200, Iteration: 5538/10000 --- Training Loss:0.337983\n",
      "Epoch: 111/200, Iteration: 5540/10000 --- Training Loss:1.602784\n",
      "Epoch: 111/200, Iteration: 5542/10000 --- Training Loss:0.613718\n",
      "Epoch: 111/200, Iteration: 5544/10000 --- Training Loss:0.644693\n",
      "Epoch: 111/200, Iteration: 5546/10000 --- Training Loss:0.317585\n",
      "Epoch: 111/200, Iteration: 5548/10000 --- Training Loss:0.554937\n",
      "Epoch: 111/200, Iteration: 5550/10000 --- Training Loss:0.621833\n",
      "Epoch: 111 finished ! Train Loss: 0.58149, Test Loss: 3.12617\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 112/200, Iteration: 5552/10000 --- Training Loss:0.393144\n",
      "Epoch: 112/200, Iteration: 5554/10000 --- Training Loss:1.438091\n",
      "Epoch: 112/200, Iteration: 5556/10000 --- Training Loss:0.373924\n",
      "Epoch: 112/200, Iteration: 5558/10000 --- Training Loss:0.443180\n",
      "Epoch: 112/200, Iteration: 5560/10000 --- Training Loss:0.783440\n",
      "Epoch: 112/200, Iteration: 5562/10000 --- Training Loss:0.469558\n",
      "Epoch: 112/200, Iteration: 5564/10000 --- Training Loss:0.441978\n",
      "Epoch: 112/200, Iteration: 5566/10000 --- Training Loss:0.344914\n",
      "Epoch: 112/200, Iteration: 5568/10000 --- Training Loss:1.043883\n",
      "Epoch: 112/200, Iteration: 5570/10000 --- Training Loss:0.548126\n",
      "Epoch: 112/200, Iteration: 5572/10000 --- Training Loss:0.994531\n",
      "Epoch: 112/200, Iteration: 5574/10000 --- Training Loss:0.897077\n",
      "Epoch: 112/200, Iteration: 5576/10000 --- Training Loss:0.622111\n",
      "Epoch: 112/200, Iteration: 5578/10000 --- Training Loss:0.426252\n",
      "Epoch: 112/200, Iteration: 5580/10000 --- Training Loss:0.620465\n",
      "Epoch: 112/200, Iteration: 5582/10000 --- Training Loss:0.506648\n",
      "Epoch: 112/200, Iteration: 5584/10000 --- Training Loss:0.623374\n",
      "Epoch: 112/200, Iteration: 5586/10000 --- Training Loss:0.429399\n",
      "Epoch: 112/200, Iteration: 5588/10000 --- Training Loss:0.142141\n",
      "Epoch: 112/200, Iteration: 5590/10000 --- Training Loss:0.864902\n",
      "Epoch: 112/200, Iteration: 5592/10000 --- Training Loss:0.247100\n",
      "Epoch: 112/200, Iteration: 5594/10000 --- Training Loss:0.488397\n",
      "Epoch: 112/200, Iteration: 5596/10000 --- Training Loss:0.597152\n",
      "Epoch: 112/200, Iteration: 5598/10000 --- Training Loss:0.248277\n",
      "Epoch: 112/200, Iteration: 5600/10000 --- Training Loss:0.302896\n",
      "Epoch: 112 finished ! Train Loss: 0.68136, Test Loss: 3.00950\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 113/200, Iteration: 5602/10000 --- Training Loss:0.238538\n",
      "Epoch: 113/200, Iteration: 5604/10000 --- Training Loss:0.387168\n",
      "Epoch: 113/200, Iteration: 5606/10000 --- Training Loss:0.307187\n",
      "Epoch: 113/200, Iteration: 5608/10000 --- Training Loss:0.681439\n",
      "Epoch: 113/200, Iteration: 5610/10000 --- Training Loss:0.427311\n",
      "Epoch: 113/200, Iteration: 5612/10000 --- Training Loss:0.331252\n",
      "Epoch: 113/200, Iteration: 5614/10000 --- Training Loss:0.191663\n",
      "Epoch: 113/200, Iteration: 5616/10000 --- Training Loss:0.578692\n",
      "Epoch: 113/200, Iteration: 5618/10000 --- Training Loss:0.349947\n",
      "Epoch: 113/200, Iteration: 5620/10000 --- Training Loss:0.487447\n",
      "Epoch: 113/200, Iteration: 5622/10000 --- Training Loss:0.511532\n",
      "Epoch: 113/200, Iteration: 5624/10000 --- Training Loss:0.426746\n",
      "Epoch: 113/200, Iteration: 5626/10000 --- Training Loss:1.824605\n",
      "Epoch: 113/200, Iteration: 5628/10000 --- Training Loss:0.397930\n",
      "Epoch: 113/200, Iteration: 5630/10000 --- Training Loss:0.483080\n",
      "Epoch: 113/200, Iteration: 5632/10000 --- Training Loss:0.132686\n",
      "Epoch: 113/200, Iteration: 5634/10000 --- Training Loss:0.534812\n",
      "Epoch: 113/200, Iteration: 5636/10000 --- Training Loss:0.911264\n",
      "Epoch: 113/200, Iteration: 5638/10000 --- Training Loss:0.610403\n",
      "Epoch: 113/200, Iteration: 5640/10000 --- Training Loss:0.353817\n",
      "Epoch: 113/200, Iteration: 5642/10000 --- Training Loss:0.567890\n",
      "Epoch: 113/200, Iteration: 5644/10000 --- Training Loss:0.583431\n",
      "Epoch: 113/200, Iteration: 5646/10000 --- Training Loss:1.002820\n",
      "Epoch: 113/200, Iteration: 5648/10000 --- Training Loss:1.293380\n",
      "Epoch: 113/200, Iteration: 5650/10000 --- Training Loss:0.318705\n",
      "Epoch: 113 finished ! Train Loss: 0.56877, Test Loss: 2.97616\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 114/200, Iteration: 5652/10000 --- Training Loss:0.279059\n",
      "Epoch: 114/200, Iteration: 5654/10000 --- Training Loss:0.232223\n",
      "Epoch: 114/200, Iteration: 5656/10000 --- Training Loss:0.295486\n",
      "Epoch: 114/200, Iteration: 5658/10000 --- Training Loss:1.766772\n",
      "Epoch: 114/200, Iteration: 5660/10000 --- Training Loss:0.179676\n",
      "Epoch: 114/200, Iteration: 5662/10000 --- Training Loss:1.296914\n",
      "Epoch: 114/200, Iteration: 5664/10000 --- Training Loss:0.933684\n",
      "Epoch: 114/200, Iteration: 5666/10000 --- Training Loss:0.839698\n",
      "Epoch: 114/200, Iteration: 5668/10000 --- Training Loss:0.706034\n",
      "Epoch: 114/200, Iteration: 5670/10000 --- Training Loss:0.640563\n",
      "Epoch: 114/200, Iteration: 5672/10000 --- Training Loss:0.912146\n",
      "Epoch: 114/200, Iteration: 5674/10000 --- Training Loss:0.800227\n",
      "Epoch: 114/200, Iteration: 5676/10000 --- Training Loss:0.932979\n",
      "Epoch: 114/200, Iteration: 5678/10000 --- Training Loss:0.363546\n",
      "Epoch: 114/200, Iteration: 5680/10000 --- Training Loss:1.339328\n",
      "Epoch: 114/200, Iteration: 5682/10000 --- Training Loss:0.280292\n",
      "Epoch: 114/200, Iteration: 5684/10000 --- Training Loss:0.576027\n",
      "Epoch: 114/200, Iteration: 5686/10000 --- Training Loss:0.490660\n",
      "Epoch: 114/200, Iteration: 5688/10000 --- Training Loss:0.448551\n",
      "Epoch: 114/200, Iteration: 5690/10000 --- Training Loss:0.897094\n",
      "Epoch: 114/200, Iteration: 5692/10000 --- Training Loss:0.171996\n",
      "Epoch: 114/200, Iteration: 5694/10000 --- Training Loss:0.625079\n",
      "Epoch: 114/200, Iteration: 5696/10000 --- Training Loss:0.487218\n",
      "Epoch: 114/200, Iteration: 5698/10000 --- Training Loss:0.483777\n",
      "Epoch: 114/200, Iteration: 5700/10000 --- Training Loss:0.648886\n",
      "Epoch: 114 finished ! Train Loss: 0.58438, Test Loss: 2.35421\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 115/200, Iteration: 5702/10000 --- Training Loss:0.662548\n",
      "Epoch: 115/200, Iteration: 5704/10000 --- Training Loss:0.533270\n",
      "Epoch: 115/200, Iteration: 5706/10000 --- Training Loss:0.474083\n",
      "Epoch: 115/200, Iteration: 5708/10000 --- Training Loss:0.276497\n",
      "Epoch: 115/200, Iteration: 5710/10000 --- Training Loss:0.238524\n",
      "Epoch: 115/200, Iteration: 5712/10000 --- Training Loss:0.256035\n",
      "Epoch: 115/200, Iteration: 5714/10000 --- Training Loss:0.617342\n",
      "Epoch: 115/200, Iteration: 5716/10000 --- Training Loss:0.538154\n",
      "Epoch: 115/200, Iteration: 5718/10000 --- Training Loss:0.599290\n",
      "Epoch: 115/200, Iteration: 5720/10000 --- Training Loss:0.225123\n",
      "Epoch: 115/200, Iteration: 5722/10000 --- Training Loss:1.527090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 115/200, Iteration: 5724/10000 --- Training Loss:0.271458\n",
      "Epoch: 115/200, Iteration: 5726/10000 --- Training Loss:0.443091\n",
      "Epoch: 115/200, Iteration: 5728/10000 --- Training Loss:0.652250\n",
      "Epoch: 115/200, Iteration: 5730/10000 --- Training Loss:0.214178\n",
      "Epoch: 115/200, Iteration: 5732/10000 --- Training Loss:0.264940\n",
      "Epoch: 115/200, Iteration: 5734/10000 --- Training Loss:0.176702\n",
      "Epoch: 115/200, Iteration: 5736/10000 --- Training Loss:0.697369\n",
      "Epoch: 115/200, Iteration: 5738/10000 --- Training Loss:0.927565\n",
      "Epoch: 115/200, Iteration: 5740/10000 --- Training Loss:1.454717\n",
      "Epoch: 115/200, Iteration: 5742/10000 --- Training Loss:1.329859\n",
      "Epoch: 115/200, Iteration: 5744/10000 --- Training Loss:0.729849\n",
      "Epoch: 115/200, Iteration: 5746/10000 --- Training Loss:0.718827\n",
      "Epoch: 115/200, Iteration: 5748/10000 --- Training Loss:0.288636\n",
      "Epoch: 115/200, Iteration: 5750/10000 --- Training Loss:1.749783\n",
      "Epoch: 115 finished ! Train Loss: 0.55976, Test Loss: 2.74131\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 116/200, Iteration: 5752/10000 --- Training Loss:0.482241\n",
      "Epoch: 116/200, Iteration: 5754/10000 --- Training Loss:0.582018\n",
      "Epoch: 116/200, Iteration: 5756/10000 --- Training Loss:0.970564\n",
      "Epoch: 116/200, Iteration: 5758/10000 --- Training Loss:0.781073\n",
      "Epoch: 116/200, Iteration: 5760/10000 --- Training Loss:0.434506\n",
      "Epoch: 116/200, Iteration: 5762/10000 --- Training Loss:0.692507\n",
      "Epoch: 116/200, Iteration: 5764/10000 --- Training Loss:1.747947\n",
      "Epoch: 116/200, Iteration: 5766/10000 --- Training Loss:0.540874\n",
      "Epoch: 116/200, Iteration: 5768/10000 --- Training Loss:0.753218\n",
      "Epoch: 116/200, Iteration: 5770/10000 --- Training Loss:0.391978\n",
      "Epoch: 116/200, Iteration: 5772/10000 --- Training Loss:0.517977\n",
      "Epoch: 116/200, Iteration: 5774/10000 --- Training Loss:0.739250\n",
      "Epoch: 116/200, Iteration: 5776/10000 --- Training Loss:0.276967\n",
      "Epoch: 116/200, Iteration: 5778/10000 --- Training Loss:0.144007\n",
      "Epoch: 116/200, Iteration: 5780/10000 --- Training Loss:0.217005\n",
      "Epoch: 116/200, Iteration: 5782/10000 --- Training Loss:0.382061\n",
      "Epoch: 116/200, Iteration: 5784/10000 --- Training Loss:0.457026\n",
      "Epoch: 116/200, Iteration: 5786/10000 --- Training Loss:0.211224\n",
      "Epoch: 116/200, Iteration: 5788/10000 --- Training Loss:0.270619\n",
      "Epoch: 116/200, Iteration: 5790/10000 --- Training Loss:0.436082\n",
      "Epoch: 116/200, Iteration: 5792/10000 --- Training Loss:0.800730\n",
      "Epoch: 116/200, Iteration: 5794/10000 --- Training Loss:0.315953\n",
      "Epoch: 116/200, Iteration: 5796/10000 --- Training Loss:0.409914\n",
      "Epoch: 116/200, Iteration: 5798/10000 --- Training Loss:0.501103\n",
      "Epoch: 116/200, Iteration: 5800/10000 --- Training Loss:0.387719\n",
      "Epoch: 116 finished ! Train Loss: 0.53789, Test Loss: 2.78879\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 117/200, Iteration: 5802/10000 --- Training Loss:0.452586\n",
      "Epoch: 117/200, Iteration: 5804/10000 --- Training Loss:0.361616\n",
      "Epoch: 117/200, Iteration: 5806/10000 --- Training Loss:0.515691\n",
      "Epoch: 117/200, Iteration: 5808/10000 --- Training Loss:0.473459\n",
      "Epoch: 117/200, Iteration: 5810/10000 --- Training Loss:0.992910\n",
      "Epoch: 117/200, Iteration: 5812/10000 --- Training Loss:0.505762\n",
      "Epoch: 117/200, Iteration: 5814/10000 --- Training Loss:0.949040\n",
      "Epoch: 117/200, Iteration: 5816/10000 --- Training Loss:0.826254\n",
      "Epoch: 117/200, Iteration: 5818/10000 --- Training Loss:0.430382\n",
      "Epoch: 117/200, Iteration: 5820/10000 --- Training Loss:1.498207\n",
      "Epoch: 117/200, Iteration: 5822/10000 --- Training Loss:1.015623\n",
      "Epoch: 117/200, Iteration: 5824/10000 --- Training Loss:0.621184\n",
      "Epoch: 117/200, Iteration: 5826/10000 --- Training Loss:0.219775\n",
      "Epoch: 117/200, Iteration: 5828/10000 --- Training Loss:0.518681\n",
      "Epoch: 117/200, Iteration: 5830/10000 --- Training Loss:0.324895\n",
      "Epoch: 117/200, Iteration: 5832/10000 --- Training Loss:0.546295\n",
      "Epoch: 117/200, Iteration: 5834/10000 --- Training Loss:0.570569\n",
      "Epoch: 117/200, Iteration: 5836/10000 --- Training Loss:0.237990\n",
      "Epoch: 117/200, Iteration: 5838/10000 --- Training Loss:0.233997\n",
      "Epoch: 117/200, Iteration: 5840/10000 --- Training Loss:0.573499\n",
      "Epoch: 117/200, Iteration: 5842/10000 --- Training Loss:0.758114\n",
      "Epoch: 117/200, Iteration: 5844/10000 --- Training Loss:0.949988\n",
      "Epoch: 117/200, Iteration: 5846/10000 --- Training Loss:0.667108\n",
      "Epoch: 117/200, Iteration: 5848/10000 --- Training Loss:0.146124\n",
      "Epoch: 117/200, Iteration: 5850/10000 --- Training Loss:0.654580\n",
      "Epoch: 117 finished ! Train Loss: 0.70022, Test Loss: 2.58752\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 118/200, Iteration: 5852/10000 --- Training Loss:0.279126\n",
      "Epoch: 118/200, Iteration: 5854/10000 --- Training Loss:0.883236\n",
      "Epoch: 118/200, Iteration: 5856/10000 --- Training Loss:0.786892\n",
      "Epoch: 118/200, Iteration: 5858/10000 --- Training Loss:0.213721\n",
      "Epoch: 118/200, Iteration: 5860/10000 --- Training Loss:0.442116\n",
      "Epoch: 118/200, Iteration: 5862/10000 --- Training Loss:0.297778\n",
      "Epoch: 118/200, Iteration: 5864/10000 --- Training Loss:0.296810\n",
      "Epoch: 118/200, Iteration: 5866/10000 --- Training Loss:0.282740\n",
      "Epoch: 118/200, Iteration: 5868/10000 --- Training Loss:0.210264\n",
      "Epoch: 118/200, Iteration: 5870/10000 --- Training Loss:0.263363\n",
      "Epoch: 118/200, Iteration: 5872/10000 --- Training Loss:0.765499\n",
      "Epoch: 118/200, Iteration: 5874/10000 --- Training Loss:0.228068\n",
      "Epoch: 118/200, Iteration: 5876/10000 --- Training Loss:1.402405\n",
      "Epoch: 118/200, Iteration: 5878/10000 --- Training Loss:0.444780\n",
      "Epoch: 118/200, Iteration: 5880/10000 --- Training Loss:2.141042\n",
      "Epoch: 118/200, Iteration: 5882/10000 --- Training Loss:0.301894\n",
      "Epoch: 118/200, Iteration: 5884/10000 --- Training Loss:1.277178\n",
      "Epoch: 118/200, Iteration: 5886/10000 --- Training Loss:0.307604\n",
      "Epoch: 118/200, Iteration: 5888/10000 --- Training Loss:0.417482\n",
      "Epoch: 118/200, Iteration: 5890/10000 --- Training Loss:0.755672\n",
      "Epoch: 118/200, Iteration: 5892/10000 --- Training Loss:0.297891\n",
      "Epoch: 118/200, Iteration: 5894/10000 --- Training Loss:0.995725\n",
      "Epoch: 118/200, Iteration: 5896/10000 --- Training Loss:0.458939\n",
      "Epoch: 118/200, Iteration: 5898/10000 --- Training Loss:0.270669\n",
      "Epoch: 118/200, Iteration: 5900/10000 --- Training Loss:0.487501\n",
      "Epoch: 118 finished ! Train Loss: 0.65253, Test Loss: 2.80805\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 119/200, Iteration: 5902/10000 --- Training Loss:0.689892\n",
      "Epoch: 119/200, Iteration: 5904/10000 --- Training Loss:0.635707\n",
      "Epoch: 119/200, Iteration: 5906/10000 --- Training Loss:0.512195\n",
      "Epoch: 119/200, Iteration: 5908/10000 --- Training Loss:0.287048\n",
      "Epoch: 119/200, Iteration: 5910/10000 --- Training Loss:0.251974\n",
      "Epoch: 119/200, Iteration: 5912/10000 --- Training Loss:0.433012\n",
      "Epoch: 119/200, Iteration: 5914/10000 --- Training Loss:0.536361\n",
      "Epoch: 119/200, Iteration: 5916/10000 --- Training Loss:0.325066\n",
      "Epoch: 119/200, Iteration: 5918/10000 --- Training Loss:0.444232\n",
      "Epoch: 119/200, Iteration: 5920/10000 --- Training Loss:0.471626\n",
      "Epoch: 119/200, Iteration: 5922/10000 --- Training Loss:0.958715\n",
      "Epoch: 119/200, Iteration: 5924/10000 --- Training Loss:0.291818\n",
      "Epoch: 119/200, Iteration: 5926/10000 --- Training Loss:0.491048\n",
      "Epoch: 119/200, Iteration: 5928/10000 --- Training Loss:0.330591\n",
      "Epoch: 119/200, Iteration: 5930/10000 --- Training Loss:0.580850\n",
      "Epoch: 119/200, Iteration: 5932/10000 --- Training Loss:0.345119\n",
      "Epoch: 119/200, Iteration: 5934/10000 --- Training Loss:0.844605\n",
      "Epoch: 119/200, Iteration: 5936/10000 --- Training Loss:0.642201\n",
      "Epoch: 119/200, Iteration: 5938/10000 --- Training Loss:0.284995\n",
      "Epoch: 119/200, Iteration: 5940/10000 --- Training Loss:1.066105\n",
      "Epoch: 119/200, Iteration: 5942/10000 --- Training Loss:1.105243\n",
      "Epoch: 119/200, Iteration: 5944/10000 --- Training Loss:0.842705\n",
      "Epoch: 119/200, Iteration: 5946/10000 --- Training Loss:0.760725\n",
      "Epoch: 119/200, Iteration: 5948/10000 --- Training Loss:0.642516\n",
      "Epoch: 119/200, Iteration: 5950/10000 --- Training Loss:0.380856\n",
      "Epoch: 119 finished ! Train Loss: 0.67135, Test Loss: 5.12082\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 120/200, Iteration: 5952/10000 --- Training Loss:0.595073\n",
      "Epoch: 120/200, Iteration: 5954/10000 --- Training Loss:0.712813\n",
      "Epoch: 120/200, Iteration: 5956/10000 --- Training Loss:0.163893\n",
      "Epoch: 120/200, Iteration: 5958/10000 --- Training Loss:0.822124\n",
      "Epoch: 120/200, Iteration: 5960/10000 --- Training Loss:0.499246\n",
      "Epoch: 120/200, Iteration: 5962/10000 --- Training Loss:0.297600\n",
      "Epoch: 120/200, Iteration: 5964/10000 --- Training Loss:0.631642\n",
      "Epoch: 120/200, Iteration: 5966/10000 --- Training Loss:0.469532\n",
      "Epoch: 120/200, Iteration: 5968/10000 --- Training Loss:0.414559\n",
      "Epoch: 120/200, Iteration: 5970/10000 --- Training Loss:0.418553\n",
      "Epoch: 120/200, Iteration: 5972/10000 --- Training Loss:0.279708\n",
      "Epoch: 120/200, Iteration: 5974/10000 --- Training Loss:0.522705\n",
      "Epoch: 120/200, Iteration: 5976/10000 --- Training Loss:0.486100\n",
      "Epoch: 120/200, Iteration: 5978/10000 --- Training Loss:0.469943\n",
      "Epoch: 120/200, Iteration: 5980/10000 --- Training Loss:1.426614\n",
      "Epoch: 120/200, Iteration: 5982/10000 --- Training Loss:0.694874\n",
      "Epoch: 120/200, Iteration: 5984/10000 --- Training Loss:0.274696\n",
      "Epoch: 120/200, Iteration: 5986/10000 --- Training Loss:1.642319\n",
      "Epoch: 120/200, Iteration: 5988/10000 --- Training Loss:0.559404\n",
      "Epoch: 120/200, Iteration: 5990/10000 --- Training Loss:0.635691\n",
      "Epoch: 120/200, Iteration: 5992/10000 --- Training Loss:0.544937\n",
      "Epoch: 120/200, Iteration: 5994/10000 --- Training Loss:0.491641\n",
      "Epoch: 120/200, Iteration: 5996/10000 --- Training Loss:0.740593\n",
      "Epoch: 120/200, Iteration: 5998/10000 --- Training Loss:1.807396\n",
      "Epoch: 120/200, Iteration: 6000/10000 --- Training Loss:0.558382\n",
      "Epoch: 120 finished ! Train Loss: 0.72771, Test Loss: 2.66280\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 121/200, Iteration: 6002/10000 --- Training Loss:0.591476\n",
      "Epoch: 121/200, Iteration: 6004/10000 --- Training Loss:0.539827\n",
      "Epoch: 121/200, Iteration: 6006/10000 --- Training Loss:0.601152\n",
      "Epoch: 121/200, Iteration: 6008/10000 --- Training Loss:0.705769\n",
      "Epoch: 121/200, Iteration: 6010/10000 --- Training Loss:0.793440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121/200, Iteration: 6012/10000 --- Training Loss:0.736784\n",
      "Epoch: 121/200, Iteration: 6014/10000 --- Training Loss:0.418737\n",
      "Epoch: 121/200, Iteration: 6016/10000 --- Training Loss:0.287961\n",
      "Epoch: 121/200, Iteration: 6018/10000 --- Training Loss:0.137613\n",
      "Epoch: 121/200, Iteration: 6020/10000 --- Training Loss:0.928665\n",
      "Epoch: 121/200, Iteration: 6022/10000 --- Training Loss:0.654144\n",
      "Epoch: 121/200, Iteration: 6024/10000 --- Training Loss:0.879385\n",
      "Epoch: 121/200, Iteration: 6026/10000 --- Training Loss:0.255437\n",
      "Epoch: 121/200, Iteration: 6028/10000 --- Training Loss:0.163109\n",
      "Epoch: 121/200, Iteration: 6030/10000 --- Training Loss:0.919070\n",
      "Epoch: 121/200, Iteration: 6032/10000 --- Training Loss:0.408683\n",
      "Epoch: 121/200, Iteration: 6034/10000 --- Training Loss:0.819925\n",
      "Epoch: 121/200, Iteration: 6036/10000 --- Training Loss:0.252624\n",
      "Epoch: 121/200, Iteration: 6038/10000 --- Training Loss:0.365785\n",
      "Epoch: 121/200, Iteration: 6040/10000 --- Training Loss:1.455892\n",
      "Epoch: 121/200, Iteration: 6042/10000 --- Training Loss:0.750299\n",
      "Epoch: 121/200, Iteration: 6044/10000 --- Training Loss:1.341611\n",
      "Epoch: 121/200, Iteration: 6046/10000 --- Training Loss:0.295571\n",
      "Epoch: 121/200, Iteration: 6048/10000 --- Training Loss:0.273756\n",
      "Epoch: 121/200, Iteration: 6050/10000 --- Training Loss:0.415470\n",
      "Epoch: 121 finished ! Train Loss: 0.58901, Test Loss: 2.63685\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 122/200, Iteration: 6052/10000 --- Training Loss:0.210884\n",
      "Epoch: 122/200, Iteration: 6054/10000 --- Training Loss:0.726371\n",
      "Epoch: 122/200, Iteration: 6056/10000 --- Training Loss:0.441344\n",
      "Epoch: 122/200, Iteration: 6058/10000 --- Training Loss:0.506196\n",
      "Epoch: 122/200, Iteration: 6060/10000 --- Training Loss:0.568311\n",
      "Epoch: 122/200, Iteration: 6062/10000 --- Training Loss:0.294663\n",
      "Epoch: 122/200, Iteration: 6064/10000 --- Training Loss:1.407154\n",
      "Epoch: 122/200, Iteration: 6066/10000 --- Training Loss:0.546023\n",
      "Epoch: 122/200, Iteration: 6068/10000 --- Training Loss:0.217330\n",
      "Epoch: 122/200, Iteration: 6070/10000 --- Training Loss:0.195513\n",
      "Epoch: 122/200, Iteration: 6072/10000 --- Training Loss:0.521391\n",
      "Epoch: 122/200, Iteration: 6074/10000 --- Training Loss:0.817313\n",
      "Epoch: 122/200, Iteration: 6076/10000 --- Training Loss:0.356682\n",
      "Epoch: 122/200, Iteration: 6078/10000 --- Training Loss:0.147250\n",
      "Epoch: 122/200, Iteration: 6080/10000 --- Training Loss:0.387657\n",
      "Epoch: 122/200, Iteration: 6082/10000 --- Training Loss:0.315531\n",
      "Epoch: 122/200, Iteration: 6084/10000 --- Training Loss:0.443939\n",
      "Epoch: 122/200, Iteration: 6086/10000 --- Training Loss:0.935879\n",
      "Epoch: 122/200, Iteration: 6088/10000 --- Training Loss:0.407640\n",
      "Epoch: 122/200, Iteration: 6090/10000 --- Training Loss:1.665030\n",
      "Epoch: 122/200, Iteration: 6092/10000 --- Training Loss:0.372337\n",
      "Epoch: 122/200, Iteration: 6094/10000 --- Training Loss:0.688972\n",
      "Epoch: 122/200, Iteration: 6096/10000 --- Training Loss:0.403636\n",
      "Epoch: 122/200, Iteration: 6098/10000 --- Training Loss:0.347882\n",
      "Epoch: 122/200, Iteration: 6100/10000 --- Training Loss:0.546883\n",
      "Epoch: 122 finished ! Train Loss: 0.53073, Test Loss: 2.22868\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 123/200, Iteration: 6102/10000 --- Training Loss:0.755620\n",
      "Epoch: 123/200, Iteration: 6104/10000 --- Training Loss:0.402823\n",
      "Epoch: 123/200, Iteration: 6106/10000 --- Training Loss:0.388458\n",
      "Epoch: 123/200, Iteration: 6108/10000 --- Training Loss:0.619034\n",
      "Epoch: 123/200, Iteration: 6110/10000 --- Training Loss:0.976295\n",
      "Epoch: 123/200, Iteration: 6112/10000 --- Training Loss:0.602671\n",
      "Epoch: 123/200, Iteration: 6114/10000 --- Training Loss:0.741198\n",
      "Epoch: 123/200, Iteration: 6116/10000 --- Training Loss:0.625994\n",
      "Epoch: 123/200, Iteration: 6118/10000 --- Training Loss:1.265651\n",
      "Epoch: 123/200, Iteration: 6120/10000 --- Training Loss:0.165433\n",
      "Epoch: 123/200, Iteration: 6122/10000 --- Training Loss:1.112263\n",
      "Epoch: 123/200, Iteration: 6124/10000 --- Training Loss:0.437118\n",
      "Epoch: 123/200, Iteration: 6126/10000 --- Training Loss:0.943963\n",
      "Epoch: 123/200, Iteration: 6128/10000 --- Training Loss:0.396224\n",
      "Epoch: 123/200, Iteration: 6130/10000 --- Training Loss:0.748589\n",
      "Epoch: 123/200, Iteration: 6132/10000 --- Training Loss:0.365191\n",
      "Epoch: 123/200, Iteration: 6134/10000 --- Training Loss:0.395522\n",
      "Epoch: 123/200, Iteration: 6136/10000 --- Training Loss:0.305505\n",
      "Epoch: 123/200, Iteration: 6138/10000 --- Training Loss:0.573315\n",
      "Epoch: 123/200, Iteration: 6140/10000 --- Training Loss:0.445087\n",
      "Epoch: 123/200, Iteration: 6142/10000 --- Training Loss:0.313091\n",
      "Epoch: 123/200, Iteration: 6144/10000 --- Training Loss:0.414916\n",
      "Epoch: 123/200, Iteration: 6146/10000 --- Training Loss:1.107375\n",
      "Epoch: 123/200, Iteration: 6148/10000 --- Training Loss:0.426224\n",
      "Epoch: 123/200, Iteration: 6150/10000 --- Training Loss:0.541025\n",
      "Epoch: 123 finished ! Train Loss: 0.63762, Test Loss: 3.01851\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 124/200, Iteration: 6152/10000 --- Training Loss:0.584382\n",
      "Epoch: 124/200, Iteration: 6154/10000 --- Training Loss:0.440144\n",
      "Epoch: 124/200, Iteration: 6156/10000 --- Training Loss:0.421505\n",
      "Epoch: 124/200, Iteration: 6158/10000 --- Training Loss:0.530107\n",
      "Epoch: 124/200, Iteration: 6160/10000 --- Training Loss:1.092538\n",
      "Epoch: 124/200, Iteration: 6162/10000 --- Training Loss:0.443739\n",
      "Epoch: 124/200, Iteration: 6164/10000 --- Training Loss:0.631040\n",
      "Epoch: 124/200, Iteration: 6166/10000 --- Training Loss:0.454611\n",
      "Epoch: 124/200, Iteration: 6168/10000 --- Training Loss:0.437079\n",
      "Epoch: 124/200, Iteration: 6170/10000 --- Training Loss:0.601396\n",
      "Epoch: 124/200, Iteration: 6172/10000 --- Training Loss:0.529459\n",
      "Epoch: 124/200, Iteration: 6174/10000 --- Training Loss:0.428402\n",
      "Epoch: 124/200, Iteration: 6176/10000 --- Training Loss:0.737527\n",
      "Epoch: 124/200, Iteration: 6178/10000 --- Training Loss:0.444670\n",
      "Epoch: 124/200, Iteration: 6180/10000 --- Training Loss:0.566711\n",
      "Epoch: 124/200, Iteration: 6182/10000 --- Training Loss:1.185766\n",
      "Epoch: 124/200, Iteration: 6184/10000 --- Training Loss:0.702317\n",
      "Epoch: 124/200, Iteration: 6186/10000 --- Training Loss:1.150789\n",
      "Epoch: 124/200, Iteration: 6188/10000 --- Training Loss:0.443627\n",
      "Epoch: 124/200, Iteration: 6190/10000 --- Training Loss:0.395315\n",
      "Epoch: 124/200, Iteration: 6192/10000 --- Training Loss:0.367915\n",
      "Epoch: 124/200, Iteration: 6194/10000 --- Training Loss:0.538226\n",
      "Epoch: 124/200, Iteration: 6196/10000 --- Training Loss:1.316463\n",
      "Epoch: 124/200, Iteration: 6198/10000 --- Training Loss:0.395826\n",
      "Epoch: 124/200, Iteration: 6200/10000 --- Training Loss:0.527094\n",
      "Epoch: 124 finished ! Train Loss: 0.58433, Test Loss: 2.37924\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 125/200, Iteration: 6202/10000 --- Training Loss:0.256035\n",
      "Epoch: 125/200, Iteration: 6204/10000 --- Training Loss:0.472629\n",
      "Epoch: 125/200, Iteration: 6206/10000 --- Training Loss:0.612672\n",
      "Epoch: 125/200, Iteration: 6208/10000 --- Training Loss:0.496775\n",
      "Epoch: 125/200, Iteration: 6210/10000 --- Training Loss:0.545232\n",
      "Epoch: 125/200, Iteration: 6212/10000 --- Training Loss:1.674953\n",
      "Epoch: 125/200, Iteration: 6214/10000 --- Training Loss:0.678866\n",
      "Epoch: 125/200, Iteration: 6216/10000 --- Training Loss:0.264417\n",
      "Epoch: 125/200, Iteration: 6218/10000 --- Training Loss:0.380432\n",
      "Epoch: 125/200, Iteration: 6220/10000 --- Training Loss:0.580841\n",
      "Epoch: 125/200, Iteration: 6222/10000 --- Training Loss:0.562778\n",
      "Epoch: 125/200, Iteration: 6224/10000 --- Training Loss:0.667968\n",
      "Epoch: 125/200, Iteration: 6226/10000 --- Training Loss:0.774805\n",
      "Epoch: 125/200, Iteration: 6228/10000 --- Training Loss:0.970610\n",
      "Epoch: 125/200, Iteration: 6230/10000 --- Training Loss:0.824499\n",
      "Epoch: 125/200, Iteration: 6232/10000 --- Training Loss:0.697759\n",
      "Epoch: 125/200, Iteration: 6234/10000 --- Training Loss:0.656940\n",
      "Epoch: 125/200, Iteration: 6236/10000 --- Training Loss:1.250947\n",
      "Epoch: 125/200, Iteration: 6238/10000 --- Training Loss:0.246004\n",
      "Epoch: 125/200, Iteration: 6240/10000 --- Training Loss:0.441216\n",
      "Epoch: 125/200, Iteration: 6242/10000 --- Training Loss:0.704633\n",
      "Epoch: 125/200, Iteration: 6244/10000 --- Training Loss:0.342040\n",
      "Epoch: 125/200, Iteration: 6246/10000 --- Training Loss:0.589423\n",
      "Epoch: 125/200, Iteration: 6248/10000 --- Training Loss:2.160956\n",
      "Epoch: 125/200, Iteration: 6250/10000 --- Training Loss:0.504935\n",
      "Epoch: 125 finished ! Train Loss: 0.62425, Test Loss: 2.54444\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 126/200, Iteration: 6252/10000 --- Training Loss:0.175225\n",
      "Epoch: 126/200, Iteration: 6254/10000 --- Training Loss:0.579864\n",
      "Epoch: 126/200, Iteration: 6256/10000 --- Training Loss:0.802486\n",
      "Epoch: 126/200, Iteration: 6258/10000 --- Training Loss:0.540887\n",
      "Epoch: 126/200, Iteration: 6260/10000 --- Training Loss:0.319703\n",
      "Epoch: 126/200, Iteration: 6262/10000 --- Training Loss:0.333185\n",
      "Epoch: 126/200, Iteration: 6264/10000 --- Training Loss:0.382892\n",
      "Epoch: 126/200, Iteration: 6266/10000 --- Training Loss:0.252835\n",
      "Epoch: 126/200, Iteration: 6268/10000 --- Training Loss:0.667713\n",
      "Epoch: 126/200, Iteration: 6270/10000 --- Training Loss:1.520405\n",
      "Epoch: 126/200, Iteration: 6272/10000 --- Training Loss:0.790814\n",
      "Epoch: 126/200, Iteration: 6274/10000 --- Training Loss:1.174271\n",
      "Epoch: 126/200, Iteration: 6276/10000 --- Training Loss:0.517163\n",
      "Epoch: 126/200, Iteration: 6278/10000 --- Training Loss:0.787066\n",
      "Epoch: 126/200, Iteration: 6280/10000 --- Training Loss:0.471704\n",
      "Epoch: 126/200, Iteration: 6282/10000 --- Training Loss:0.341531\n",
      "Epoch: 126/200, Iteration: 6284/10000 --- Training Loss:0.356986\n",
      "Epoch: 126/200, Iteration: 6286/10000 --- Training Loss:0.159082\n",
      "Epoch: 126/200, Iteration: 6288/10000 --- Training Loss:0.263226\n",
      "Epoch: 126/200, Iteration: 6290/10000 --- Training Loss:0.423347\n",
      "Epoch: 126/200, Iteration: 6292/10000 --- Training Loss:0.740147\n",
      "Epoch: 126/200, Iteration: 6294/10000 --- Training Loss:0.541894\n",
      "Epoch: 126/200, Iteration: 6296/10000 --- Training Loss:0.504405\n",
      "Epoch: 126/200, Iteration: 6298/10000 --- Training Loss:1.092059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 126/200, Iteration: 6300/10000 --- Training Loss:1.086666\n",
      "Epoch: 126 finished ! Train Loss: 0.60871, Test Loss: 2.07263\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 63 percent completed\n",
      "Epoch: 127/200, Iteration: 6302/10000 --- Training Loss:0.731759\n",
      "Epoch: 127/200, Iteration: 6304/10000 --- Training Loss:1.866962\n",
      "Epoch: 127/200, Iteration: 6306/10000 --- Training Loss:0.491950\n",
      "Epoch: 127/200, Iteration: 6308/10000 --- Training Loss:2.028534\n",
      "Epoch: 127/200, Iteration: 6310/10000 --- Training Loss:0.449986\n",
      "Epoch: 127/200, Iteration: 6312/10000 --- Training Loss:1.741330\n",
      "Epoch: 127/200, Iteration: 6314/10000 --- Training Loss:0.626947\n",
      "Epoch: 127/200, Iteration: 6316/10000 --- Training Loss:1.002727\n",
      "Epoch: 127/200, Iteration: 6318/10000 --- Training Loss:0.377791\n",
      "Epoch: 127/200, Iteration: 6320/10000 --- Training Loss:0.385297\n",
      "Epoch: 127/200, Iteration: 6322/10000 --- Training Loss:0.920511\n",
      "Epoch: 127/200, Iteration: 6324/10000 --- Training Loss:0.361003\n",
      "Epoch: 127/200, Iteration: 6326/10000 --- Training Loss:0.446429\n",
      "Epoch: 127/200, Iteration: 6328/10000 --- Training Loss:0.547494\n",
      "Epoch: 127/200, Iteration: 6330/10000 --- Training Loss:1.491261\n",
      "Epoch: 127/200, Iteration: 6332/10000 --- Training Loss:0.250105\n",
      "Epoch: 127/200, Iteration: 6334/10000 --- Training Loss:0.181916\n",
      "Epoch: 127/200, Iteration: 6336/10000 --- Training Loss:0.458483\n",
      "Epoch: 127/200, Iteration: 6338/10000 --- Training Loss:0.748133\n",
      "Epoch: 127/200, Iteration: 6340/10000 --- Training Loss:0.726448\n",
      "Epoch: 127/200, Iteration: 6342/10000 --- Training Loss:0.365046\n",
      "Epoch: 127/200, Iteration: 6344/10000 --- Training Loss:0.508011\n",
      "Epoch: 127/200, Iteration: 6346/10000 --- Training Loss:0.351214\n",
      "Epoch: 127/200, Iteration: 6348/10000 --- Training Loss:1.198781\n",
      "Epoch: 127/200, Iteration: 6350/10000 --- Training Loss:0.267092\n",
      "Epoch: 127 finished ! Train Loss: 0.69504, Test Loss: 2.86994\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 128/200, Iteration: 6352/10000 --- Training Loss:0.372816\n",
      "Epoch: 128/200, Iteration: 6354/10000 --- Training Loss:0.240148\n",
      "Epoch: 128/200, Iteration: 6356/10000 --- Training Loss:0.384029\n",
      "Epoch: 128/200, Iteration: 6358/10000 --- Training Loss:0.357922\n",
      "Epoch: 128/200, Iteration: 6360/10000 --- Training Loss:0.359971\n",
      "Epoch: 128/200, Iteration: 6362/10000 --- Training Loss:0.277347\n",
      "Epoch: 128/200, Iteration: 6364/10000 --- Training Loss:0.369483\n",
      "Epoch: 128/200, Iteration: 6366/10000 --- Training Loss:1.804645\n",
      "Epoch: 128/200, Iteration: 6368/10000 --- Training Loss:0.232741\n",
      "Epoch: 128/200, Iteration: 6370/10000 --- Training Loss:0.625314\n",
      "Epoch: 128/200, Iteration: 6372/10000 --- Training Loss:0.265157\n",
      "Epoch: 128/200, Iteration: 6374/10000 --- Training Loss:2.638132\n",
      "Epoch: 128/200, Iteration: 6376/10000 --- Training Loss:0.880167\n",
      "Epoch: 128/200, Iteration: 6378/10000 --- Training Loss:0.562378\n",
      "Epoch: 128/200, Iteration: 6380/10000 --- Training Loss:1.204976\n",
      "Epoch: 128/200, Iteration: 6382/10000 --- Training Loss:0.107785\n",
      "Epoch: 128/200, Iteration: 6384/10000 --- Training Loss:0.745715\n",
      "Epoch: 128/200, Iteration: 6386/10000 --- Training Loss:0.306025\n",
      "Epoch: 128/200, Iteration: 6388/10000 --- Training Loss:0.790832\n",
      "Epoch: 128/200, Iteration: 6390/10000 --- Training Loss:0.310444\n",
      "Epoch: 128/200, Iteration: 6392/10000 --- Training Loss:0.160067\n",
      "Epoch: 128/200, Iteration: 6394/10000 --- Training Loss:0.436196\n",
      "Epoch: 128/200, Iteration: 6396/10000 --- Training Loss:0.321988\n",
      "Epoch: 128/200, Iteration: 6398/10000 --- Training Loss:0.205702\n",
      "Epoch: 128/200, Iteration: 6400/10000 --- Training Loss:0.712898\n",
      "Epoch: 128 finished ! Train Loss: 0.57906, Test Loss: 2.32862\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 129/200, Iteration: 6402/10000 --- Training Loss:0.306985\n",
      "Epoch: 129/200, Iteration: 6404/10000 --- Training Loss:0.302498\n",
      "Epoch: 129/200, Iteration: 6406/10000 --- Training Loss:0.229213\n",
      "Epoch: 129/200, Iteration: 6408/10000 --- Training Loss:1.341861\n",
      "Epoch: 129/200, Iteration: 6410/10000 --- Training Loss:0.316510\n",
      "Epoch: 129/200, Iteration: 6412/10000 --- Training Loss:0.278294\n",
      "Epoch: 129/200, Iteration: 6414/10000 --- Training Loss:0.276662\n",
      "Epoch: 129/200, Iteration: 6416/10000 --- Training Loss:0.696296\n",
      "Epoch: 129/200, Iteration: 6418/10000 --- Training Loss:0.501551\n",
      "Epoch: 129/200, Iteration: 6420/10000 --- Training Loss:0.380220\n",
      "Epoch: 129/200, Iteration: 6422/10000 --- Training Loss:0.380378\n",
      "Epoch: 129/200, Iteration: 6424/10000 --- Training Loss:0.182612\n",
      "Epoch: 129/200, Iteration: 6426/10000 --- Training Loss:0.287053\n",
      "Epoch: 129/200, Iteration: 6428/10000 --- Training Loss:0.225630\n",
      "Epoch: 129/200, Iteration: 6430/10000 --- Training Loss:0.394217\n",
      "Epoch: 129/200, Iteration: 6432/10000 --- Training Loss:0.362482\n",
      "Epoch: 129/200, Iteration: 6434/10000 --- Training Loss:0.713946\n",
      "Epoch: 129/200, Iteration: 6436/10000 --- Training Loss:0.279044\n",
      "Epoch: 129/200, Iteration: 6438/10000 --- Training Loss:0.382990\n",
      "Epoch: 129/200, Iteration: 6440/10000 --- Training Loss:0.292312\n",
      "Epoch: 129/200, Iteration: 6442/10000 --- Training Loss:0.285837\n",
      "Epoch: 129/200, Iteration: 6444/10000 --- Training Loss:0.132478\n",
      "Epoch: 129/200, Iteration: 6446/10000 --- Training Loss:0.312495\n",
      "Epoch: 129/200, Iteration: 6448/10000 --- Training Loss:0.479120\n",
      "Epoch: 129/200, Iteration: 6450/10000 --- Training Loss:0.395080\n",
      "Epoch: 129 finished ! Train Loss: 0.48086, Test Loss: 1.85418\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 64 percent completed\n",
      "Epoch: 130/200, Iteration: 6452/10000 --- Training Loss:0.587473\n",
      "Epoch: 130/200, Iteration: 6454/10000 --- Training Loss:0.528071\n",
      "Epoch: 130/200, Iteration: 6456/10000 --- Training Loss:0.932730\n",
      "Epoch: 130/200, Iteration: 6458/10000 --- Training Loss:0.449230\n",
      "Epoch: 130/200, Iteration: 6460/10000 --- Training Loss:0.535275\n",
      "Epoch: 130/200, Iteration: 6462/10000 --- Training Loss:0.533640\n",
      "Epoch: 130/200, Iteration: 6464/10000 --- Training Loss:0.195341\n",
      "Epoch: 130/200, Iteration: 6466/10000 --- Training Loss:0.225056\n",
      "Epoch: 130/200, Iteration: 6468/10000 --- Training Loss:0.366057\n",
      "Epoch: 130/200, Iteration: 6470/10000 --- Training Loss:0.348892\n",
      "Epoch: 130/200, Iteration: 6472/10000 --- Training Loss:0.399527\n",
      "Epoch: 130/200, Iteration: 6474/10000 --- Training Loss:0.337091\n",
      "Epoch: 130/200, Iteration: 6476/10000 --- Training Loss:0.270769\n",
      "Epoch: 130/200, Iteration: 6478/10000 --- Training Loss:0.713804\n",
      "Epoch: 130/200, Iteration: 6480/10000 --- Training Loss:0.290234\n",
      "Epoch: 130/200, Iteration: 6482/10000 --- Training Loss:0.477561\n",
      "Epoch: 130/200, Iteration: 6484/10000 --- Training Loss:1.876937\n",
      "Epoch: 130/200, Iteration: 6486/10000 --- Training Loss:0.408139\n",
      "Epoch: 130/200, Iteration: 6488/10000 --- Training Loss:0.278751\n",
      "Epoch: 130/200, Iteration: 6490/10000 --- Training Loss:0.206940\n",
      "Epoch: 130/200, Iteration: 6492/10000 --- Training Loss:0.667998\n",
      "Epoch: 130/200, Iteration: 6494/10000 --- Training Loss:0.406502\n",
      "Epoch: 130/200, Iteration: 6496/10000 --- Training Loss:0.695373\n",
      "Epoch: 130/200, Iteration: 6498/10000 --- Training Loss:1.618386\n",
      "Epoch: 130/200, Iteration: 6500/10000 --- Training Loss:1.257805\n",
      "Epoch: 130 finished ! Train Loss: 0.57253, Test Loss: 2.82006\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 131/200, Iteration: 6502/10000 --- Training Loss:0.306002\n",
      "Epoch: 131/200, Iteration: 6504/10000 --- Training Loss:0.326749\n",
      "Epoch: 131/200, Iteration: 6506/10000 --- Training Loss:0.451427\n",
      "Epoch: 131/200, Iteration: 6508/10000 --- Training Loss:0.952371\n",
      "Epoch: 131/200, Iteration: 6510/10000 --- Training Loss:0.406405\n",
      "Epoch: 131/200, Iteration: 6512/10000 --- Training Loss:0.534950\n",
      "Epoch: 131/200, Iteration: 6514/10000 --- Training Loss:0.997376\n",
      "Epoch: 131/200, Iteration: 6516/10000 --- Training Loss:0.351953\n",
      "Epoch: 131/200, Iteration: 6518/10000 --- Training Loss:0.471286\n",
      "Epoch: 131/200, Iteration: 6520/10000 --- Training Loss:0.683997\n",
      "Epoch: 131/200, Iteration: 6522/10000 --- Training Loss:0.519237\n",
      "Epoch: 131/200, Iteration: 6524/10000 --- Training Loss:0.843648\n",
      "Epoch: 131/200, Iteration: 6526/10000 --- Training Loss:0.552561\n",
      "Epoch: 131/200, Iteration: 6528/10000 --- Training Loss:0.642697\n",
      "Epoch: 131/200, Iteration: 6530/10000 --- Training Loss:0.153358\n",
      "Epoch: 131/200, Iteration: 6532/10000 --- Training Loss:0.226531\n",
      "Epoch: 131/200, Iteration: 6534/10000 --- Training Loss:0.389415\n",
      "Epoch: 131/200, Iteration: 6536/10000 --- Training Loss:0.567130\n",
      "Epoch: 131/200, Iteration: 6538/10000 --- Training Loss:0.325654\n",
      "Epoch: 131/200, Iteration: 6540/10000 --- Training Loss:0.705314\n",
      "Epoch: 131/200, Iteration: 6542/10000 --- Training Loss:0.332768\n",
      "Epoch: 131/200, Iteration: 6544/10000 --- Training Loss:0.583470\n",
      "Epoch: 131/200, Iteration: 6546/10000 --- Training Loss:0.838179\n",
      "Epoch: 131/200, Iteration: 6548/10000 --- Training Loss:0.307688\n",
      "Epoch: 131/200, Iteration: 6550/10000 --- Training Loss:0.793173\n",
      "Epoch: 131 finished ! Train Loss: 0.58638, Test Loss: 2.33002\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 132/200, Iteration: 6552/10000 --- Training Loss:1.033324\n",
      "Epoch: 132/200, Iteration: 6554/10000 --- Training Loss:0.416719\n",
      "Epoch: 132/200, Iteration: 6556/10000 --- Training Loss:0.356856\n",
      "Epoch: 132/200, Iteration: 6558/10000 --- Training Loss:0.890422\n",
      "Epoch: 132/200, Iteration: 6560/10000 --- Training Loss:0.583511\n",
      "Epoch: 132/200, Iteration: 6562/10000 --- Training Loss:0.565925\n",
      "Epoch: 132/200, Iteration: 6564/10000 --- Training Loss:0.277803\n",
      "Epoch: 132/200, Iteration: 6566/10000 --- Training Loss:0.093684\n",
      "Epoch: 132/200, Iteration: 6568/10000 --- Training Loss:0.363328\n",
      "Epoch: 132/200, Iteration: 6570/10000 --- Training Loss:0.595236\n",
      "Epoch: 132/200, Iteration: 6572/10000 --- Training Loss:0.545999\n",
      "Epoch: 132/200, Iteration: 6574/10000 --- Training Loss:0.974055\n",
      "Epoch: 132/200, Iteration: 6576/10000 --- Training Loss:0.202397\n",
      "Epoch: 132/200, Iteration: 6578/10000 --- Training Loss:1.885380\n",
      "Epoch: 132/200, Iteration: 6580/10000 --- Training Loss:0.625403\n",
      "Epoch: 132/200, Iteration: 6582/10000 --- Training Loss:0.872080\n",
      "Epoch: 132/200, Iteration: 6584/10000 --- Training Loss:1.550294\n",
      "Epoch: 132/200, Iteration: 6586/10000 --- Training Loss:0.330123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 132/200, Iteration: 6588/10000 --- Training Loss:0.701785\n",
      "Epoch: 132/200, Iteration: 6590/10000 --- Training Loss:0.694785\n",
      "Epoch: 132/200, Iteration: 6592/10000 --- Training Loss:0.165052\n",
      "Epoch: 132/200, Iteration: 6594/10000 --- Training Loss:1.446137\n",
      "Epoch: 132/200, Iteration: 6596/10000 --- Training Loss:0.593411\n",
      "Epoch: 132/200, Iteration: 6598/10000 --- Training Loss:0.497541\n",
      "Epoch: 132/200, Iteration: 6600/10000 --- Training Loss:0.722369\n",
      "Epoch: 132 finished ! Train Loss: 0.63800, Test Loss: 2.13903\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 133/200, Iteration: 6602/10000 --- Training Loss:0.295010\n",
      "Epoch: 133/200, Iteration: 6604/10000 --- Training Loss:0.342326\n",
      "Epoch: 133/200, Iteration: 6606/10000 --- Training Loss:0.594267\n",
      "Epoch: 133/200, Iteration: 6608/10000 --- Training Loss:0.364820\n",
      "Epoch: 133/200, Iteration: 6610/10000 --- Training Loss:1.971569\n",
      "Epoch: 133/200, Iteration: 6612/10000 --- Training Loss:0.624068\n",
      "Epoch: 133/200, Iteration: 6614/10000 --- Training Loss:0.368945\n",
      "Epoch: 133/200, Iteration: 6616/10000 --- Training Loss:1.962413\n",
      "Epoch: 133/200, Iteration: 6618/10000 --- Training Loss:0.465648\n",
      "Epoch: 133/200, Iteration: 6620/10000 --- Training Loss:1.183821\n",
      "Epoch: 133/200, Iteration: 6622/10000 --- Training Loss:0.250345\n",
      "Epoch: 133/200, Iteration: 6624/10000 --- Training Loss:1.198719\n",
      "Epoch: 133/200, Iteration: 6626/10000 --- Training Loss:0.299439\n",
      "Epoch: 133/200, Iteration: 6628/10000 --- Training Loss:0.310346\n",
      "Epoch: 133/200, Iteration: 6630/10000 --- Training Loss:0.522582\n",
      "Epoch: 133/200, Iteration: 6632/10000 --- Training Loss:0.239359\n",
      "Epoch: 133/200, Iteration: 6634/10000 --- Training Loss:0.192407\n",
      "Epoch: 133/200, Iteration: 6636/10000 --- Training Loss:0.436555\n",
      "Epoch: 133/200, Iteration: 6638/10000 --- Training Loss:0.587402\n",
      "Epoch: 133/200, Iteration: 6640/10000 --- Training Loss:0.450135\n",
      "Epoch: 133/200, Iteration: 6642/10000 --- Training Loss:0.363672\n",
      "Epoch: 133/200, Iteration: 6644/10000 --- Training Loss:0.484891\n",
      "Epoch: 133/200, Iteration: 6646/10000 --- Training Loss:1.298282\n",
      "Epoch: 133/200, Iteration: 6648/10000 --- Training Loss:0.499096\n",
      "Epoch: 133/200, Iteration: 6650/10000 --- Training Loss:0.741147\n",
      "Epoch: 133 finished ! Train Loss: 0.64747, Test Loss: 2.43536\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 134/200, Iteration: 6652/10000 --- Training Loss:1.242180\n",
      "Epoch: 134/200, Iteration: 6654/10000 --- Training Loss:0.811698\n",
      "Epoch: 134/200, Iteration: 6656/10000 --- Training Loss:0.736945\n",
      "Epoch: 134/200, Iteration: 6658/10000 --- Training Loss:0.527349\n",
      "Epoch: 134/200, Iteration: 6660/10000 --- Training Loss:0.320716\n",
      "Epoch: 134/200, Iteration: 6662/10000 --- Training Loss:1.364262\n",
      "Epoch: 134/200, Iteration: 6664/10000 --- Training Loss:0.392060\n",
      "Epoch: 134/200, Iteration: 6666/10000 --- Training Loss:2.144419\n",
      "Epoch: 134/200, Iteration: 6668/10000 --- Training Loss:0.267850\n",
      "Epoch: 134/200, Iteration: 6670/10000 --- Training Loss:0.300792\n",
      "Epoch: 134/200, Iteration: 6672/10000 --- Training Loss:0.689522\n",
      "Epoch: 134/200, Iteration: 6674/10000 --- Training Loss:0.970776\n",
      "Epoch: 134/200, Iteration: 6676/10000 --- Training Loss:0.685718\n",
      "Epoch: 134/200, Iteration: 6678/10000 --- Training Loss:0.306638\n",
      "Epoch: 134/200, Iteration: 6680/10000 --- Training Loss:0.356221\n",
      "Epoch: 134/200, Iteration: 6682/10000 --- Training Loss:0.543720\n",
      "Epoch: 134/200, Iteration: 6684/10000 --- Training Loss:0.446916\n",
      "Epoch: 134/200, Iteration: 6686/10000 --- Training Loss:0.266428\n",
      "Epoch: 134/200, Iteration: 6688/10000 --- Training Loss:0.160617\n",
      "Epoch: 134/200, Iteration: 6690/10000 --- Training Loss:0.544272\n",
      "Epoch: 134/200, Iteration: 6692/10000 --- Training Loss:0.316547\n",
      "Epoch: 134/200, Iteration: 6694/10000 --- Training Loss:0.280645\n",
      "Epoch: 134/200, Iteration: 6696/10000 --- Training Loss:0.598137\n",
      "Epoch: 134/200, Iteration: 6698/10000 --- Training Loss:0.297471\n",
      "Epoch: 134/200, Iteration: 6700/10000 --- Training Loss:0.303320\n",
      "Epoch: 134 finished ! Train Loss: 0.55007, Test Loss: 1.81569\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 67 percent completed\n",
      "Epoch: 135/200, Iteration: 6702/10000 --- Training Loss:0.473829\n",
      "Epoch: 135/200, Iteration: 6704/10000 --- Training Loss:0.321972\n",
      "Epoch: 135/200, Iteration: 6706/10000 --- Training Loss:0.262016\n",
      "Epoch: 135/200, Iteration: 6708/10000 --- Training Loss:0.971761\n",
      "Epoch: 135/200, Iteration: 6710/10000 --- Training Loss:0.387333\n",
      "Epoch: 135/200, Iteration: 6712/10000 --- Training Loss:0.873430\n",
      "Epoch: 135/200, Iteration: 6714/10000 --- Training Loss:0.243361\n",
      "Epoch: 135/200, Iteration: 6716/10000 --- Training Loss:0.195078\n",
      "Epoch: 135/200, Iteration: 6718/10000 --- Training Loss:0.318739\n",
      "Epoch: 135/200, Iteration: 6720/10000 --- Training Loss:0.400427\n",
      "Epoch: 135/200, Iteration: 6722/10000 --- Training Loss:0.653074\n",
      "Epoch: 135/200, Iteration: 6724/10000 --- Training Loss:1.204977\n",
      "Epoch: 135/200, Iteration: 6726/10000 --- Training Loss:0.343002\n",
      "Epoch: 135/200, Iteration: 6728/10000 --- Training Loss:0.458003\n",
      "Epoch: 135/200, Iteration: 6730/10000 --- Training Loss:0.355827\n",
      "Epoch: 135/200, Iteration: 6732/10000 --- Training Loss:0.279335\n",
      "Epoch: 135/200, Iteration: 6734/10000 --- Training Loss:1.316270\n",
      "Epoch: 135/200, Iteration: 6736/10000 --- Training Loss:0.368505\n",
      "Epoch: 135/200, Iteration: 6738/10000 --- Training Loss:0.220160\n",
      "Epoch: 135/200, Iteration: 6740/10000 --- Training Loss:1.269751\n",
      "Epoch: 135/200, Iteration: 6742/10000 --- Training Loss:0.455705\n",
      "Epoch: 135/200, Iteration: 6744/10000 --- Training Loss:0.474086\n",
      "Epoch: 135/200, Iteration: 6746/10000 --- Training Loss:0.980962\n",
      "Epoch: 135/200, Iteration: 6748/10000 --- Training Loss:0.425464\n",
      "Epoch: 135/200, Iteration: 6750/10000 --- Training Loss:0.788204\n",
      "Epoch: 135 finished ! Train Loss: 0.56792, Test Loss: 2.30311\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 136/200, Iteration: 6752/10000 --- Training Loss:0.769584\n",
      "Epoch: 136/200, Iteration: 6754/10000 --- Training Loss:0.208702\n",
      "Epoch: 136/200, Iteration: 6756/10000 --- Training Loss:0.646860\n",
      "Epoch: 136/200, Iteration: 6758/10000 --- Training Loss:1.496334\n",
      "Epoch: 136/200, Iteration: 6760/10000 --- Training Loss:0.458140\n",
      "Epoch: 136/200, Iteration: 6762/10000 --- Training Loss:1.010790\n",
      "Epoch: 136/200, Iteration: 6764/10000 --- Training Loss:1.606664\n",
      "Epoch: 136/200, Iteration: 6766/10000 --- Training Loss:0.545840\n",
      "Epoch: 136/200, Iteration: 6768/10000 --- Training Loss:0.404549\n",
      "Epoch: 136/200, Iteration: 6770/10000 --- Training Loss:0.461811\n",
      "Epoch: 136/200, Iteration: 6772/10000 --- Training Loss:0.233534\n",
      "Epoch: 136/200, Iteration: 6774/10000 --- Training Loss:0.828258\n",
      "Epoch: 136/200, Iteration: 6776/10000 --- Training Loss:0.194158\n",
      "Epoch: 136/200, Iteration: 6778/10000 --- Training Loss:0.668453\n",
      "Epoch: 136/200, Iteration: 6780/10000 --- Training Loss:0.404802\n",
      "Epoch: 136/200, Iteration: 6782/10000 --- Training Loss:0.563960\n",
      "Epoch: 136/200, Iteration: 6784/10000 --- Training Loss:0.447138\n",
      "Epoch: 136/200, Iteration: 6786/10000 --- Training Loss:0.141756\n",
      "Epoch: 136/200, Iteration: 6788/10000 --- Training Loss:0.188426\n",
      "Epoch: 136/200, Iteration: 6790/10000 --- Training Loss:0.110541\n",
      "Epoch: 136/200, Iteration: 6792/10000 --- Training Loss:0.485817\n",
      "Epoch: 136/200, Iteration: 6794/10000 --- Training Loss:0.284963\n",
      "Epoch: 136/200, Iteration: 6796/10000 --- Training Loss:0.989473\n",
      "Epoch: 136/200, Iteration: 6798/10000 --- Training Loss:1.031579\n",
      "Epoch: 136/200, Iteration: 6800/10000 --- Training Loss:0.719470\n",
      "Epoch: 136 finished ! Train Loss: 0.55361, Test Loss: 1.74687\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 68 percent completed\n",
      "Epoch: 137/200, Iteration: 6802/10000 --- Training Loss:1.485363\n",
      "Epoch: 137/200, Iteration: 6804/10000 --- Training Loss:0.674938\n",
      "Epoch: 137/200, Iteration: 6806/10000 --- Training Loss:0.803507\n",
      "Epoch: 137/200, Iteration: 6808/10000 --- Training Loss:1.421425\n",
      "Epoch: 137/200, Iteration: 6810/10000 --- Training Loss:0.437136\n",
      "Epoch: 137/200, Iteration: 6812/10000 --- Training Loss:0.196094\n",
      "Epoch: 137/200, Iteration: 6814/10000 --- Training Loss:0.496407\n",
      "Epoch: 137/200, Iteration: 6816/10000 --- Training Loss:0.905632\n",
      "Epoch: 137/200, Iteration: 6818/10000 --- Training Loss:0.693911\n",
      "Epoch: 137/200, Iteration: 6820/10000 --- Training Loss:0.192737\n",
      "Epoch: 137/200, Iteration: 6822/10000 --- Training Loss:0.532434\n",
      "Epoch: 137/200, Iteration: 6824/10000 --- Training Loss:0.653848\n",
      "Epoch: 137/200, Iteration: 6826/10000 --- Training Loss:0.990119\n",
      "Epoch: 137/200, Iteration: 6828/10000 --- Training Loss:0.241318\n",
      "Epoch: 137/200, Iteration: 6830/10000 --- Training Loss:0.871201\n",
      "Epoch: 137/200, Iteration: 6832/10000 --- Training Loss:0.274906\n",
      "Epoch: 137/200, Iteration: 6834/10000 --- Training Loss:1.095848\n",
      "Epoch: 137/200, Iteration: 6836/10000 --- Training Loss:0.419528\n",
      "Epoch: 137/200, Iteration: 6838/10000 --- Training Loss:1.312578\n",
      "Epoch: 137/200, Iteration: 6840/10000 --- Training Loss:0.354471\n",
      "Epoch: 137/200, Iteration: 6842/10000 --- Training Loss:0.468589\n",
      "Epoch: 137/200, Iteration: 6844/10000 --- Training Loss:0.437648\n",
      "Epoch: 137/200, Iteration: 6846/10000 --- Training Loss:0.389124\n",
      "Epoch: 137/200, Iteration: 6848/10000 --- Training Loss:0.328886\n",
      "Epoch: 137/200, Iteration: 6850/10000 --- Training Loss:0.580872\n",
      "Epoch: 137 finished ! Train Loss: 0.63686, Test Loss: 1.80877\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 138/200, Iteration: 6852/10000 --- Training Loss:0.276182\n",
      "Epoch: 138/200, Iteration: 6854/10000 --- Training Loss:0.203433\n",
      "Epoch: 138/200, Iteration: 6856/10000 --- Training Loss:0.425611\n",
      "Epoch: 138/200, Iteration: 6858/10000 --- Training Loss:0.223931\n",
      "Epoch: 138/200, Iteration: 6860/10000 --- Training Loss:0.270808\n",
      "Epoch: 138/200, Iteration: 6862/10000 --- Training Loss:0.253684\n",
      "Epoch: 138/200, Iteration: 6864/10000 --- Training Loss:0.108311\n",
      "Epoch: 138/200, Iteration: 6866/10000 --- Training Loss:0.520660\n",
      "Epoch: 138/200, Iteration: 6868/10000 --- Training Loss:1.464885\n",
      "Epoch: 138/200, Iteration: 6870/10000 --- Training Loss:0.436330\n",
      "Epoch: 138/200, Iteration: 6872/10000 --- Training Loss:0.262392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 138/200, Iteration: 6874/10000 --- Training Loss:0.309731\n",
      "Epoch: 138/200, Iteration: 6876/10000 --- Training Loss:0.402466\n",
      "Epoch: 138/200, Iteration: 6878/10000 --- Training Loss:0.246156\n",
      "Epoch: 138/200, Iteration: 6880/10000 --- Training Loss:0.553013\n",
      "Epoch: 138/200, Iteration: 6882/10000 --- Training Loss:0.820138\n",
      "Epoch: 138/200, Iteration: 6884/10000 --- Training Loss:0.821464\n",
      "Epoch: 138/200, Iteration: 6886/10000 --- Training Loss:0.702232\n",
      "Epoch: 138/200, Iteration: 6888/10000 --- Training Loss:0.522053\n",
      "Epoch: 138/200, Iteration: 6890/10000 --- Training Loss:0.640076\n",
      "Epoch: 138/200, Iteration: 6892/10000 --- Training Loss:0.978255\n",
      "Epoch: 138/200, Iteration: 6894/10000 --- Training Loss:0.363678\n",
      "Epoch: 138/200, Iteration: 6896/10000 --- Training Loss:0.282753\n",
      "Epoch: 138/200, Iteration: 6898/10000 --- Training Loss:0.561077\n",
      "Epoch: 138/200, Iteration: 6900/10000 --- Training Loss:0.437068\n",
      "Epoch: 138 finished ! Train Loss: 0.49268, Test Loss: 1.81747\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 139/200, Iteration: 6902/10000 --- Training Loss:0.347050\n",
      "Epoch: 139/200, Iteration: 6904/10000 --- Training Loss:0.876375\n",
      "Epoch: 139/200, Iteration: 6906/10000 --- Training Loss:0.442703\n",
      "Epoch: 139/200, Iteration: 6908/10000 --- Training Loss:0.607336\n",
      "Epoch: 139/200, Iteration: 6910/10000 --- Training Loss:0.585358\n",
      "Epoch: 139/200, Iteration: 6912/10000 --- Training Loss:0.312948\n",
      "Epoch: 139/200, Iteration: 6914/10000 --- Training Loss:0.974537\n",
      "Epoch: 139/200, Iteration: 6916/10000 --- Training Loss:0.887750\n",
      "Epoch: 139/200, Iteration: 6918/10000 --- Training Loss:0.689581\n",
      "Epoch: 139/200, Iteration: 6920/10000 --- Training Loss:0.557966\n",
      "Epoch: 139/200, Iteration: 6922/10000 --- Training Loss:0.154119\n",
      "Epoch: 139/200, Iteration: 6924/10000 --- Training Loss:0.449233\n",
      "Epoch: 139/200, Iteration: 6926/10000 --- Training Loss:0.357056\n",
      "Epoch: 139/200, Iteration: 6928/10000 --- Training Loss:0.186895\n",
      "Epoch: 139/200, Iteration: 6930/10000 --- Training Loss:0.198369\n",
      "Epoch: 139/200, Iteration: 6932/10000 --- Training Loss:0.349622\n",
      "Epoch: 139/200, Iteration: 6934/10000 --- Training Loss:0.395438\n",
      "Epoch: 139/200, Iteration: 6936/10000 --- Training Loss:0.164124\n",
      "Epoch: 139/200, Iteration: 6938/10000 --- Training Loss:0.603847\n",
      "Epoch: 139/200, Iteration: 6940/10000 --- Training Loss:0.299568\n",
      "Epoch: 139/200, Iteration: 6942/10000 --- Training Loss:0.255669\n",
      "Epoch: 139/200, Iteration: 6944/10000 --- Training Loss:0.335974\n",
      "Epoch: 139/200, Iteration: 6946/10000 --- Training Loss:0.625034\n",
      "Epoch: 139/200, Iteration: 6948/10000 --- Training Loss:0.365066\n",
      "Epoch: 139/200, Iteration: 6950/10000 --- Training Loss:0.256532\n",
      "Epoch: 139 finished ! Train Loss: 0.55455, Test Loss: 2.62939\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 140/200, Iteration: 6952/10000 --- Training Loss:0.139487\n",
      "Epoch: 140/200, Iteration: 6954/10000 --- Training Loss:0.500525\n",
      "Epoch: 140/200, Iteration: 6956/10000 --- Training Loss:0.290891\n",
      "Epoch: 140/200, Iteration: 6958/10000 --- Training Loss:1.270171\n",
      "Epoch: 140/200, Iteration: 6960/10000 --- Training Loss:0.439550\n",
      "Epoch: 140/200, Iteration: 6962/10000 --- Training Loss:0.807027\n",
      "Epoch: 140/200, Iteration: 6964/10000 --- Training Loss:0.614800\n",
      "Epoch: 140/200, Iteration: 6966/10000 --- Training Loss:0.511722\n",
      "Epoch: 140/200, Iteration: 6968/10000 --- Training Loss:0.237669\n",
      "Epoch: 140/200, Iteration: 6970/10000 --- Training Loss:0.740332\n",
      "Epoch: 140/200, Iteration: 6972/10000 --- Training Loss:0.999969\n",
      "Epoch: 140/200, Iteration: 6974/10000 --- Training Loss:0.599995\n",
      "Epoch: 140/200, Iteration: 6976/10000 --- Training Loss:0.372937\n",
      "Epoch: 140/200, Iteration: 6978/10000 --- Training Loss:0.463955\n",
      "Epoch: 140/200, Iteration: 6980/10000 --- Training Loss:0.403702\n",
      "Epoch: 140/200, Iteration: 6982/10000 --- Training Loss:0.279491\n",
      "Epoch: 140/200, Iteration: 6984/10000 --- Training Loss:0.386051\n",
      "Epoch: 140/200, Iteration: 6986/10000 --- Training Loss:0.507631\n",
      "Epoch: 140/200, Iteration: 6988/10000 --- Training Loss:0.858165\n",
      "Epoch: 140/200, Iteration: 6990/10000 --- Training Loss:0.188246\n",
      "Epoch: 140/200, Iteration: 6992/10000 --- Training Loss:0.420271\n",
      "Epoch: 140/200, Iteration: 6994/10000 --- Training Loss:0.477300\n",
      "Epoch: 140/200, Iteration: 6996/10000 --- Training Loss:0.382979\n",
      "Epoch: 140/200, Iteration: 6998/10000 --- Training Loss:0.318116\n",
      "Epoch: 140/200, Iteration: 7000/10000 --- Training Loss:1.024035\n",
      "Epoch: 140 finished ! Train Loss: 0.61225, Test Loss: 2.99612\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 141/200, Iteration: 7002/10000 --- Training Loss:0.504946\n",
      "Epoch: 141/200, Iteration: 7004/10000 --- Training Loss:0.861202\n",
      "Epoch: 141/200, Iteration: 7006/10000 --- Training Loss:0.666294\n",
      "Epoch: 141/200, Iteration: 7008/10000 --- Training Loss:0.757500\n",
      "Epoch: 141/200, Iteration: 7010/10000 --- Training Loss:0.400666\n",
      "Epoch: 141/200, Iteration: 7012/10000 --- Training Loss:0.319081\n",
      "Epoch: 141/200, Iteration: 7014/10000 --- Training Loss:0.432235\n",
      "Epoch: 141/200, Iteration: 7016/10000 --- Training Loss:1.169211\n",
      "Epoch: 141/200, Iteration: 7018/10000 --- Training Loss:0.537175\n",
      "Epoch: 141/200, Iteration: 7020/10000 --- Training Loss:0.793878\n",
      "Epoch: 141/200, Iteration: 7022/10000 --- Training Loss:0.898404\n",
      "Epoch: 141/200, Iteration: 7024/10000 --- Training Loss:0.298148\n",
      "Epoch: 141/200, Iteration: 7026/10000 --- Training Loss:0.949392\n",
      "Epoch: 141/200, Iteration: 7028/10000 --- Training Loss:0.330303\n",
      "Epoch: 141/200, Iteration: 7030/10000 --- Training Loss:0.474010\n",
      "Epoch: 141/200, Iteration: 7032/10000 --- Training Loss:0.636562\n",
      "Epoch: 141/200, Iteration: 7034/10000 --- Training Loss:0.255600\n",
      "Epoch: 141/200, Iteration: 7036/10000 --- Training Loss:1.317324\n",
      "Epoch: 141/200, Iteration: 7038/10000 --- Training Loss:1.650463\n",
      "Epoch: 141/200, Iteration: 7040/10000 --- Training Loss:0.362555\n",
      "Epoch: 141/200, Iteration: 7042/10000 --- Training Loss:0.535112\n",
      "Epoch: 141/200, Iteration: 7044/10000 --- Training Loss:0.834307\n",
      "Epoch: 141/200, Iteration: 7046/10000 --- Training Loss:0.519109\n",
      "Epoch: 141/200, Iteration: 7048/10000 --- Training Loss:0.817484\n",
      "Epoch: 141/200, Iteration: 7050/10000 --- Training Loss:0.574616\n",
      "Epoch: 141 finished ! Train Loss: 0.56539, Test Loss: 2.56131\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 142/200, Iteration: 7052/10000 --- Training Loss:0.284306\n",
      "Epoch: 142/200, Iteration: 7054/10000 --- Training Loss:1.004826\n",
      "Epoch: 142/200, Iteration: 7056/10000 --- Training Loss:0.462858\n",
      "Epoch: 142/200, Iteration: 7058/10000 --- Training Loss:0.283431\n",
      "Epoch: 142/200, Iteration: 7060/10000 --- Training Loss:0.218013\n",
      "Epoch: 142/200, Iteration: 7062/10000 --- Training Loss:1.909942\n",
      "Epoch: 142/200, Iteration: 7064/10000 --- Training Loss:0.546593\n",
      "Epoch: 142/200, Iteration: 7066/10000 --- Training Loss:0.318644\n",
      "Epoch: 142/200, Iteration: 7068/10000 --- Training Loss:2.157264\n",
      "Epoch: 142/200, Iteration: 7070/10000 --- Training Loss:0.382512\n",
      "Epoch: 142/200, Iteration: 7072/10000 --- Training Loss:0.393376\n",
      "Epoch: 142/200, Iteration: 7074/10000 --- Training Loss:0.374954\n",
      "Epoch: 142/200, Iteration: 7076/10000 --- Training Loss:0.426684\n",
      "Epoch: 142/200, Iteration: 7078/10000 --- Training Loss:0.357604\n",
      "Epoch: 142/200, Iteration: 7080/10000 --- Training Loss:0.493703\n",
      "Epoch: 142/200, Iteration: 7082/10000 --- Training Loss:0.566178\n",
      "Epoch: 142/200, Iteration: 7084/10000 --- Training Loss:0.793315\n",
      "Epoch: 142/200, Iteration: 7086/10000 --- Training Loss:0.828092\n",
      "Epoch: 142/200, Iteration: 7088/10000 --- Training Loss:0.446625\n",
      "Epoch: 142/200, Iteration: 7090/10000 --- Training Loss:0.508896\n",
      "Epoch: 142/200, Iteration: 7092/10000 --- Training Loss:0.735608\n",
      "Epoch: 142/200, Iteration: 7094/10000 --- Training Loss:1.664922\n",
      "Epoch: 142/200, Iteration: 7096/10000 --- Training Loss:1.522699\n",
      "Epoch: 142/200, Iteration: 7098/10000 --- Training Loss:0.635067\n",
      "Epoch: 142/200, Iteration: 7100/10000 --- Training Loss:0.822869\n",
      "Epoch: 142 finished ! Train Loss: 0.62661, Test Loss: 3.79073\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 143/200, Iteration: 7102/10000 --- Training Loss:1.322824\n",
      "Epoch: 143/200, Iteration: 7104/10000 --- Training Loss:0.870771\n",
      "Epoch: 143/200, Iteration: 7106/10000 --- Training Loss:0.950521\n",
      "Epoch: 143/200, Iteration: 7108/10000 --- Training Loss:0.132072\n",
      "Epoch: 143/200, Iteration: 7110/10000 --- Training Loss:1.117329\n",
      "Epoch: 143/200, Iteration: 7112/10000 --- Training Loss:1.007444\n",
      "Epoch: 143/200, Iteration: 7114/10000 --- Training Loss:0.327247\n",
      "Epoch: 143/200, Iteration: 7116/10000 --- Training Loss:1.069616\n",
      "Epoch: 143/200, Iteration: 7118/10000 --- Training Loss:0.216395\n",
      "Epoch: 143/200, Iteration: 7120/10000 --- Training Loss:0.499340\n",
      "Epoch: 143/200, Iteration: 7122/10000 --- Training Loss:0.216168\n",
      "Epoch: 143/200, Iteration: 7124/10000 --- Training Loss:1.811044\n",
      "Epoch: 143/200, Iteration: 7126/10000 --- Training Loss:0.183301\n",
      "Epoch: 143/200, Iteration: 7128/10000 --- Training Loss:0.587959\n",
      "Epoch: 143/200, Iteration: 7130/10000 --- Training Loss:0.565869\n",
      "Epoch: 143/200, Iteration: 7132/10000 --- Training Loss:0.295549\n",
      "Epoch: 143/200, Iteration: 7134/10000 --- Training Loss:0.563799\n",
      "Epoch: 143/200, Iteration: 7136/10000 --- Training Loss:0.234576\n",
      "Epoch: 143/200, Iteration: 7138/10000 --- Training Loss:0.885666\n",
      "Epoch: 143/200, Iteration: 7140/10000 --- Training Loss:0.567455\n",
      "Epoch: 143/200, Iteration: 7142/10000 --- Training Loss:0.450636\n",
      "Epoch: 143/200, Iteration: 7144/10000 --- Training Loss:0.255437\n",
      "Epoch: 143/200, Iteration: 7146/10000 --- Training Loss:0.232269\n",
      "Epoch: 143/200, Iteration: 7148/10000 --- Training Loss:0.438970\n",
      "Epoch: 143/200, Iteration: 7150/10000 --- Training Loss:0.532920\n",
      "Epoch: 143 finished ! Train Loss: 0.62766, Test Loss: 2.45009\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 144/200, Iteration: 7152/10000 --- Training Loss:0.158541\n",
      "Epoch: 144/200, Iteration: 7154/10000 --- Training Loss:0.835363\n",
      "Epoch: 144/200, Iteration: 7156/10000 --- Training Loss:0.855413\n",
      "Epoch: 144/200, Iteration: 7158/10000 --- Training Loss:0.425868\n",
      "Epoch: 144/200, Iteration: 7160/10000 --- Training Loss:1.170799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144/200, Iteration: 7162/10000 --- Training Loss:0.338152\n",
      "Epoch: 144/200, Iteration: 7164/10000 --- Training Loss:0.379690\n",
      "Epoch: 144/200, Iteration: 7166/10000 --- Training Loss:0.566210\n",
      "Epoch: 144/200, Iteration: 7168/10000 --- Training Loss:0.745169\n",
      "Epoch: 144/200, Iteration: 7170/10000 --- Training Loss:0.114666\n",
      "Epoch: 144/200, Iteration: 7172/10000 --- Training Loss:1.083434\n",
      "Epoch: 144/200, Iteration: 7174/10000 --- Training Loss:0.306131\n",
      "Epoch: 144/200, Iteration: 7176/10000 --- Training Loss:0.608294\n",
      "Epoch: 144/200, Iteration: 7178/10000 --- Training Loss:0.595454\n",
      "Epoch: 144/200, Iteration: 7180/10000 --- Training Loss:1.050835\n",
      "Epoch: 144/200, Iteration: 7182/10000 --- Training Loss:0.453108\n",
      "Epoch: 144/200, Iteration: 7184/10000 --- Training Loss:1.118966\n",
      "Epoch: 144/200, Iteration: 7186/10000 --- Training Loss:0.292748\n",
      "Epoch: 144/200, Iteration: 7188/10000 --- Training Loss:1.662476\n",
      "Epoch: 144/200, Iteration: 7190/10000 --- Training Loss:0.489008\n",
      "Epoch: 144/200, Iteration: 7192/10000 --- Training Loss:0.818306\n",
      "Epoch: 144/200, Iteration: 7194/10000 --- Training Loss:0.848943\n",
      "Epoch: 144/200, Iteration: 7196/10000 --- Training Loss:0.455125\n",
      "Epoch: 144/200, Iteration: 7198/10000 --- Training Loss:0.692333\n",
      "Epoch: 144/200, Iteration: 7200/10000 --- Training Loss:0.384781\n",
      "Epoch: 144 finished ! Train Loss: 0.76368, Test Loss: 2.42647\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 145/200, Iteration: 7202/10000 --- Training Loss:0.352943\n",
      "Epoch: 145/200, Iteration: 7204/10000 --- Training Loss:0.205793\n",
      "Epoch: 145/200, Iteration: 7206/10000 --- Training Loss:0.386737\n",
      "Epoch: 145/200, Iteration: 7208/10000 --- Training Loss:0.956015\n",
      "Epoch: 145/200, Iteration: 7210/10000 --- Training Loss:0.206780\n",
      "Epoch: 145/200, Iteration: 7212/10000 --- Training Loss:0.329321\n",
      "Epoch: 145/200, Iteration: 7214/10000 --- Training Loss:0.238994\n",
      "Epoch: 145/200, Iteration: 7216/10000 --- Training Loss:0.246782\n",
      "Epoch: 145/200, Iteration: 7218/10000 --- Training Loss:0.380547\n",
      "Epoch: 145/200, Iteration: 7220/10000 --- Training Loss:0.578481\n",
      "Epoch: 145/200, Iteration: 7222/10000 --- Training Loss:0.600100\n",
      "Epoch: 145/200, Iteration: 7224/10000 --- Training Loss:0.307610\n",
      "Epoch: 145/200, Iteration: 7226/10000 --- Training Loss:0.157559\n",
      "Epoch: 145/200, Iteration: 7228/10000 --- Training Loss:0.821456\n",
      "Epoch: 145/200, Iteration: 7230/10000 --- Training Loss:0.515060\n",
      "Epoch: 145/200, Iteration: 7232/10000 --- Training Loss:0.837078\n",
      "Epoch: 145/200, Iteration: 7234/10000 --- Training Loss:1.734767\n",
      "Epoch: 145/200, Iteration: 7236/10000 --- Training Loss:0.346671\n",
      "Epoch: 145/200, Iteration: 7238/10000 --- Training Loss:0.419964\n",
      "Epoch: 145/200, Iteration: 7240/10000 --- Training Loss:0.642973\n",
      "Epoch: 145/200, Iteration: 7242/10000 --- Training Loss:0.620956\n",
      "Epoch: 145/200, Iteration: 7244/10000 --- Training Loss:0.320168\n",
      "Epoch: 145/200, Iteration: 7246/10000 --- Training Loss:0.988592\n",
      "Epoch: 145/200, Iteration: 7248/10000 --- Training Loss:0.687033\n",
      "Epoch: 145/200, Iteration: 7250/10000 --- Training Loss:0.716243\n",
      "Epoch: 145 finished ! Train Loss: 0.58905, Test Loss: 1.71555\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 72 percent completed\n",
      "Epoch: 146/200, Iteration: 7252/10000 --- Training Loss:0.554935\n",
      "Epoch: 146/200, Iteration: 7254/10000 --- Training Loss:0.783352\n",
      "Epoch: 146/200, Iteration: 7256/10000 --- Training Loss:1.710146\n",
      "Epoch: 146/200, Iteration: 7258/10000 --- Training Loss:0.512686\n",
      "Epoch: 146/200, Iteration: 7260/10000 --- Training Loss:0.303403\n",
      "Epoch: 146/200, Iteration: 7262/10000 --- Training Loss:0.694504\n",
      "Epoch: 146/200, Iteration: 7264/10000 --- Training Loss:0.283820\n",
      "Epoch: 146/200, Iteration: 7266/10000 --- Training Loss:0.426431\n",
      "Epoch: 146/200, Iteration: 7268/10000 --- Training Loss:0.456624\n",
      "Epoch: 146/200, Iteration: 7270/10000 --- Training Loss:0.315146\n",
      "Epoch: 146/200, Iteration: 7272/10000 --- Training Loss:0.377981\n",
      "Epoch: 146/200, Iteration: 7274/10000 --- Training Loss:0.446952\n",
      "Epoch: 146/200, Iteration: 7276/10000 --- Training Loss:0.912968\n",
      "Epoch: 146/200, Iteration: 7278/10000 --- Training Loss:0.627936\n",
      "Epoch: 146/200, Iteration: 7280/10000 --- Training Loss:0.688533\n",
      "Epoch: 146/200, Iteration: 7282/10000 --- Training Loss:0.671658\n",
      "Epoch: 146/200, Iteration: 7284/10000 --- Training Loss:1.214160\n",
      "Epoch: 146/200, Iteration: 7286/10000 --- Training Loss:0.310271\n",
      "Epoch: 146/200, Iteration: 7288/10000 --- Training Loss:0.462116\n",
      "Epoch: 146/200, Iteration: 7290/10000 --- Training Loss:0.285654\n",
      "Epoch: 146/200, Iteration: 7292/10000 --- Training Loss:0.183843\n",
      "Epoch: 146/200, Iteration: 7294/10000 --- Training Loss:0.379837\n",
      "Epoch: 146/200, Iteration: 7296/10000 --- Training Loss:0.295716\n",
      "Epoch: 146/200, Iteration: 7298/10000 --- Training Loss:0.272547\n",
      "Epoch: 146/200, Iteration: 7300/10000 --- Training Loss:0.376069\n",
      "Epoch: 146 finished ! Train Loss: 0.50325, Test Loss: 2.11524\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 147/200, Iteration: 7302/10000 --- Training Loss:0.383200\n",
      "Epoch: 147/200, Iteration: 7304/10000 --- Training Loss:0.334486\n",
      "Epoch: 147/200, Iteration: 7306/10000 --- Training Loss:0.466700\n",
      "Epoch: 147/200, Iteration: 7308/10000 --- Training Loss:0.202127\n",
      "Epoch: 147/200, Iteration: 7310/10000 --- Training Loss:0.246314\n",
      "Epoch: 147/200, Iteration: 7312/10000 --- Training Loss:0.251064\n",
      "Epoch: 147/200, Iteration: 7314/10000 --- Training Loss:0.789403\n",
      "Epoch: 147/200, Iteration: 7316/10000 --- Training Loss:1.361160\n",
      "Epoch: 147/200, Iteration: 7318/10000 --- Training Loss:0.243596\n",
      "Epoch: 147/200, Iteration: 7320/10000 --- Training Loss:0.355215\n",
      "Epoch: 147/200, Iteration: 7322/10000 --- Training Loss:0.407042\n",
      "Epoch: 147/200, Iteration: 7324/10000 --- Training Loss:0.130060\n",
      "Epoch: 147/200, Iteration: 7326/10000 --- Training Loss:0.972075\n",
      "Epoch: 147/200, Iteration: 7328/10000 --- Training Loss:0.412922\n",
      "Epoch: 147/200, Iteration: 7330/10000 --- Training Loss:0.379095\n",
      "Epoch: 147/200, Iteration: 7332/10000 --- Training Loss:0.180048\n",
      "Epoch: 147/200, Iteration: 7334/10000 --- Training Loss:0.303390\n",
      "Epoch: 147/200, Iteration: 7336/10000 --- Training Loss:0.291098\n",
      "Epoch: 147/200, Iteration: 7338/10000 --- Training Loss:0.477833\n",
      "Epoch: 147/200, Iteration: 7340/10000 --- Training Loss:0.231859\n",
      "Epoch: 147/200, Iteration: 7342/10000 --- Training Loss:0.406561\n",
      "Epoch: 147/200, Iteration: 7344/10000 --- Training Loss:0.746528\n",
      "Epoch: 147/200, Iteration: 7346/10000 --- Training Loss:0.490480\n",
      "Epoch: 147/200, Iteration: 7348/10000 --- Training Loss:1.140875\n",
      "Epoch: 147/200, Iteration: 7350/10000 --- Training Loss:0.374132\n",
      "Epoch: 147 finished ! Train Loss: 0.54954, Test Loss: 1.68176\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 73 percent completed\n",
      "Epoch: 148/200, Iteration: 7352/10000 --- Training Loss:0.589339\n",
      "Epoch: 148/200, Iteration: 7354/10000 --- Training Loss:0.522975\n",
      "Epoch: 148/200, Iteration: 7356/10000 --- Training Loss:0.910502\n",
      "Epoch: 148/200, Iteration: 7358/10000 --- Training Loss:0.453466\n",
      "Epoch: 148/200, Iteration: 7360/10000 --- Training Loss:0.716973\n",
      "Epoch: 148/200, Iteration: 7362/10000 --- Training Loss:0.339476\n",
      "Epoch: 148/200, Iteration: 7364/10000 --- Training Loss:0.939307\n",
      "Epoch: 148/200, Iteration: 7366/10000 --- Training Loss:0.395218\n",
      "Epoch: 148/200, Iteration: 7368/10000 --- Training Loss:1.112363\n",
      "Epoch: 148/200, Iteration: 7370/10000 --- Training Loss:0.720896\n",
      "Epoch: 148/200, Iteration: 7372/10000 --- Training Loss:1.271957\n",
      "Epoch: 148/200, Iteration: 7374/10000 --- Training Loss:0.297030\n",
      "Epoch: 148/200, Iteration: 7376/10000 --- Training Loss:0.599413\n",
      "Epoch: 148/200, Iteration: 7378/10000 --- Training Loss:1.318478\n",
      "Epoch: 148/200, Iteration: 7380/10000 --- Training Loss:1.285500\n",
      "Epoch: 148/200, Iteration: 7382/10000 --- Training Loss:0.408016\n",
      "Epoch: 148/200, Iteration: 7384/10000 --- Training Loss:2.336925\n",
      "Epoch: 148/200, Iteration: 7386/10000 --- Training Loss:0.348496\n",
      "Epoch: 148/200, Iteration: 7388/10000 --- Training Loss:1.304568\n",
      "Epoch: 148/200, Iteration: 7390/10000 --- Training Loss:0.571691\n",
      "Epoch: 148/200, Iteration: 7392/10000 --- Training Loss:1.367372\n",
      "Epoch: 148/200, Iteration: 7394/10000 --- Training Loss:1.123040\n",
      "Epoch: 148/200, Iteration: 7396/10000 --- Training Loss:0.979159\n",
      "Epoch: 148/200, Iteration: 7398/10000 --- Training Loss:0.867645\n",
      "Epoch: 148/200, Iteration: 7400/10000 --- Training Loss:1.375415\n",
      "Epoch: 148 finished ! Train Loss: 0.93122, Test Loss: 2.35037\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 149/200, Iteration: 7402/10000 --- Training Loss:1.530450\n",
      "Epoch: 149/200, Iteration: 7404/10000 --- Training Loss:2.175780\n",
      "Epoch: 149/200, Iteration: 7406/10000 --- Training Loss:0.495170\n",
      "Epoch: 149/200, Iteration: 7408/10000 --- Training Loss:1.622080\n",
      "Epoch: 149/200, Iteration: 7410/10000 --- Training Loss:0.401750\n",
      "Epoch: 149/200, Iteration: 7412/10000 --- Training Loss:0.757535\n",
      "Epoch: 149/200, Iteration: 7414/10000 --- Training Loss:0.492433\n",
      "Epoch: 149/200, Iteration: 7416/10000 --- Training Loss:0.418629\n",
      "Epoch: 149/200, Iteration: 7418/10000 --- Training Loss:0.233440\n",
      "Epoch: 149/200, Iteration: 7420/10000 --- Training Loss:0.976492\n",
      "Epoch: 149/200, Iteration: 7422/10000 --- Training Loss:0.684316\n",
      "Epoch: 149/200, Iteration: 7424/10000 --- Training Loss:0.556884\n",
      "Epoch: 149/200, Iteration: 7426/10000 --- Training Loss:0.225811\n",
      "Epoch: 149/200, Iteration: 7428/10000 --- Training Loss:0.482145\n",
      "Epoch: 149/200, Iteration: 7430/10000 --- Training Loss:1.699184\n",
      "Epoch: 149/200, Iteration: 7432/10000 --- Training Loss:0.444668\n",
      "Epoch: 149/200, Iteration: 7434/10000 --- Training Loss:0.616440\n",
      "Epoch: 149/200, Iteration: 7436/10000 --- Training Loss:0.274300\n",
      "Epoch: 149/200, Iteration: 7438/10000 --- Training Loss:0.735610\n",
      "Epoch: 149/200, Iteration: 7440/10000 --- Training Loss:0.360776\n",
      "Epoch: 149/200, Iteration: 7442/10000 --- Training Loss:0.726630\n",
      "Epoch: 149/200, Iteration: 7444/10000 --- Training Loss:0.474426\n",
      "Epoch: 149/200, Iteration: 7446/10000 --- Training Loss:0.404427\n",
      "Epoch: 149/200, Iteration: 7448/10000 --- Training Loss:0.797317\n",
      "Epoch: 149/200, Iteration: 7450/10000 --- Training Loss:0.421666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149 finished ! Train Loss: 0.79999, Test Loss: 2.50786\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 150/200, Iteration: 7452/10000 --- Training Loss:1.105686\n",
      "Epoch: 150/200, Iteration: 7454/10000 --- Training Loss:0.275608\n",
      "Epoch: 150/200, Iteration: 7456/10000 --- Training Loss:0.323059\n",
      "Epoch: 150/200, Iteration: 7458/10000 --- Training Loss:0.259161\n",
      "Epoch: 150/200, Iteration: 7460/10000 --- Training Loss:0.564979\n",
      "Epoch: 150/200, Iteration: 7462/10000 --- Training Loss:0.482674\n",
      "Epoch: 150/200, Iteration: 7464/10000 --- Training Loss:0.671349\n",
      "Epoch: 150/200, Iteration: 7466/10000 --- Training Loss:0.188682\n",
      "Epoch: 150/200, Iteration: 7468/10000 --- Training Loss:1.560105\n",
      "Epoch: 150/200, Iteration: 7470/10000 --- Training Loss:0.228225\n",
      "Epoch: 150/200, Iteration: 7472/10000 --- Training Loss:0.424478\n",
      "Epoch: 150/200, Iteration: 7474/10000 --- Training Loss:0.158395\n",
      "Epoch: 150/200, Iteration: 7476/10000 --- Training Loss:0.650456\n",
      "Epoch: 150/200, Iteration: 7478/10000 --- Training Loss:0.688432\n",
      "Epoch: 150/200, Iteration: 7480/10000 --- Training Loss:0.295514\n",
      "Epoch: 150/200, Iteration: 7482/10000 --- Training Loss:0.505594\n",
      "Epoch: 150/200, Iteration: 7484/10000 --- Training Loss:0.470587\n",
      "Epoch: 150/200, Iteration: 7486/10000 --- Training Loss:0.650190\n",
      "Epoch: 150/200, Iteration: 7488/10000 --- Training Loss:0.361848\n",
      "Epoch: 150/200, Iteration: 7490/10000 --- Training Loss:0.660254\n",
      "Epoch: 150/200, Iteration: 7492/10000 --- Training Loss:0.268941\n",
      "Epoch: 150/200, Iteration: 7494/10000 --- Training Loss:0.484999\n",
      "Epoch: 150/200, Iteration: 7496/10000 --- Training Loss:0.265541\n",
      "Epoch: 150/200, Iteration: 7498/10000 --- Training Loss:0.240816\n",
      "Epoch: 150/200, Iteration: 7500/10000 --- Training Loss:0.492453\n",
      "Epoch: 150 finished ! Train Loss: 0.51654, Test Loss: 2.17295\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 151/200, Iteration: 7502/10000 --- Training Loss:0.260565\n",
      "Epoch: 151/200, Iteration: 7504/10000 --- Training Loss:0.439578\n",
      "Epoch: 151/200, Iteration: 7506/10000 --- Training Loss:0.319948\n",
      "Epoch: 151/200, Iteration: 7508/10000 --- Training Loss:0.315348\n",
      "Epoch: 151/200, Iteration: 7510/10000 --- Training Loss:1.229332\n",
      "Epoch: 151/200, Iteration: 7512/10000 --- Training Loss:0.645799\n",
      "Epoch: 151/200, Iteration: 7514/10000 --- Training Loss:0.268841\n",
      "Epoch: 151/200, Iteration: 7516/10000 --- Training Loss:0.288799\n",
      "Epoch: 151/200, Iteration: 7518/10000 --- Training Loss:0.348552\n",
      "Epoch: 151/200, Iteration: 7520/10000 --- Training Loss:0.637387\n",
      "Epoch: 151/200, Iteration: 7522/10000 --- Training Loss:1.352266\n",
      "Epoch: 151/200, Iteration: 7524/10000 --- Training Loss:0.168536\n",
      "Epoch: 151/200, Iteration: 7526/10000 --- Training Loss:0.109344\n",
      "Epoch: 151/200, Iteration: 7528/10000 --- Training Loss:0.231138\n",
      "Epoch: 151/200, Iteration: 7530/10000 --- Training Loss:0.726948\n",
      "Epoch: 151/200, Iteration: 7532/10000 --- Training Loss:0.829249\n",
      "Epoch: 151/200, Iteration: 7534/10000 --- Training Loss:0.537017\n",
      "Epoch: 151/200, Iteration: 7536/10000 --- Training Loss:0.318159\n",
      "Epoch: 151/200, Iteration: 7538/10000 --- Training Loss:0.206968\n",
      "Epoch: 151/200, Iteration: 7540/10000 --- Training Loss:0.412263\n",
      "Epoch: 151/200, Iteration: 7542/10000 --- Training Loss:0.364195\n",
      "Epoch: 151/200, Iteration: 7544/10000 --- Training Loss:1.253966\n",
      "Epoch: 151/200, Iteration: 7546/10000 --- Training Loss:0.450041\n",
      "Epoch: 151/200, Iteration: 7548/10000 --- Training Loss:0.393972\n",
      "Epoch: 151/200, Iteration: 7550/10000 --- Training Loss:0.117393\n",
      "Epoch: 151 finished ! Train Loss: 0.51072, Test Loss: 2.34276\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 152/200, Iteration: 7552/10000 --- Training Loss:0.868253\n",
      "Epoch: 152/200, Iteration: 7554/10000 --- Training Loss:0.198019\n",
      "Epoch: 152/200, Iteration: 7556/10000 --- Training Loss:0.428289\n",
      "Epoch: 152/200, Iteration: 7558/10000 --- Training Loss:0.535094\n",
      "Epoch: 152/200, Iteration: 7560/10000 --- Training Loss:0.476901\n",
      "Epoch: 152/200, Iteration: 7562/10000 --- Training Loss:0.416441\n",
      "Epoch: 152/200, Iteration: 7564/10000 --- Training Loss:0.444248\n",
      "Epoch: 152/200, Iteration: 7566/10000 --- Training Loss:0.873233\n",
      "Epoch: 152/200, Iteration: 7568/10000 --- Training Loss:0.243728\n",
      "Epoch: 152/200, Iteration: 7570/10000 --- Training Loss:0.821595\n",
      "Epoch: 152/200, Iteration: 7572/10000 --- Training Loss:0.597573\n",
      "Epoch: 152/200, Iteration: 7574/10000 --- Training Loss:0.662974\n",
      "Epoch: 152/200, Iteration: 7576/10000 --- Training Loss:0.119068\n",
      "Epoch: 152/200, Iteration: 7578/10000 --- Training Loss:0.377858\n",
      "Epoch: 152/200, Iteration: 7580/10000 --- Training Loss:1.144640\n",
      "Epoch: 152/200, Iteration: 7582/10000 --- Training Loss:0.275223\n",
      "Epoch: 152/200, Iteration: 7584/10000 --- Training Loss:0.364652\n",
      "Epoch: 152/200, Iteration: 7586/10000 --- Training Loss:0.260414\n",
      "Epoch: 152/200, Iteration: 7588/10000 --- Training Loss:0.236250\n",
      "Epoch: 152/200, Iteration: 7590/10000 --- Training Loss:0.317155\n",
      "Epoch: 152/200, Iteration: 7592/10000 --- Training Loss:0.747905\n",
      "Epoch: 152/200, Iteration: 7594/10000 --- Training Loss:0.314199\n",
      "Epoch: 152/200, Iteration: 7596/10000 --- Training Loss:0.633800\n",
      "Epoch: 152/200, Iteration: 7598/10000 --- Training Loss:0.244061\n",
      "Epoch: 152/200, Iteration: 7600/10000 --- Training Loss:0.476153\n",
      "Epoch: 152 finished ! Train Loss: 0.51943, Test Loss: 1.88076\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 153/200, Iteration: 7602/10000 --- Training Loss:0.637432\n",
      "Epoch: 153/200, Iteration: 7604/10000 --- Training Loss:0.234636\n",
      "Epoch: 153/200, Iteration: 7606/10000 --- Training Loss:0.190843\n",
      "Epoch: 153/200, Iteration: 7608/10000 --- Training Loss:0.693377\n",
      "Epoch: 153/200, Iteration: 7610/10000 --- Training Loss:0.394961\n",
      "Epoch: 153/200, Iteration: 7612/10000 --- Training Loss:0.354359\n",
      "Epoch: 153/200, Iteration: 7614/10000 --- Training Loss:0.607136\n",
      "Epoch: 153/200, Iteration: 7616/10000 --- Training Loss:0.977047\n",
      "Epoch: 153/200, Iteration: 7618/10000 --- Training Loss:0.462166\n",
      "Epoch: 153/200, Iteration: 7620/10000 --- Training Loss:0.313248\n",
      "Epoch: 153/200, Iteration: 7622/10000 --- Training Loss:0.398130\n",
      "Epoch: 153/200, Iteration: 7624/10000 --- Training Loss:1.597108\n",
      "Epoch: 153/200, Iteration: 7626/10000 --- Training Loss:0.382995\n",
      "Epoch: 153/200, Iteration: 7628/10000 --- Training Loss:0.443601\n",
      "Epoch: 153/200, Iteration: 7630/10000 --- Training Loss:0.220759\n",
      "Epoch: 153/200, Iteration: 7632/10000 --- Training Loss:0.629119\n",
      "Epoch: 153/200, Iteration: 7634/10000 --- Training Loss:0.212486\n",
      "Epoch: 153/200, Iteration: 7636/10000 --- Training Loss:0.619880\n",
      "Epoch: 153/200, Iteration: 7638/10000 --- Training Loss:0.388694\n",
      "Epoch: 153/200, Iteration: 7640/10000 --- Training Loss:0.502200\n",
      "Epoch: 153/200, Iteration: 7642/10000 --- Training Loss:0.820145\n",
      "Epoch: 153/200, Iteration: 7644/10000 --- Training Loss:0.315243\n",
      "Epoch: 153/200, Iteration: 7646/10000 --- Training Loss:0.323417\n",
      "Epoch: 153/200, Iteration: 7648/10000 --- Training Loss:0.233774\n",
      "Epoch: 153/200, Iteration: 7650/10000 --- Training Loss:0.538592\n",
      "Epoch: 153 finished ! Train Loss: 0.58769, Test Loss: 2.28830\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 154/200, Iteration: 7652/10000 --- Training Loss:0.324047\n",
      "Epoch: 154/200, Iteration: 7654/10000 --- Training Loss:0.371446\n",
      "Epoch: 154/200, Iteration: 7656/10000 --- Training Loss:0.665718\n",
      "Epoch: 154/200, Iteration: 7658/10000 --- Training Loss:0.519973\n",
      "Epoch: 154/200, Iteration: 7660/10000 --- Training Loss:0.233332\n",
      "Epoch: 154/200, Iteration: 7662/10000 --- Training Loss:0.341736\n",
      "Epoch: 154/200, Iteration: 7664/10000 --- Training Loss:0.849247\n",
      "Epoch: 154/200, Iteration: 7666/10000 --- Training Loss:0.140671\n",
      "Epoch: 154/200, Iteration: 7668/10000 --- Training Loss:0.335860\n",
      "Epoch: 154/200, Iteration: 7670/10000 --- Training Loss:0.561215\n",
      "Epoch: 154/200, Iteration: 7672/10000 --- Training Loss:2.052341\n",
      "Epoch: 154/200, Iteration: 7674/10000 --- Training Loss:1.404705\n",
      "Epoch: 154/200, Iteration: 7676/10000 --- Training Loss:0.400217\n",
      "Epoch: 154/200, Iteration: 7678/10000 --- Training Loss:0.698619\n",
      "Epoch: 154/200, Iteration: 7680/10000 --- Training Loss:1.417618\n",
      "Epoch: 154/200, Iteration: 7682/10000 --- Training Loss:1.539583\n",
      "Epoch: 154/200, Iteration: 7684/10000 --- Training Loss:0.661160\n",
      "Epoch: 154/200, Iteration: 7686/10000 --- Training Loss:1.223644\n",
      "Epoch: 154/200, Iteration: 7688/10000 --- Training Loss:0.294294\n",
      "Epoch: 154/200, Iteration: 7690/10000 --- Training Loss:1.114259\n",
      "Epoch: 154/200, Iteration: 7692/10000 --- Training Loss:0.535515\n",
      "Epoch: 154/200, Iteration: 7694/10000 --- Training Loss:1.300598\n",
      "Epoch: 154/200, Iteration: 7696/10000 --- Training Loss:0.478439\n",
      "Epoch: 154/200, Iteration: 7698/10000 --- Training Loss:0.928332\n",
      "Epoch: 154/200, Iteration: 7700/10000 --- Training Loss:0.443797\n",
      "Epoch: 154 finished ! Train Loss: 0.71595, Test Loss: 1.98319\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 155/200, Iteration: 7702/10000 --- Training Loss:0.596835\n",
      "Epoch: 155/200, Iteration: 7704/10000 --- Training Loss:0.249899\n",
      "Epoch: 155/200, Iteration: 7706/10000 --- Training Loss:0.247811\n",
      "Epoch: 155/200, Iteration: 7708/10000 --- Training Loss:0.372079\n",
      "Epoch: 155/200, Iteration: 7710/10000 --- Training Loss:1.080997\n",
      "Epoch: 155/200, Iteration: 7712/10000 --- Training Loss:0.463767\n",
      "Epoch: 155/200, Iteration: 7714/10000 --- Training Loss:0.246449\n",
      "Epoch: 155/200, Iteration: 7716/10000 --- Training Loss:0.574815\n",
      "Epoch: 155/200, Iteration: 7718/10000 --- Training Loss:0.701222\n",
      "Epoch: 155/200, Iteration: 7720/10000 --- Training Loss:0.488541\n",
      "Epoch: 155/200, Iteration: 7722/10000 --- Training Loss:0.154817\n",
      "Epoch: 155/200, Iteration: 7724/10000 --- Training Loss:0.413861\n",
      "Epoch: 155/200, Iteration: 7726/10000 --- Training Loss:0.690282\n",
      "Epoch: 155/200, Iteration: 7728/10000 --- Training Loss:0.258883\n",
      "Epoch: 155/200, Iteration: 7730/10000 --- Training Loss:0.434738\n",
      "Epoch: 155/200, Iteration: 7732/10000 --- Training Loss:0.857730\n",
      "Epoch: 155/200, Iteration: 7734/10000 --- Training Loss:0.727461\n",
      "Epoch: 155/200, Iteration: 7736/10000 --- Training Loss:0.412992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155/200, Iteration: 7738/10000 --- Training Loss:0.920870\n",
      "Epoch: 155/200, Iteration: 7740/10000 --- Training Loss:0.465506\n",
      "Epoch: 155/200, Iteration: 7742/10000 --- Training Loss:1.397989\n",
      "Epoch: 155/200, Iteration: 7744/10000 --- Training Loss:0.666533\n",
      "Epoch: 155/200, Iteration: 7746/10000 --- Training Loss:0.546012\n",
      "Epoch: 155/200, Iteration: 7748/10000 --- Training Loss:0.578303\n",
      "Epoch: 155/200, Iteration: 7750/10000 --- Training Loss:0.406150\n",
      "Epoch: 155 finished ! Train Loss: 0.60475, Test Loss: 2.45983\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 156/200, Iteration: 7752/10000 --- Training Loss:0.473703\n",
      "Epoch: 156/200, Iteration: 7754/10000 --- Training Loss:0.420197\n",
      "Epoch: 156/200, Iteration: 7756/10000 --- Training Loss:0.416296\n",
      "Epoch: 156/200, Iteration: 7758/10000 --- Training Loss:0.564551\n",
      "Epoch: 156/200, Iteration: 7760/10000 --- Training Loss:1.448318\n",
      "Epoch: 156/200, Iteration: 7762/10000 --- Training Loss:0.167421\n",
      "Epoch: 156/200, Iteration: 7764/10000 --- Training Loss:1.182186\n",
      "Epoch: 156/200, Iteration: 7766/10000 --- Training Loss:0.376604\n",
      "Epoch: 156/200, Iteration: 7768/10000 --- Training Loss:0.499593\n",
      "Epoch: 156/200, Iteration: 7770/10000 --- Training Loss:1.172931\n",
      "Epoch: 156/200, Iteration: 7772/10000 --- Training Loss:0.353675\n",
      "Epoch: 156/200, Iteration: 7774/10000 --- Training Loss:0.827251\n",
      "Epoch: 156/200, Iteration: 7776/10000 --- Training Loss:0.423460\n",
      "Epoch: 156/200, Iteration: 7778/10000 --- Training Loss:1.570858\n",
      "Epoch: 156/200, Iteration: 7780/10000 --- Training Loss:0.391053\n",
      "Epoch: 156/200, Iteration: 7782/10000 --- Training Loss:0.711498\n",
      "Epoch: 156/200, Iteration: 7784/10000 --- Training Loss:0.382892\n",
      "Epoch: 156/200, Iteration: 7786/10000 --- Training Loss:0.427310\n",
      "Epoch: 156/200, Iteration: 7788/10000 --- Training Loss:0.433055\n",
      "Epoch: 156/200, Iteration: 7790/10000 --- Training Loss:0.368829\n",
      "Epoch: 156/200, Iteration: 7792/10000 --- Training Loss:0.196458\n",
      "Epoch: 156/200, Iteration: 7794/10000 --- Training Loss:0.808628\n",
      "Epoch: 156/200, Iteration: 7796/10000 --- Training Loss:0.571230\n",
      "Epoch: 156/200, Iteration: 7798/10000 --- Training Loss:0.720439\n",
      "Epoch: 156/200, Iteration: 7800/10000 --- Training Loss:0.109574\n",
      "Epoch: 156 finished ! Train Loss: 0.61090, Test Loss: 1.41836\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 78 percent completed\n",
      "Epoch: 157/200, Iteration: 7802/10000 --- Training Loss:1.705803\n",
      "Epoch: 157/200, Iteration: 7804/10000 --- Training Loss:1.422276\n",
      "Epoch: 157/200, Iteration: 7806/10000 --- Training Loss:1.499896\n",
      "Epoch: 157/200, Iteration: 7808/10000 --- Training Loss:0.351730\n",
      "Epoch: 157/200, Iteration: 7810/10000 --- Training Loss:1.789382\n",
      "Epoch: 157/200, Iteration: 7812/10000 --- Training Loss:0.752048\n",
      "Epoch: 157/200, Iteration: 7814/10000 --- Training Loss:0.645498\n",
      "Epoch: 157/200, Iteration: 7816/10000 --- Training Loss:0.778021\n",
      "Epoch: 157/200, Iteration: 7818/10000 --- Training Loss:0.412203\n",
      "Epoch: 157/200, Iteration: 7820/10000 --- Training Loss:0.619870\n",
      "Epoch: 157/200, Iteration: 7822/10000 --- Training Loss:0.067235\n",
      "Epoch: 157/200, Iteration: 7824/10000 --- Training Loss:0.734636\n",
      "Epoch: 157/200, Iteration: 7826/10000 --- Training Loss:0.558324\n",
      "Epoch: 157/200, Iteration: 7828/10000 --- Training Loss:0.560524\n",
      "Epoch: 157/200, Iteration: 7830/10000 --- Training Loss:0.173495\n",
      "Epoch: 157/200, Iteration: 7832/10000 --- Training Loss:0.134612\n",
      "Epoch: 157/200, Iteration: 7834/10000 --- Training Loss:0.485251\n",
      "Epoch: 157/200, Iteration: 7836/10000 --- Training Loss:0.216889\n",
      "Epoch: 157/200, Iteration: 7838/10000 --- Training Loss:0.375246\n",
      "Epoch: 157/200, Iteration: 7840/10000 --- Training Loss:0.341050\n",
      "Epoch: 157/200, Iteration: 7842/10000 --- Training Loss:0.329515\n",
      "Epoch: 157/200, Iteration: 7844/10000 --- Training Loss:0.486569\n",
      "Epoch: 157/200, Iteration: 7846/10000 --- Training Loss:0.394112\n",
      "Epoch: 157/200, Iteration: 7848/10000 --- Training Loss:0.346688\n",
      "Epoch: 157/200, Iteration: 7850/10000 --- Training Loss:0.205072\n",
      "Epoch: 157 finished ! Train Loss: 0.66424, Test Loss: 1.60669\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 158/200, Iteration: 7852/10000 --- Training Loss:0.359666\n",
      "Epoch: 158/200, Iteration: 7854/10000 --- Training Loss:0.430514\n",
      "Epoch: 158/200, Iteration: 7856/10000 --- Training Loss:0.759229\n",
      "Epoch: 158/200, Iteration: 7858/10000 --- Training Loss:0.214726\n",
      "Epoch: 158/200, Iteration: 7860/10000 --- Training Loss:0.910211\n",
      "Epoch: 158/200, Iteration: 7862/10000 --- Training Loss:0.520121\n",
      "Epoch: 158/200, Iteration: 7864/10000 --- Training Loss:0.539221\n",
      "Epoch: 158/200, Iteration: 7866/10000 --- Training Loss:0.441016\n",
      "Epoch: 158/200, Iteration: 7868/10000 --- Training Loss:0.406615\n",
      "Epoch: 158/200, Iteration: 7870/10000 --- Training Loss:0.343882\n",
      "Epoch: 158/200, Iteration: 7872/10000 --- Training Loss:0.511933\n",
      "Epoch: 158/200, Iteration: 7874/10000 --- Training Loss:0.331753\n",
      "Epoch: 158/200, Iteration: 7876/10000 --- Training Loss:0.814240\n",
      "Epoch: 158/200, Iteration: 7878/10000 --- Training Loss:0.281636\n",
      "Epoch: 158/200, Iteration: 7880/10000 --- Training Loss:0.880738\n",
      "Epoch: 158/200, Iteration: 7882/10000 --- Training Loss:0.327605\n",
      "Epoch: 158/200, Iteration: 7884/10000 --- Training Loss:0.282526\n",
      "Epoch: 158/200, Iteration: 7886/10000 --- Training Loss:0.415625\n",
      "Epoch: 158/200, Iteration: 7888/10000 --- Training Loss:0.329606\n",
      "Epoch: 158/200, Iteration: 7890/10000 --- Training Loss:0.067669\n",
      "Epoch: 158/200, Iteration: 7892/10000 --- Training Loss:1.063904\n",
      "Epoch: 158/200, Iteration: 7894/10000 --- Training Loss:0.319278\n",
      "Epoch: 158/200, Iteration: 7896/10000 --- Training Loss:0.909541\n",
      "Epoch: 158/200, Iteration: 7898/10000 --- Training Loss:0.920757\n",
      "Epoch: 158/200, Iteration: 7900/10000 --- Training Loss:0.837541\n",
      "Epoch: 158 finished ! Train Loss: 0.54690, Test Loss: 1.61563\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 159/200, Iteration: 7902/10000 --- Training Loss:0.692932\n",
      "Epoch: 159/200, Iteration: 7904/10000 --- Training Loss:0.359960\n",
      "Epoch: 159/200, Iteration: 7906/10000 --- Training Loss:0.264680\n",
      "Epoch: 159/200, Iteration: 7908/10000 --- Training Loss:1.110553\n",
      "Epoch: 159/200, Iteration: 7910/10000 --- Training Loss:0.275607\n",
      "Epoch: 159/200, Iteration: 7912/10000 --- Training Loss:0.263285\n",
      "Epoch: 159/200, Iteration: 7914/10000 --- Training Loss:2.004860\n",
      "Epoch: 159/200, Iteration: 7916/10000 --- Training Loss:0.711066\n",
      "Epoch: 159/200, Iteration: 7918/10000 --- Training Loss:0.553658\n",
      "Epoch: 159/200, Iteration: 7920/10000 --- Training Loss:0.471979\n",
      "Epoch: 159/200, Iteration: 7922/10000 --- Training Loss:0.485938\n",
      "Epoch: 159/200, Iteration: 7924/10000 --- Training Loss:0.312518\n",
      "Epoch: 159/200, Iteration: 7926/10000 --- Training Loss:0.320532\n",
      "Epoch: 159/200, Iteration: 7928/10000 --- Training Loss:0.680102\n",
      "Epoch: 159/200, Iteration: 7930/10000 --- Training Loss:0.259463\n",
      "Epoch: 159/200, Iteration: 7932/10000 --- Training Loss:0.877813\n",
      "Epoch: 159/200, Iteration: 7934/10000 --- Training Loss:0.120738\n",
      "Epoch: 159/200, Iteration: 7936/10000 --- Training Loss:0.822919\n",
      "Epoch: 159/200, Iteration: 7938/10000 --- Training Loss:0.704109\n",
      "Epoch: 159/200, Iteration: 7940/10000 --- Training Loss:0.736466\n",
      "Epoch: 159/200, Iteration: 7942/10000 --- Training Loss:0.315941\n",
      "Epoch: 159/200, Iteration: 7944/10000 --- Training Loss:0.493694\n",
      "Epoch: 159/200, Iteration: 7946/10000 --- Training Loss:0.409121\n",
      "Epoch: 159/200, Iteration: 7948/10000 --- Training Loss:0.715951\n",
      "Epoch: 159/200, Iteration: 7950/10000 --- Training Loss:0.219381\n",
      "Epoch: 159 finished ! Train Loss: 0.56064, Test Loss: 2.11911\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 160/200, Iteration: 7952/10000 --- Training Loss:0.862067\n",
      "Epoch: 160/200, Iteration: 7954/10000 --- Training Loss:0.326165\n",
      "Epoch: 160/200, Iteration: 7956/10000 --- Training Loss:0.267759\n",
      "Epoch: 160/200, Iteration: 7958/10000 --- Training Loss:0.265115\n",
      "Epoch: 160/200, Iteration: 7960/10000 --- Training Loss:0.168632\n",
      "Epoch: 160/200, Iteration: 7962/10000 --- Training Loss:0.363227\n",
      "Epoch: 160/200, Iteration: 7964/10000 --- Training Loss:0.334858\n",
      "Epoch: 160/200, Iteration: 7966/10000 --- Training Loss:0.440921\n",
      "Epoch: 160/200, Iteration: 7968/10000 --- Training Loss:0.128568\n",
      "Epoch: 160/200, Iteration: 7970/10000 --- Training Loss:0.636299\n",
      "Epoch: 160/200, Iteration: 7972/10000 --- Training Loss:0.532349\n",
      "Epoch: 160/200, Iteration: 7974/10000 --- Training Loss:0.273138\n",
      "Epoch: 160/200, Iteration: 7976/10000 --- Training Loss:0.447665\n",
      "Epoch: 160/200, Iteration: 7978/10000 --- Training Loss:0.410539\n",
      "Epoch: 160/200, Iteration: 7980/10000 --- Training Loss:0.896497\n",
      "Epoch: 160/200, Iteration: 7982/10000 --- Training Loss:0.469979\n",
      "Epoch: 160/200, Iteration: 7984/10000 --- Training Loss:0.426452\n",
      "Epoch: 160/200, Iteration: 7986/10000 --- Training Loss:0.559582\n",
      "Epoch: 160/200, Iteration: 7988/10000 --- Training Loss:0.379080\n",
      "Epoch: 160/200, Iteration: 7990/10000 --- Training Loss:0.356056\n",
      "Epoch: 160/200, Iteration: 7992/10000 --- Training Loss:0.799678\n",
      "Epoch: 160/200, Iteration: 7994/10000 --- Training Loss:1.212685\n",
      "Epoch: 160/200, Iteration: 7996/10000 --- Training Loss:0.406951\n",
      "Epoch: 160/200, Iteration: 7998/10000 --- Training Loss:0.220038\n",
      "Epoch: 160/200, Iteration: 8000/10000 --- Training Loss:0.686617\n",
      "Epoch: 160 finished ! Train Loss: 0.50696, Test Loss: 3.51766\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 161/200, Iteration: 8002/10000 --- Training Loss:0.239967\n",
      "Epoch: 161/200, Iteration: 8004/10000 --- Training Loss:0.522309\n",
      "Epoch: 161/200, Iteration: 8006/10000 --- Training Loss:0.636024\n",
      "Epoch: 161/200, Iteration: 8008/10000 --- Training Loss:0.529588\n",
      "Epoch: 161/200, Iteration: 8010/10000 --- Training Loss:0.404664\n",
      "Epoch: 161/200, Iteration: 8012/10000 --- Training Loss:1.625856\n",
      "Epoch: 161/200, Iteration: 8014/10000 --- Training Loss:0.556290\n",
      "Epoch: 161/200, Iteration: 8016/10000 --- Training Loss:0.502072\n",
      "Epoch: 161/200, Iteration: 8018/10000 --- Training Loss:0.349977\n",
      "Epoch: 161/200, Iteration: 8020/10000 --- Training Loss:0.621173\n",
      "Epoch: 161/200, Iteration: 8022/10000 --- Training Loss:0.468099\n",
      "Epoch: 161/200, Iteration: 8024/10000 --- Training Loss:0.286319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 161/200, Iteration: 8026/10000 --- Training Loss:0.403716\n",
      "Epoch: 161/200, Iteration: 8028/10000 --- Training Loss:0.151071\n",
      "Epoch: 161/200, Iteration: 8030/10000 --- Training Loss:0.492183\n",
      "Epoch: 161/200, Iteration: 8032/10000 --- Training Loss:0.388970\n",
      "Epoch: 161/200, Iteration: 8034/10000 --- Training Loss:0.513556\n",
      "Epoch: 161/200, Iteration: 8036/10000 --- Training Loss:0.200330\n",
      "Epoch: 161/200, Iteration: 8038/10000 --- Training Loss:1.070586\n",
      "Epoch: 161/200, Iteration: 8040/10000 --- Training Loss:0.502143\n",
      "Epoch: 161/200, Iteration: 8042/10000 --- Training Loss:0.568622\n",
      "Epoch: 161/200, Iteration: 8044/10000 --- Training Loss:0.324289\n",
      "Epoch: 161/200, Iteration: 8046/10000 --- Training Loss:0.885247\n",
      "Epoch: 161/200, Iteration: 8048/10000 --- Training Loss:0.817316\n",
      "Epoch: 161/200, Iteration: 8050/10000 --- Training Loss:1.217633\n",
      "Epoch: 161 finished ! Train Loss: 0.57712, Test Loss: 3.20693\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 162/200, Iteration: 8052/10000 --- Training Loss:0.271716\n",
      "Epoch: 162/200, Iteration: 8054/10000 --- Training Loss:1.335250\n",
      "Epoch: 162/200, Iteration: 8056/10000 --- Training Loss:0.768456\n",
      "Epoch: 162/200, Iteration: 8058/10000 --- Training Loss:0.972274\n",
      "Epoch: 162/200, Iteration: 8060/10000 --- Training Loss:0.879164\n",
      "Epoch: 162/200, Iteration: 8062/10000 --- Training Loss:0.286158\n",
      "Epoch: 162/200, Iteration: 8064/10000 --- Training Loss:1.102201\n",
      "Epoch: 162/200, Iteration: 8066/10000 --- Training Loss:0.195195\n",
      "Epoch: 162/200, Iteration: 8068/10000 --- Training Loss:0.328875\n",
      "Epoch: 162/200, Iteration: 8070/10000 --- Training Loss:1.622923\n",
      "Epoch: 162/200, Iteration: 8072/10000 --- Training Loss:0.426486\n",
      "Epoch: 162/200, Iteration: 8074/10000 --- Training Loss:0.309050\n",
      "Epoch: 162/200, Iteration: 8076/10000 --- Training Loss:0.344744\n",
      "Epoch: 162/200, Iteration: 8078/10000 --- Training Loss:0.298521\n",
      "Epoch: 162/200, Iteration: 8080/10000 --- Training Loss:0.256887\n",
      "Epoch: 162/200, Iteration: 8082/10000 --- Training Loss:0.546838\n",
      "Epoch: 162/200, Iteration: 8084/10000 --- Training Loss:0.469528\n",
      "Epoch: 162/200, Iteration: 8086/10000 --- Training Loss:0.290767\n",
      "Epoch: 162/200, Iteration: 8088/10000 --- Training Loss:0.228398\n",
      "Epoch: 162/200, Iteration: 8090/10000 --- Training Loss:0.379199\n",
      "Epoch: 162/200, Iteration: 8092/10000 --- Training Loss:0.321489\n",
      "Epoch: 162/200, Iteration: 8094/10000 --- Training Loss:0.544612\n",
      "Epoch: 162/200, Iteration: 8096/10000 --- Training Loss:0.187907\n",
      "Epoch: 162/200, Iteration: 8098/10000 --- Training Loss:0.458021\n",
      "Epoch: 162/200, Iteration: 8100/10000 --- Training Loss:0.527314\n",
      "Epoch: 162 finished ! Train Loss: 0.63576, Test Loss: 2.83261\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 163/200, Iteration: 8102/10000 --- Training Loss:0.555621\n",
      "Epoch: 163/200, Iteration: 8104/10000 --- Training Loss:0.607735\n",
      "Epoch: 163/200, Iteration: 8106/10000 --- Training Loss:0.586049\n",
      "Epoch: 163/200, Iteration: 8108/10000 --- Training Loss:0.758336\n",
      "Epoch: 163/200, Iteration: 8110/10000 --- Training Loss:0.347738\n",
      "Epoch: 163/200, Iteration: 8112/10000 --- Training Loss:0.352643\n",
      "Epoch: 163/200, Iteration: 8114/10000 --- Training Loss:0.276924\n",
      "Epoch: 163/200, Iteration: 8116/10000 --- Training Loss:0.985882\n",
      "Epoch: 163/200, Iteration: 8118/10000 --- Training Loss:0.322460\n",
      "Epoch: 163/200, Iteration: 8120/10000 --- Training Loss:0.470669\n",
      "Epoch: 163/200, Iteration: 8122/10000 --- Training Loss:0.439474\n",
      "Epoch: 163/200, Iteration: 8124/10000 --- Training Loss:0.123554\n",
      "Epoch: 163/200, Iteration: 8126/10000 --- Training Loss:0.427704\n",
      "Epoch: 163/200, Iteration: 8128/10000 --- Training Loss:0.521960\n",
      "Epoch: 163/200, Iteration: 8130/10000 --- Training Loss:0.715790\n",
      "Epoch: 163/200, Iteration: 8132/10000 --- Training Loss:0.496462\n",
      "Epoch: 163/200, Iteration: 8134/10000 --- Training Loss:0.710845\n",
      "Epoch: 163/200, Iteration: 8136/10000 --- Training Loss:0.354109\n",
      "Epoch: 163/200, Iteration: 8138/10000 --- Training Loss:0.277648\n",
      "Epoch: 163/200, Iteration: 8140/10000 --- Training Loss:0.425473\n",
      "Epoch: 163/200, Iteration: 8142/10000 --- Training Loss:0.231846\n",
      "Epoch: 163/200, Iteration: 8144/10000 --- Training Loss:1.377906\n",
      "Epoch: 163/200, Iteration: 8146/10000 --- Training Loss:0.956769\n",
      "Epoch: 163/200, Iteration: 8148/10000 --- Training Loss:0.283756\n",
      "Epoch: 163/200, Iteration: 8150/10000 --- Training Loss:0.706062\n",
      "Epoch: 163 finished ! Train Loss: 0.56251, Test Loss: 1.95995\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 164/200, Iteration: 8152/10000 --- Training Loss:0.229589\n",
      "Epoch: 164/200, Iteration: 8154/10000 --- Training Loss:1.012993\n",
      "Epoch: 164/200, Iteration: 8156/10000 --- Training Loss:0.660780\n",
      "Epoch: 164/200, Iteration: 8158/10000 --- Training Loss:0.828541\n",
      "Epoch: 164/200, Iteration: 8160/10000 --- Training Loss:0.179892\n",
      "Epoch: 164/200, Iteration: 8162/10000 --- Training Loss:0.443029\n",
      "Epoch: 164/200, Iteration: 8164/10000 --- Training Loss:0.413622\n",
      "Epoch: 164/200, Iteration: 8166/10000 --- Training Loss:0.973204\n",
      "Epoch: 164/200, Iteration: 8168/10000 --- Training Loss:0.533288\n",
      "Epoch: 164/200, Iteration: 8170/10000 --- Training Loss:0.287319\n",
      "Epoch: 164/200, Iteration: 8172/10000 --- Training Loss:0.350077\n",
      "Epoch: 164/200, Iteration: 8174/10000 --- Training Loss:0.441521\n",
      "Epoch: 164/200, Iteration: 8176/10000 --- Training Loss:0.299859\n",
      "Epoch: 164/200, Iteration: 8178/10000 --- Training Loss:0.362870\n",
      "Epoch: 164/200, Iteration: 8180/10000 --- Training Loss:0.218500\n",
      "Epoch: 164/200, Iteration: 8182/10000 --- Training Loss:0.381052\n",
      "Epoch: 164/200, Iteration: 8184/10000 --- Training Loss:1.196543\n",
      "Epoch: 164/200, Iteration: 8186/10000 --- Training Loss:0.575374\n",
      "Epoch: 164/200, Iteration: 8188/10000 --- Training Loss:0.453719\n",
      "Epoch: 164/200, Iteration: 8190/10000 --- Training Loss:1.923130\n",
      "Epoch: 164/200, Iteration: 8192/10000 --- Training Loss:0.752279\n",
      "Epoch: 164/200, Iteration: 8194/10000 --- Training Loss:0.976931\n",
      "Epoch: 164/200, Iteration: 8196/10000 --- Training Loss:0.287934\n",
      "Epoch: 164/200, Iteration: 8198/10000 --- Training Loss:0.812366\n",
      "Epoch: 164/200, Iteration: 8200/10000 --- Training Loss:0.329716\n",
      "Epoch: 164 finished ! Train Loss: 0.57437, Test Loss: 2.38314\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 165/200, Iteration: 8202/10000 --- Training Loss:0.937817\n",
      "Epoch: 165/200, Iteration: 8204/10000 --- Training Loss:0.667712\n",
      "Epoch: 165/200, Iteration: 8206/10000 --- Training Loss:0.400786\n",
      "Epoch: 165/200, Iteration: 8208/10000 --- Training Loss:0.451690\n",
      "Epoch: 165/200, Iteration: 8210/10000 --- Training Loss:0.736098\n",
      "Epoch: 165/200, Iteration: 8212/10000 --- Training Loss:0.721143\n",
      "Epoch: 165/200, Iteration: 8214/10000 --- Training Loss:0.260238\n",
      "Epoch: 165/200, Iteration: 8216/10000 --- Training Loss:0.502805\n",
      "Epoch: 165/200, Iteration: 8218/10000 --- Training Loss:0.420866\n",
      "Epoch: 165/200, Iteration: 8220/10000 --- Training Loss:0.219594\n",
      "Epoch: 165/200, Iteration: 8222/10000 --- Training Loss:0.296751\n",
      "Epoch: 165/200, Iteration: 8224/10000 --- Training Loss:0.252222\n",
      "Epoch: 165/200, Iteration: 8226/10000 --- Training Loss:0.614184\n",
      "Epoch: 165/200, Iteration: 8228/10000 --- Training Loss:0.334286\n",
      "Epoch: 165/200, Iteration: 8230/10000 --- Training Loss:0.311642\n",
      "Epoch: 165/200, Iteration: 8232/10000 --- Training Loss:0.175305\n",
      "Epoch: 165/200, Iteration: 8234/10000 --- Training Loss:0.150616\n",
      "Epoch: 165/200, Iteration: 8236/10000 --- Training Loss:0.210040\n",
      "Epoch: 165/200, Iteration: 8238/10000 --- Training Loss:0.122853\n",
      "Epoch: 165/200, Iteration: 8240/10000 --- Training Loss:0.542332\n",
      "Epoch: 165/200, Iteration: 8242/10000 --- Training Loss:0.302650\n",
      "Epoch: 165/200, Iteration: 8244/10000 --- Training Loss:0.238947\n",
      "Epoch: 165/200, Iteration: 8246/10000 --- Training Loss:0.339153\n",
      "Epoch: 165/200, Iteration: 8248/10000 --- Training Loss:0.596873\n",
      "Epoch: 165/200, Iteration: 8250/10000 --- Training Loss:0.299568\n",
      "Epoch: 165 finished ! Train Loss: 0.48478, Test Loss: 1.61237\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 166/200, Iteration: 8252/10000 --- Training Loss:0.341340\n",
      "Epoch: 166/200, Iteration: 8254/10000 --- Training Loss:0.396144\n",
      "Epoch: 166/200, Iteration: 8256/10000 --- Training Loss:0.244443\n",
      "Epoch: 166/200, Iteration: 8258/10000 --- Training Loss:0.279209\n",
      "Epoch: 166/200, Iteration: 8260/10000 --- Training Loss:0.280284\n",
      "Epoch: 166/200, Iteration: 8262/10000 --- Training Loss:0.530238\n",
      "Epoch: 166/200, Iteration: 8264/10000 --- Training Loss:0.351355\n",
      "Epoch: 166/200, Iteration: 8266/10000 --- Training Loss:0.104279\n",
      "Epoch: 166/200, Iteration: 8268/10000 --- Training Loss:0.448007\n",
      "Epoch: 166/200, Iteration: 8270/10000 --- Training Loss:0.995765\n",
      "Epoch: 166/200, Iteration: 8272/10000 --- Training Loss:0.486718\n",
      "Epoch: 166/200, Iteration: 8274/10000 --- Training Loss:0.529896\n",
      "Epoch: 166/200, Iteration: 8276/10000 --- Training Loss:0.385095\n",
      "Epoch: 166/200, Iteration: 8278/10000 --- Training Loss:0.500867\n",
      "Epoch: 166/200, Iteration: 8280/10000 --- Training Loss:0.319734\n",
      "Epoch: 166/200, Iteration: 8282/10000 --- Training Loss:0.313593\n",
      "Epoch: 166/200, Iteration: 8284/10000 --- Training Loss:0.559029\n",
      "Epoch: 166/200, Iteration: 8286/10000 --- Training Loss:0.743899\n",
      "Epoch: 166/200, Iteration: 8288/10000 --- Training Loss:0.232640\n",
      "Epoch: 166/200, Iteration: 8290/10000 --- Training Loss:0.276553\n",
      "Epoch: 166/200, Iteration: 8292/10000 --- Training Loss:0.279205\n",
      "Epoch: 166/200, Iteration: 8294/10000 --- Training Loss:0.299379\n",
      "Epoch: 166/200, Iteration: 8296/10000 --- Training Loss:0.482884\n",
      "Epoch: 166/200, Iteration: 8298/10000 --- Training Loss:0.361190\n",
      "Epoch: 166/200, Iteration: 8300/10000 --- Training Loss:0.220978\n",
      "Epoch: 166 finished ! Train Loss: 0.48648, Test Loss: 1.73951\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 167/200, Iteration: 8302/10000 --- Training Loss:0.535188\n",
      "Epoch: 167/200, Iteration: 8304/10000 --- Training Loss:0.095878\n",
      "Epoch: 167/200, Iteration: 8306/10000 --- Training Loss:0.522581\n",
      "Epoch: 167/200, Iteration: 8308/10000 --- Training Loss:0.337986\n",
      "Epoch: 167/200, Iteration: 8310/10000 --- Training Loss:0.586977\n",
      "Epoch: 167/200, Iteration: 8312/10000 --- Training Loss:0.510713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 167/200, Iteration: 8314/10000 --- Training Loss:0.598354\n",
      "Epoch: 167/200, Iteration: 8316/10000 --- Training Loss:0.922688\n",
      "Epoch: 167/200, Iteration: 8318/10000 --- Training Loss:0.672595\n",
      "Epoch: 167/200, Iteration: 8320/10000 --- Training Loss:1.063789\n",
      "Epoch: 167/200, Iteration: 8322/10000 --- Training Loss:0.216974\n",
      "Epoch: 167/200, Iteration: 8324/10000 --- Training Loss:1.210980\n",
      "Epoch: 167/200, Iteration: 8326/10000 --- Training Loss:0.514643\n",
      "Epoch: 167/200, Iteration: 8328/10000 --- Training Loss:1.321717\n",
      "Epoch: 167/200, Iteration: 8330/10000 --- Training Loss:1.107696\n",
      "Epoch: 167/200, Iteration: 8332/10000 --- Training Loss:0.275694\n",
      "Epoch: 167/200, Iteration: 8334/10000 --- Training Loss:0.259106\n",
      "Epoch: 167/200, Iteration: 8336/10000 --- Training Loss:0.506181\n",
      "Epoch: 167/200, Iteration: 8338/10000 --- Training Loss:0.197063\n",
      "Epoch: 167/200, Iteration: 8340/10000 --- Training Loss:0.287255\n",
      "Epoch: 167/200, Iteration: 8342/10000 --- Training Loss:0.323536\n",
      "Epoch: 167/200, Iteration: 8344/10000 --- Training Loss:0.504836\n",
      "Epoch: 167/200, Iteration: 8346/10000 --- Training Loss:0.382409\n",
      "Epoch: 167/200, Iteration: 8348/10000 --- Training Loss:0.424861\n",
      "Epoch: 167/200, Iteration: 8350/10000 --- Training Loss:0.528168\n",
      "Epoch: 167 finished ! Train Loss: 0.60490, Test Loss: 2.03094\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 168/200, Iteration: 8352/10000 --- Training Loss:0.261995\n",
      "Epoch: 168/200, Iteration: 8354/10000 --- Training Loss:0.328732\n",
      "Epoch: 168/200, Iteration: 8356/10000 --- Training Loss:0.361210\n",
      "Epoch: 168/200, Iteration: 8358/10000 --- Training Loss:0.188543\n",
      "Epoch: 168/200, Iteration: 8360/10000 --- Training Loss:0.255945\n",
      "Epoch: 168/200, Iteration: 8362/10000 --- Training Loss:0.321208\n",
      "Epoch: 168/200, Iteration: 8364/10000 --- Training Loss:0.821247\n",
      "Epoch: 168/200, Iteration: 8366/10000 --- Training Loss:0.460161\n",
      "Epoch: 168/200, Iteration: 8368/10000 --- Training Loss:0.561126\n",
      "Epoch: 168/200, Iteration: 8370/10000 --- Training Loss:0.456524\n",
      "Epoch: 168/200, Iteration: 8372/10000 --- Training Loss:0.522832\n",
      "Epoch: 168/200, Iteration: 8374/10000 --- Training Loss:0.795142\n",
      "Epoch: 168/200, Iteration: 8376/10000 --- Training Loss:0.194734\n",
      "Epoch: 168/200, Iteration: 8378/10000 --- Training Loss:0.402084\n",
      "Epoch: 168/200, Iteration: 8380/10000 --- Training Loss:0.464710\n",
      "Epoch: 168/200, Iteration: 8382/10000 --- Training Loss:1.016047\n",
      "Epoch: 168/200, Iteration: 8384/10000 --- Training Loss:0.377431\n",
      "Epoch: 168/200, Iteration: 8386/10000 --- Training Loss:0.852219\n",
      "Epoch: 168/200, Iteration: 8388/10000 --- Training Loss:0.122256\n",
      "Epoch: 168/200, Iteration: 8390/10000 --- Training Loss:0.041062\n",
      "Epoch: 168/200, Iteration: 8392/10000 --- Training Loss:0.473630\n",
      "Epoch: 168/200, Iteration: 8394/10000 --- Training Loss:0.227425\n",
      "Epoch: 168/200, Iteration: 8396/10000 --- Training Loss:0.445991\n",
      "Epoch: 168/200, Iteration: 8398/10000 --- Training Loss:1.203608\n",
      "Epoch: 168/200, Iteration: 8400/10000 --- Training Loss:0.978900\n",
      "Epoch: 168 finished ! Train Loss: 0.45323, Test Loss: 1.83106\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 169/200, Iteration: 8402/10000 --- Training Loss:0.138451\n",
      "Epoch: 169/200, Iteration: 8404/10000 --- Training Loss:0.248708\n",
      "Epoch: 169/200, Iteration: 8406/10000 --- Training Loss:0.344882\n",
      "Epoch: 169/200, Iteration: 8408/10000 --- Training Loss:0.196317\n",
      "Epoch: 169/200, Iteration: 8410/10000 --- Training Loss:0.332443\n",
      "Epoch: 169/200, Iteration: 8412/10000 --- Training Loss:0.276195\n",
      "Epoch: 169/200, Iteration: 8414/10000 --- Training Loss:0.526860\n",
      "Epoch: 169/200, Iteration: 8416/10000 --- Training Loss:0.460542\n",
      "Epoch: 169/200, Iteration: 8418/10000 --- Training Loss:0.211679\n",
      "Epoch: 169/200, Iteration: 8420/10000 --- Training Loss:0.315675\n",
      "Epoch: 169/200, Iteration: 8422/10000 --- Training Loss:0.303717\n",
      "Epoch: 169/200, Iteration: 8424/10000 --- Training Loss:0.168408\n",
      "Epoch: 169/200, Iteration: 8426/10000 --- Training Loss:0.815768\n",
      "Epoch: 169/200, Iteration: 8428/10000 --- Training Loss:0.877975\n",
      "Epoch: 169/200, Iteration: 8430/10000 --- Training Loss:0.581487\n",
      "Epoch: 169/200, Iteration: 8432/10000 --- Training Loss:0.279026\n",
      "Epoch: 169/200, Iteration: 8434/10000 --- Training Loss:0.213519\n",
      "Epoch: 169/200, Iteration: 8436/10000 --- Training Loss:0.300314\n",
      "Epoch: 169/200, Iteration: 8438/10000 --- Training Loss:0.343200\n",
      "Epoch: 169/200, Iteration: 8440/10000 --- Training Loss:0.374442\n",
      "Epoch: 169/200, Iteration: 8442/10000 --- Training Loss:0.706043\n",
      "Epoch: 169/200, Iteration: 8444/10000 --- Training Loss:0.538546\n",
      "Epoch: 169/200, Iteration: 8446/10000 --- Training Loss:0.441313\n",
      "Epoch: 169/200, Iteration: 8448/10000 --- Training Loss:0.564582\n",
      "Epoch: 169/200, Iteration: 8450/10000 --- Training Loss:0.141232\n",
      "Epoch: 169 finished ! Train Loss: 0.46676, Test Loss: 2.36726\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 170/200, Iteration: 8452/10000 --- Training Loss:0.441973\n",
      "Epoch: 170/200, Iteration: 8454/10000 --- Training Loss:0.624400\n",
      "Epoch: 170/200, Iteration: 8456/10000 --- Training Loss:1.017594\n",
      "Epoch: 170/200, Iteration: 8458/10000 --- Training Loss:1.286955\n",
      "Epoch: 170/200, Iteration: 8460/10000 --- Training Loss:0.693921\n",
      "Epoch: 170/200, Iteration: 8462/10000 --- Training Loss:1.108249\n",
      "Epoch: 170/200, Iteration: 8464/10000 --- Training Loss:0.454729\n",
      "Epoch: 170/200, Iteration: 8466/10000 --- Training Loss:1.040773\n",
      "Epoch: 170/200, Iteration: 8468/10000 --- Training Loss:1.208886\n",
      "Epoch: 170/200, Iteration: 8470/10000 --- Training Loss:0.449313\n",
      "Epoch: 170/200, Iteration: 8472/10000 --- Training Loss:0.345705\n",
      "Epoch: 170/200, Iteration: 8474/10000 --- Training Loss:0.693886\n",
      "Epoch: 170/200, Iteration: 8476/10000 --- Training Loss:0.402400\n",
      "Epoch: 170/200, Iteration: 8478/10000 --- Training Loss:0.290830\n",
      "Epoch: 170/200, Iteration: 8480/10000 --- Training Loss:0.222678\n",
      "Epoch: 170/200, Iteration: 8482/10000 --- Training Loss:0.335143\n",
      "Epoch: 170/200, Iteration: 8484/10000 --- Training Loss:0.390141\n",
      "Epoch: 170/200, Iteration: 8486/10000 --- Training Loss:0.636153\n",
      "Epoch: 170/200, Iteration: 8488/10000 --- Training Loss:0.849003\n",
      "Epoch: 170/200, Iteration: 8490/10000 --- Training Loss:0.455016\n",
      "Epoch: 170/200, Iteration: 8492/10000 --- Training Loss:0.155386\n",
      "Epoch: 170/200, Iteration: 8494/10000 --- Training Loss:1.032839\n",
      "Epoch: 170/200, Iteration: 8496/10000 --- Training Loss:0.255495\n",
      "Epoch: 170/200, Iteration: 8498/10000 --- Training Loss:0.304410\n",
      "Epoch: 170/200, Iteration: 8500/10000 --- Training Loss:0.691028\n",
      "Epoch: 170 finished ! Train Loss: 0.60883, Test Loss: 1.49803\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 171/200, Iteration: 8502/10000 --- Training Loss:0.387408\n",
      "Epoch: 171/200, Iteration: 8504/10000 --- Training Loss:0.383490\n",
      "Epoch: 171/200, Iteration: 8506/10000 --- Training Loss:0.400442\n",
      "Epoch: 171/200, Iteration: 8508/10000 --- Training Loss:0.340895\n",
      "Epoch: 171/200, Iteration: 8510/10000 --- Training Loss:0.362450\n",
      "Epoch: 171/200, Iteration: 8512/10000 --- Training Loss:0.458562\n",
      "Epoch: 171/200, Iteration: 8514/10000 --- Training Loss:0.242903\n",
      "Epoch: 171/200, Iteration: 8516/10000 --- Training Loss:0.435427\n",
      "Epoch: 171/200, Iteration: 8518/10000 --- Training Loss:0.272665\n",
      "Epoch: 171/200, Iteration: 8520/10000 --- Training Loss:0.169292\n",
      "Epoch: 171/200, Iteration: 8522/10000 --- Training Loss:0.639116\n",
      "Epoch: 171/200, Iteration: 8524/10000 --- Training Loss:0.467055\n",
      "Epoch: 171/200, Iteration: 8526/10000 --- Training Loss:1.115567\n",
      "Epoch: 171/200, Iteration: 8528/10000 --- Training Loss:1.160725\n",
      "Epoch: 171/200, Iteration: 8530/10000 --- Training Loss:0.231307\n",
      "Epoch: 171/200, Iteration: 8532/10000 --- Training Loss:0.718314\n",
      "Epoch: 171/200, Iteration: 8534/10000 --- Training Loss:0.451890\n",
      "Epoch: 171/200, Iteration: 8536/10000 --- Training Loss:0.272655\n",
      "Epoch: 171/200, Iteration: 8538/10000 --- Training Loss:0.279510\n",
      "Epoch: 171/200, Iteration: 8540/10000 --- Training Loss:0.609803\n",
      "Epoch: 171/200, Iteration: 8542/10000 --- Training Loss:0.391689\n",
      "Epoch: 171/200, Iteration: 8544/10000 --- Training Loss:0.177642\n",
      "Epoch: 171/200, Iteration: 8546/10000 --- Training Loss:0.295170\n",
      "Epoch: 171/200, Iteration: 8548/10000 --- Training Loss:0.618976\n",
      "Epoch: 171/200, Iteration: 8550/10000 --- Training Loss:0.507750\n",
      "Epoch: 171 finished ! Train Loss: 0.44164, Test Loss: 1.77208\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 172/200, Iteration: 8552/10000 --- Training Loss:0.243055\n",
      "Epoch: 172/200, Iteration: 8554/10000 --- Training Loss:1.027274\n",
      "Epoch: 172/200, Iteration: 8556/10000 --- Training Loss:0.528988\n",
      "Epoch: 172/200, Iteration: 8558/10000 --- Training Loss:0.565381\n",
      "Epoch: 172/200, Iteration: 8560/10000 --- Training Loss:0.487716\n",
      "Epoch: 172/200, Iteration: 8562/10000 --- Training Loss:1.914769\n",
      "Epoch: 172/200, Iteration: 8564/10000 --- Training Loss:1.186358\n",
      "Epoch: 172/200, Iteration: 8566/10000 --- Training Loss:0.448169\n",
      "Epoch: 172/200, Iteration: 8568/10000 --- Training Loss:0.697615\n",
      "Epoch: 172/200, Iteration: 8570/10000 --- Training Loss:0.699692\n",
      "Epoch: 172/200, Iteration: 8572/10000 --- Training Loss:0.837852\n",
      "Epoch: 172/200, Iteration: 8574/10000 --- Training Loss:0.272127\n",
      "Epoch: 172/200, Iteration: 8576/10000 --- Training Loss:0.362666\n",
      "Epoch: 172/200, Iteration: 8578/10000 --- Training Loss:0.216379\n",
      "Epoch: 172/200, Iteration: 8580/10000 --- Training Loss:0.289721\n",
      "Epoch: 172/200, Iteration: 8582/10000 --- Training Loss:0.511349\n",
      "Epoch: 172/200, Iteration: 8584/10000 --- Training Loss:0.368141\n",
      "Epoch: 172/200, Iteration: 8586/10000 --- Training Loss:0.431700\n",
      "Epoch: 172/200, Iteration: 8588/10000 --- Training Loss:0.309936\n",
      "Epoch: 172/200, Iteration: 8590/10000 --- Training Loss:0.573640\n",
      "Epoch: 172/200, Iteration: 8592/10000 --- Training Loss:0.784847\n",
      "Epoch: 172/200, Iteration: 8594/10000 --- Training Loss:0.336796\n",
      "Epoch: 172/200, Iteration: 8596/10000 --- Training Loss:0.626339\n",
      "Epoch: 172/200, Iteration: 8598/10000 --- Training Loss:0.286652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 172/200, Iteration: 8600/10000 --- Training Loss:0.913428\n",
      "Epoch: 172 finished ! Train Loss: 0.58452, Test Loss: 1.46010\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 173/200, Iteration: 8602/10000 --- Training Loss:0.577199\n",
      "Epoch: 173/200, Iteration: 8604/10000 --- Training Loss:0.335191\n",
      "Epoch: 173/200, Iteration: 8606/10000 --- Training Loss:0.313546\n",
      "Epoch: 173/200, Iteration: 8608/10000 --- Training Loss:0.363915\n",
      "Epoch: 173/200, Iteration: 8610/10000 --- Training Loss:0.359890\n",
      "Epoch: 173/200, Iteration: 8612/10000 --- Training Loss:0.503208\n",
      "Epoch: 173/200, Iteration: 8614/10000 --- Training Loss:0.603418\n",
      "Epoch: 173/200, Iteration: 8616/10000 --- Training Loss:0.378586\n",
      "Epoch: 173/200, Iteration: 8618/10000 --- Training Loss:0.446233\n",
      "Epoch: 173/200, Iteration: 8620/10000 --- Training Loss:0.537011\n",
      "Epoch: 173/200, Iteration: 8622/10000 --- Training Loss:0.298865\n",
      "Epoch: 173/200, Iteration: 8624/10000 --- Training Loss:0.612324\n",
      "Epoch: 173/200, Iteration: 8626/10000 --- Training Loss:0.842482\n",
      "Epoch: 173/200, Iteration: 8628/10000 --- Training Loss:0.941009\n",
      "Epoch: 173/200, Iteration: 8630/10000 --- Training Loss:1.045660\n",
      "Epoch: 173/200, Iteration: 8632/10000 --- Training Loss:0.520411\n",
      "Epoch: 173/200, Iteration: 8634/10000 --- Training Loss:0.769150\n",
      "Epoch: 173/200, Iteration: 8636/10000 --- Training Loss:0.418519\n",
      "Epoch: 173/200, Iteration: 8638/10000 --- Training Loss:0.356558\n",
      "Epoch: 173/200, Iteration: 8640/10000 --- Training Loss:0.484902\n",
      "Epoch: 173/200, Iteration: 8642/10000 --- Training Loss:0.494915\n",
      "Epoch: 173/200, Iteration: 8644/10000 --- Training Loss:1.150764\n",
      "Epoch: 173/200, Iteration: 8646/10000 --- Training Loss:0.405906\n",
      "Epoch: 173/200, Iteration: 8648/10000 --- Training Loss:0.183919\n",
      "Epoch: 173/200, Iteration: 8650/10000 --- Training Loss:0.425393\n",
      "Epoch: 173 finished ! Train Loss: 0.47150, Test Loss: 2.07026\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 174/200, Iteration: 8652/10000 --- Training Loss:0.920925\n",
      "Epoch: 174/200, Iteration: 8654/10000 --- Training Loss:0.784585\n",
      "Epoch: 174/200, Iteration: 8656/10000 --- Training Loss:0.238308\n",
      "Epoch: 174/200, Iteration: 8658/10000 --- Training Loss:0.697841\n",
      "Epoch: 174/200, Iteration: 8660/10000 --- Training Loss:0.557874\n",
      "Epoch: 174/200, Iteration: 8662/10000 --- Training Loss:0.640079\n",
      "Epoch: 174/200, Iteration: 8664/10000 --- Training Loss:0.789166\n",
      "Epoch: 174/200, Iteration: 8666/10000 --- Training Loss:0.475343\n",
      "Epoch: 174/200, Iteration: 8668/10000 --- Training Loss:0.231261\n",
      "Epoch: 174/200, Iteration: 8670/10000 --- Training Loss:0.370830\n",
      "Epoch: 174/200, Iteration: 8672/10000 --- Training Loss:0.782788\n",
      "Epoch: 174/200, Iteration: 8674/10000 --- Training Loss:0.556142\n",
      "Epoch: 174/200, Iteration: 8676/10000 --- Training Loss:0.281176\n",
      "Epoch: 174/200, Iteration: 8678/10000 --- Training Loss:0.610972\n",
      "Epoch: 174/200, Iteration: 8680/10000 --- Training Loss:0.393203\n",
      "Epoch: 174/200, Iteration: 8682/10000 --- Training Loss:0.252967\n",
      "Epoch: 174/200, Iteration: 8684/10000 --- Training Loss:0.387980\n",
      "Epoch: 174/200, Iteration: 8686/10000 --- Training Loss:0.231408\n",
      "Epoch: 174/200, Iteration: 8688/10000 --- Training Loss:0.300804\n",
      "Epoch: 174/200, Iteration: 8690/10000 --- Training Loss:0.624282\n",
      "Epoch: 174/200, Iteration: 8692/10000 --- Training Loss:0.221822\n",
      "Epoch: 174/200, Iteration: 8694/10000 --- Training Loss:1.268645\n",
      "Epoch: 174/200, Iteration: 8696/10000 --- Training Loss:0.350693\n",
      "Epoch: 174/200, Iteration: 8698/10000 --- Training Loss:1.028816\n",
      "Epoch: 174/200, Iteration: 8700/10000 --- Training Loss:0.469232\n",
      "Epoch: 174 finished ! Train Loss: 0.53941, Test Loss: 1.75522\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 175/200, Iteration: 8702/10000 --- Training Loss:0.595808\n",
      "Epoch: 175/200, Iteration: 8704/10000 --- Training Loss:0.233171\n",
      "Epoch: 175/200, Iteration: 8706/10000 --- Training Loss:0.456383\n",
      "Epoch: 175/200, Iteration: 8708/10000 --- Training Loss:1.159163\n",
      "Epoch: 175/200, Iteration: 8710/10000 --- Training Loss:0.574197\n",
      "Epoch: 175/200, Iteration: 8712/10000 --- Training Loss:1.039883\n",
      "Epoch: 175/200, Iteration: 8714/10000 --- Training Loss:0.513018\n",
      "Epoch: 175/200, Iteration: 8716/10000 --- Training Loss:0.204222\n",
      "Epoch: 175/200, Iteration: 8718/10000 --- Training Loss:0.449395\n",
      "Epoch: 175/200, Iteration: 8720/10000 --- Training Loss:0.253187\n",
      "Epoch: 175/200, Iteration: 8722/10000 --- Training Loss:0.177378\n",
      "Epoch: 175/200, Iteration: 8724/10000 --- Training Loss:0.513694\n",
      "Epoch: 175/200, Iteration: 8726/10000 --- Training Loss:0.570334\n",
      "Epoch: 175/200, Iteration: 8728/10000 --- Training Loss:1.106666\n",
      "Epoch: 175/200, Iteration: 8730/10000 --- Training Loss:0.318781\n",
      "Epoch: 175/200, Iteration: 8732/10000 --- Training Loss:0.251858\n",
      "Epoch: 175/200, Iteration: 8734/10000 --- Training Loss:0.384443\n",
      "Epoch: 175/200, Iteration: 8736/10000 --- Training Loss:0.377472\n",
      "Epoch: 175/200, Iteration: 8738/10000 --- Training Loss:0.300924\n",
      "Epoch: 175/200, Iteration: 8740/10000 --- Training Loss:0.957453\n",
      "Epoch: 175/200, Iteration: 8742/10000 --- Training Loss:0.492379\n",
      "Epoch: 175/200, Iteration: 8744/10000 --- Training Loss:0.330005\n",
      "Epoch: 175/200, Iteration: 8746/10000 --- Training Loss:0.260701\n",
      "Epoch: 175/200, Iteration: 8748/10000 --- Training Loss:0.244262\n",
      "Epoch: 175/200, Iteration: 8750/10000 --- Training Loss:0.297873\n",
      "Epoch: 175 finished ! Train Loss: 0.48879, Test Loss: 2.03126\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 176/200, Iteration: 8752/10000 --- Training Loss:0.312892\n",
      "Epoch: 176/200, Iteration: 8754/10000 --- Training Loss:0.306068\n",
      "Epoch: 176/200, Iteration: 8756/10000 --- Training Loss:0.294203\n",
      "Epoch: 176/200, Iteration: 8758/10000 --- Training Loss:0.478597\n",
      "Epoch: 176/200, Iteration: 8760/10000 --- Training Loss:0.243043\n",
      "Epoch: 176/200, Iteration: 8762/10000 --- Training Loss:0.181772\n",
      "Epoch: 176/200, Iteration: 8764/10000 --- Training Loss:0.581790\n",
      "Epoch: 176/200, Iteration: 8766/10000 --- Training Loss:0.141886\n",
      "Epoch: 176/200, Iteration: 8768/10000 --- Training Loss:0.316372\n",
      "Epoch: 176/200, Iteration: 8770/10000 --- Training Loss:0.326156\n",
      "Epoch: 176/200, Iteration: 8772/10000 --- Training Loss:0.854665\n",
      "Epoch: 176/200, Iteration: 8774/10000 --- Training Loss:0.344646\n",
      "Epoch: 176/200, Iteration: 8776/10000 --- Training Loss:0.597603\n",
      "Epoch: 176/200, Iteration: 8778/10000 --- Training Loss:0.164389\n",
      "Epoch: 176/200, Iteration: 8780/10000 --- Training Loss:0.258667\n",
      "Epoch: 176/200, Iteration: 8782/10000 --- Training Loss:0.504040\n",
      "Epoch: 176/200, Iteration: 8784/10000 --- Training Loss:0.242623\n",
      "Epoch: 176/200, Iteration: 8786/10000 --- Training Loss:1.314634\n",
      "Epoch: 176/200, Iteration: 8788/10000 --- Training Loss:1.193090\n",
      "Epoch: 176/200, Iteration: 8790/10000 --- Training Loss:0.437417\n",
      "Epoch: 176/200, Iteration: 8792/10000 --- Training Loss:0.583983\n",
      "Epoch: 176/200, Iteration: 8794/10000 --- Training Loss:0.319955\n",
      "Epoch: 176/200, Iteration: 8796/10000 --- Training Loss:0.377562\n",
      "Epoch: 176/200, Iteration: 8798/10000 --- Training Loss:0.333246\n",
      "Epoch: 176/200, Iteration: 8800/10000 --- Training Loss:0.445553\n",
      "Epoch: 176 finished ! Train Loss: 0.44303, Test Loss: 2.37534\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 177/200, Iteration: 8802/10000 --- Training Loss:0.467701\n",
      "Epoch: 177/200, Iteration: 8804/10000 --- Training Loss:0.392115\n",
      "Epoch: 177/200, Iteration: 8806/10000 --- Training Loss:0.561326\n",
      "Epoch: 177/200, Iteration: 8808/10000 --- Training Loss:0.216023\n",
      "Epoch: 177/200, Iteration: 8810/10000 --- Training Loss:0.409077\n",
      "Epoch: 177/200, Iteration: 8812/10000 --- Training Loss:0.485729\n",
      "Epoch: 177/200, Iteration: 8814/10000 --- Training Loss:0.287839\n",
      "Epoch: 177/200, Iteration: 8816/10000 --- Training Loss:0.202333\n",
      "Epoch: 177/200, Iteration: 8818/10000 --- Training Loss:0.274519\n",
      "Epoch: 177/200, Iteration: 8820/10000 --- Training Loss:0.503163\n",
      "Epoch: 177/200, Iteration: 8822/10000 --- Training Loss:0.220156\n",
      "Epoch: 177/200, Iteration: 8824/10000 --- Training Loss:1.282451\n",
      "Epoch: 177/200, Iteration: 8826/10000 --- Training Loss:0.136388\n",
      "Epoch: 177/200, Iteration: 8828/10000 --- Training Loss:0.837551\n",
      "Epoch: 177/200, Iteration: 8830/10000 --- Training Loss:0.530019\n",
      "Epoch: 177/200, Iteration: 8832/10000 --- Training Loss:0.332888\n",
      "Epoch: 177/200, Iteration: 8834/10000 --- Training Loss:0.348843\n",
      "Epoch: 177/200, Iteration: 8836/10000 --- Training Loss:0.355352\n",
      "Epoch: 177/200, Iteration: 8838/10000 --- Training Loss:0.242796\n",
      "Epoch: 177/200, Iteration: 8840/10000 --- Training Loss:0.492650\n",
      "Epoch: 177/200, Iteration: 8842/10000 --- Training Loss:0.343802\n",
      "Epoch: 177/200, Iteration: 8844/10000 --- Training Loss:1.060822\n",
      "Epoch: 177/200, Iteration: 8846/10000 --- Training Loss:0.351210\n",
      "Epoch: 177/200, Iteration: 8848/10000 --- Training Loss:1.328346\n",
      "Epoch: 177/200, Iteration: 8850/10000 --- Training Loss:0.381610\n",
      "Epoch: 177 finished ! Train Loss: 0.50124, Test Loss: 1.89669\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 178/200, Iteration: 8852/10000 --- Training Loss:0.553511\n",
      "Epoch: 178/200, Iteration: 8854/10000 --- Training Loss:0.298117\n",
      "Epoch: 178/200, Iteration: 8856/10000 --- Training Loss:0.656989\n",
      "Epoch: 178/200, Iteration: 8858/10000 --- Training Loss:0.436225\n",
      "Epoch: 178/200, Iteration: 8860/10000 --- Training Loss:0.522243\n",
      "Epoch: 178/200, Iteration: 8862/10000 --- Training Loss:0.989652\n",
      "Epoch: 178/200, Iteration: 8864/10000 --- Training Loss:0.514404\n",
      "Epoch: 178/200, Iteration: 8866/10000 --- Training Loss:0.998278\n",
      "Epoch: 178/200, Iteration: 8868/10000 --- Training Loss:0.724453\n",
      "Epoch: 178/200, Iteration: 8870/10000 --- Training Loss:0.296980\n",
      "Epoch: 178/200, Iteration: 8872/10000 --- Training Loss:0.612176\n",
      "Epoch: 178/200, Iteration: 8874/10000 --- Training Loss:0.439694\n",
      "Epoch: 178/200, Iteration: 8876/10000 --- Training Loss:0.757856\n",
      "Epoch: 178/200, Iteration: 8878/10000 --- Training Loss:0.752801\n",
      "Epoch: 178/200, Iteration: 8880/10000 --- Training Loss:0.276416\n",
      "Epoch: 178/200, Iteration: 8882/10000 --- Training Loss:0.189809\n",
      "Epoch: 178/200, Iteration: 8884/10000 --- Training Loss:1.067510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 178/200, Iteration: 8886/10000 --- Training Loss:0.422565\n",
      "Epoch: 178/200, Iteration: 8888/10000 --- Training Loss:0.205614\n",
      "Epoch: 178/200, Iteration: 8890/10000 --- Training Loss:1.045702\n",
      "Epoch: 178/200, Iteration: 8892/10000 --- Training Loss:0.838344\n",
      "Epoch: 178/200, Iteration: 8894/10000 --- Training Loss:0.207528\n",
      "Epoch: 178/200, Iteration: 8896/10000 --- Training Loss:0.542191\n",
      "Epoch: 178/200, Iteration: 8898/10000 --- Training Loss:0.148508\n",
      "Epoch: 178/200, Iteration: 8900/10000 --- Training Loss:0.897359\n",
      "Epoch: 178 finished ! Train Loss: 0.50799, Test Loss: 1.76791\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 179/200, Iteration: 8902/10000 --- Training Loss:0.805781\n",
      "Epoch: 179/200, Iteration: 8904/10000 --- Training Loss:0.574495\n",
      "Epoch: 179/200, Iteration: 8906/10000 --- Training Loss:0.872093\n",
      "Epoch: 179/200, Iteration: 8908/10000 --- Training Loss:0.683014\n",
      "Epoch: 179/200, Iteration: 8910/10000 --- Training Loss:2.119310\n",
      "Epoch: 179/200, Iteration: 8912/10000 --- Training Loss:0.916790\n",
      "Epoch: 179/200, Iteration: 8914/10000 --- Training Loss:0.804352\n",
      "Epoch: 179/200, Iteration: 8916/10000 --- Training Loss:0.467008\n",
      "Epoch: 179/200, Iteration: 8918/10000 --- Training Loss:0.286975\n",
      "Epoch: 179/200, Iteration: 8920/10000 --- Training Loss:1.150718\n",
      "Epoch: 179/200, Iteration: 8922/10000 --- Training Loss:0.561462\n",
      "Epoch: 179/200, Iteration: 8924/10000 --- Training Loss:0.422294\n",
      "Epoch: 179/200, Iteration: 8926/10000 --- Training Loss:0.636295\n",
      "Epoch: 179/200, Iteration: 8928/10000 --- Training Loss:0.491197\n",
      "Epoch: 179/200, Iteration: 8930/10000 --- Training Loss:0.322418\n",
      "Epoch: 179/200, Iteration: 8932/10000 --- Training Loss:0.589205\n",
      "Epoch: 179/200, Iteration: 8934/10000 --- Training Loss:0.236625\n",
      "Epoch: 179/200, Iteration: 8936/10000 --- Training Loss:1.223209\n",
      "Epoch: 179/200, Iteration: 8938/10000 --- Training Loss:0.300739\n",
      "Epoch: 179/200, Iteration: 8940/10000 --- Training Loss:0.320436\n",
      "Epoch: 179/200, Iteration: 8942/10000 --- Training Loss:1.180723\n",
      "Epoch: 179/200, Iteration: 8944/10000 --- Training Loss:0.165575\n",
      "Epoch: 179/200, Iteration: 8946/10000 --- Training Loss:0.812698\n",
      "Epoch: 179/200, Iteration: 8948/10000 --- Training Loss:0.206522\n",
      "Epoch: 179/200, Iteration: 8950/10000 --- Training Loss:0.628666\n",
      "Epoch: 179 finished ! Train Loss: 0.73257, Test Loss: 1.83873\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 180/200, Iteration: 8952/10000 --- Training Loss:0.364563\n",
      "Epoch: 180/200, Iteration: 8954/10000 --- Training Loss:0.697020\n",
      "Epoch: 180/200, Iteration: 8956/10000 --- Training Loss:0.407686\n",
      "Epoch: 180/200, Iteration: 8958/10000 --- Training Loss:0.252078\n",
      "Epoch: 180/200, Iteration: 8960/10000 --- Training Loss:1.385915\n",
      "Epoch: 180/200, Iteration: 8962/10000 --- Training Loss:0.434842\n",
      "Epoch: 180/200, Iteration: 8964/10000 --- Training Loss:0.191139\n",
      "Epoch: 180/200, Iteration: 8966/10000 --- Training Loss:1.409293\n",
      "Epoch: 180/200, Iteration: 8968/10000 --- Training Loss:0.270750\n",
      "Epoch: 180/200, Iteration: 8970/10000 --- Training Loss:0.427846\n",
      "Epoch: 180/200, Iteration: 8972/10000 --- Training Loss:0.345456\n",
      "Epoch: 180/200, Iteration: 8974/10000 --- Training Loss:0.404931\n",
      "Epoch: 180/200, Iteration: 8976/10000 --- Training Loss:0.417275\n",
      "Epoch: 180/200, Iteration: 8978/10000 --- Training Loss:0.525831\n",
      "Epoch: 180/200, Iteration: 8980/10000 --- Training Loss:0.255297\n",
      "Epoch: 180/200, Iteration: 8982/10000 --- Training Loss:0.176037\n",
      "Epoch: 180/200, Iteration: 8984/10000 --- Training Loss:1.016288\n",
      "Epoch: 180/200, Iteration: 8986/10000 --- Training Loss:0.979049\n",
      "Epoch: 180/200, Iteration: 8988/10000 --- Training Loss:0.622769\n",
      "Epoch: 180/200, Iteration: 8990/10000 --- Training Loss:1.171803\n",
      "Epoch: 180/200, Iteration: 8992/10000 --- Training Loss:0.418208\n",
      "Epoch: 180/200, Iteration: 8994/10000 --- Training Loss:0.475120\n",
      "Epoch: 180/200, Iteration: 8996/10000 --- Training Loss:0.291950\n",
      "Epoch: 180/200, Iteration: 8998/10000 --- Training Loss:0.258826\n",
      "Epoch: 180/200, Iteration: 9000/10000 --- Training Loss:0.600804\n",
      "Epoch: 180 finished ! Train Loss: 0.54232, Test Loss: 2.14345\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 181/200, Iteration: 9002/10000 --- Training Loss:1.353149\n",
      "Epoch: 181/200, Iteration: 9004/10000 --- Training Loss:0.267663\n",
      "Epoch: 181/200, Iteration: 9006/10000 --- Training Loss:0.394394\n",
      "Epoch: 181/200, Iteration: 9008/10000 --- Training Loss:0.289366\n",
      "Epoch: 181/200, Iteration: 9010/10000 --- Training Loss:0.271808\n",
      "Epoch: 181/200, Iteration: 9012/10000 --- Training Loss:0.295021\n",
      "Epoch: 181/200, Iteration: 9014/10000 --- Training Loss:0.668441\n",
      "Epoch: 181/200, Iteration: 9016/10000 --- Training Loss:0.749991\n",
      "Epoch: 181/200, Iteration: 9018/10000 --- Training Loss:0.907446\n",
      "Epoch: 181/200, Iteration: 9020/10000 --- Training Loss:0.602530\n",
      "Epoch: 181/200, Iteration: 9022/10000 --- Training Loss:0.856331\n",
      "Epoch: 181/200, Iteration: 9024/10000 --- Training Loss:0.540286\n",
      "Epoch: 181/200, Iteration: 9026/10000 --- Training Loss:0.242740\n",
      "Epoch: 181/200, Iteration: 9028/10000 --- Training Loss:0.764855\n",
      "Epoch: 181/200, Iteration: 9030/10000 --- Training Loss:0.288103\n",
      "Epoch: 181/200, Iteration: 9032/10000 --- Training Loss:0.378333\n",
      "Epoch: 181/200, Iteration: 9034/10000 --- Training Loss:0.099854\n",
      "Epoch: 181/200, Iteration: 9036/10000 --- Training Loss:0.286420\n",
      "Epoch: 181/200, Iteration: 9038/10000 --- Training Loss:0.366677\n",
      "Epoch: 181/200, Iteration: 9040/10000 --- Training Loss:0.478540\n",
      "Epoch: 181/200, Iteration: 9042/10000 --- Training Loss:0.495422\n",
      "Epoch: 181/200, Iteration: 9044/10000 --- Training Loss:0.751207\n",
      "Epoch: 181/200, Iteration: 9046/10000 --- Training Loss:0.648528\n",
      "Epoch: 181/200, Iteration: 9048/10000 --- Training Loss:0.155267\n",
      "Epoch: 181/200, Iteration: 9050/10000 --- Training Loss:0.715129\n",
      "Epoch: 181 finished ! Train Loss: 0.49953, Test Loss: 2.15620\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 182/200, Iteration: 9052/10000 --- Training Loss:0.252298\n",
      "Epoch: 182/200, Iteration: 9054/10000 --- Training Loss:0.357855\n",
      "Epoch: 182/200, Iteration: 9056/10000 --- Training Loss:0.434003\n",
      "Epoch: 182/200, Iteration: 9058/10000 --- Training Loss:0.688357\n",
      "Epoch: 182/200, Iteration: 9060/10000 --- Training Loss:0.291822\n",
      "Epoch: 182/200, Iteration: 9062/10000 --- Training Loss:0.414762\n",
      "Epoch: 182/200, Iteration: 9064/10000 --- Training Loss:0.200881\n",
      "Epoch: 182/200, Iteration: 9066/10000 --- Training Loss:0.607530\n",
      "Epoch: 182/200, Iteration: 9068/10000 --- Training Loss:0.337175\n",
      "Epoch: 182/200, Iteration: 9070/10000 --- Training Loss:0.517784\n",
      "Epoch: 182/200, Iteration: 9072/10000 --- Training Loss:0.262249\n",
      "Epoch: 182/200, Iteration: 9074/10000 --- Training Loss:0.541906\n",
      "Epoch: 182/200, Iteration: 9076/10000 --- Training Loss:0.494313\n",
      "Epoch: 182/200, Iteration: 9078/10000 --- Training Loss:0.273806\n",
      "Epoch: 182/200, Iteration: 9080/10000 --- Training Loss:0.274075\n",
      "Epoch: 182/200, Iteration: 9082/10000 --- Training Loss:0.402432\n",
      "Epoch: 182/200, Iteration: 9084/10000 --- Training Loss:0.240850\n",
      "Epoch: 182/200, Iteration: 9086/10000 --- Training Loss:0.234372\n",
      "Epoch: 182/200, Iteration: 9088/10000 --- Training Loss:0.883044\n",
      "Epoch: 182/200, Iteration: 9090/10000 --- Training Loss:0.239467\n",
      "Epoch: 182/200, Iteration: 9092/10000 --- Training Loss:0.367129\n",
      "Epoch: 182/200, Iteration: 9094/10000 --- Training Loss:0.404727\n",
      "Epoch: 182/200, Iteration: 9096/10000 --- Training Loss:1.991889\n",
      "Epoch: 182/200, Iteration: 9098/10000 --- Training Loss:1.207246\n",
      "Epoch: 182/200, Iteration: 9100/10000 --- Training Loss:0.343510\n",
      "Epoch: 182 finished ! Train Loss: 0.49499, Test Loss: 1.90908\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 183/200, Iteration: 9102/10000 --- Training Loss:0.567866\n",
      "Epoch: 183/200, Iteration: 9104/10000 --- Training Loss:0.278578\n",
      "Epoch: 183/200, Iteration: 9106/10000 --- Training Loss:0.408873\n",
      "Epoch: 183/200, Iteration: 9108/10000 --- Training Loss:0.251700\n",
      "Epoch: 183/200, Iteration: 9110/10000 --- Training Loss:0.533868\n",
      "Epoch: 183/200, Iteration: 9112/10000 --- Training Loss:1.172100\n",
      "Epoch: 183/200, Iteration: 9114/10000 --- Training Loss:0.550613\n",
      "Epoch: 183/200, Iteration: 9116/10000 --- Training Loss:0.359755\n",
      "Epoch: 183/200, Iteration: 9118/10000 --- Training Loss:0.445667\n",
      "Epoch: 183/200, Iteration: 9120/10000 --- Training Loss:0.828559\n",
      "Epoch: 183/200, Iteration: 9122/10000 --- Training Loss:0.283870\n",
      "Epoch: 183/200, Iteration: 9124/10000 --- Training Loss:0.201872\n",
      "Epoch: 183/200, Iteration: 9126/10000 --- Training Loss:0.253277\n",
      "Epoch: 183/200, Iteration: 9128/10000 --- Training Loss:0.074039\n",
      "Epoch: 183/200, Iteration: 9130/10000 --- Training Loss:0.327017\n",
      "Epoch: 183/200, Iteration: 9132/10000 --- Training Loss:0.377640\n",
      "Epoch: 183/200, Iteration: 9134/10000 --- Training Loss:0.486701\n",
      "Epoch: 183/200, Iteration: 9136/10000 --- Training Loss:0.279464\n",
      "Epoch: 183/200, Iteration: 9138/10000 --- Training Loss:0.418913\n",
      "Epoch: 183/200, Iteration: 9140/10000 --- Training Loss:0.237672\n",
      "Epoch: 183/200, Iteration: 9142/10000 --- Training Loss:1.029549\n",
      "Epoch: 183/200, Iteration: 9144/10000 --- Training Loss:0.586459\n",
      "Epoch: 183/200, Iteration: 9146/10000 --- Training Loss:0.200182\n",
      "Epoch: 183/200, Iteration: 9148/10000 --- Training Loss:0.474506\n",
      "Epoch: 183/200, Iteration: 9150/10000 --- Training Loss:0.357185\n",
      "Epoch: 183 finished ! Train Loss: 0.44612, Test Loss: 2.31333\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 184/200, Iteration: 9152/10000 --- Training Loss:0.341779\n",
      "Epoch: 184/200, Iteration: 9154/10000 --- Training Loss:0.275297\n",
      "Epoch: 184/200, Iteration: 9156/10000 --- Training Loss:0.660577\n",
      "Epoch: 184/200, Iteration: 9158/10000 --- Training Loss:0.414699\n",
      "Epoch: 184/200, Iteration: 9160/10000 --- Training Loss:0.487986\n",
      "Epoch: 184/200, Iteration: 9162/10000 --- Training Loss:0.290086\n",
      "Epoch: 184/200, Iteration: 9164/10000 --- Training Loss:0.126533\n",
      "Epoch: 184/200, Iteration: 9166/10000 --- Training Loss:0.179476\n",
      "Epoch: 184/200, Iteration: 9168/10000 --- Training Loss:0.690679\n",
      "Epoch: 184/200, Iteration: 9170/10000 --- Training Loss:0.637555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 184/200, Iteration: 9172/10000 --- Training Loss:0.419972\n",
      "Epoch: 184/200, Iteration: 9174/10000 --- Training Loss:0.444439\n",
      "Epoch: 184/200, Iteration: 9176/10000 --- Training Loss:0.365061\n",
      "Epoch: 184/200, Iteration: 9178/10000 --- Training Loss:0.411791\n",
      "Epoch: 184/200, Iteration: 9180/10000 --- Training Loss:0.456135\n",
      "Epoch: 184/200, Iteration: 9182/10000 --- Training Loss:0.786725\n",
      "Epoch: 184/200, Iteration: 9184/10000 --- Training Loss:0.916427\n",
      "Epoch: 184/200, Iteration: 9186/10000 --- Training Loss:0.214031\n",
      "Epoch: 184/200, Iteration: 9188/10000 --- Training Loss:0.377959\n",
      "Epoch: 184/200, Iteration: 9190/10000 --- Training Loss:0.583431\n",
      "Epoch: 184/200, Iteration: 9192/10000 --- Training Loss:0.851246\n",
      "Epoch: 184/200, Iteration: 9194/10000 --- Training Loss:0.508140\n",
      "Epoch: 184/200, Iteration: 9196/10000 --- Training Loss:1.439586\n",
      "Epoch: 184/200, Iteration: 9198/10000 --- Training Loss:1.169764\n",
      "Epoch: 184/200, Iteration: 9200/10000 --- Training Loss:0.891981\n",
      "Epoch: 184 finished ! Train Loss: 0.56384, Test Loss: 1.79673\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 185/200, Iteration: 9202/10000 --- Training Loss:0.807318\n",
      "Epoch: 185/200, Iteration: 9204/10000 --- Training Loss:0.316468\n",
      "Epoch: 185/200, Iteration: 9206/10000 --- Training Loss:0.534774\n",
      "Epoch: 185/200, Iteration: 9208/10000 --- Training Loss:0.836490\n",
      "Epoch: 185/200, Iteration: 9210/10000 --- Training Loss:0.636070\n",
      "Epoch: 185/200, Iteration: 9212/10000 --- Training Loss:0.163202\n",
      "Epoch: 185/200, Iteration: 9214/10000 --- Training Loss:1.006884\n",
      "Epoch: 185/200, Iteration: 9216/10000 --- Training Loss:0.256034\n",
      "Epoch: 185/200, Iteration: 9218/10000 --- Training Loss:0.912408\n",
      "Epoch: 185/200, Iteration: 9220/10000 --- Training Loss:0.227901\n",
      "Epoch: 185/200, Iteration: 9222/10000 --- Training Loss:1.139762\n",
      "Epoch: 185/200, Iteration: 9224/10000 --- Training Loss:0.143351\n",
      "Epoch: 185/200, Iteration: 9226/10000 --- Training Loss:0.334246\n",
      "Epoch: 185/200, Iteration: 9228/10000 --- Training Loss:0.383562\n",
      "Epoch: 185/200, Iteration: 9230/10000 --- Training Loss:0.967230\n",
      "Epoch: 185/200, Iteration: 9232/10000 --- Training Loss:0.296956\n",
      "Epoch: 185/200, Iteration: 9234/10000 --- Training Loss:0.250388\n",
      "Epoch: 185/200, Iteration: 9236/10000 --- Training Loss:0.273367\n",
      "Epoch: 185/200, Iteration: 9238/10000 --- Training Loss:0.594431\n",
      "Epoch: 185/200, Iteration: 9240/10000 --- Training Loss:0.559539\n",
      "Epoch: 185/200, Iteration: 9242/10000 --- Training Loss:0.255349\n",
      "Epoch: 185/200, Iteration: 9244/10000 --- Training Loss:0.366419\n",
      "Epoch: 185/200, Iteration: 9246/10000 --- Training Loss:0.284211\n",
      "Epoch: 185/200, Iteration: 9248/10000 --- Training Loss:0.395468\n",
      "Epoch: 185/200, Iteration: 9250/10000 --- Training Loss:0.386945\n",
      "Epoch: 185 finished ! Train Loss: 0.56132, Test Loss: 2.04397\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 186/200, Iteration: 9252/10000 --- Training Loss:0.361391\n",
      "Epoch: 186/200, Iteration: 9254/10000 --- Training Loss:0.291401\n",
      "Epoch: 186/200, Iteration: 9256/10000 --- Training Loss:0.357989\n",
      "Epoch: 186/200, Iteration: 9258/10000 --- Training Loss:0.407893\n",
      "Epoch: 186/200, Iteration: 9260/10000 --- Training Loss:0.581971\n",
      "Epoch: 186/200, Iteration: 9262/10000 --- Training Loss:0.132289\n",
      "Epoch: 186/200, Iteration: 9264/10000 --- Training Loss:0.878962\n",
      "Epoch: 186/200, Iteration: 9266/10000 --- Training Loss:0.810342\n",
      "Epoch: 186/200, Iteration: 9268/10000 --- Training Loss:1.090076\n",
      "Epoch: 186/200, Iteration: 9270/10000 --- Training Loss:1.091063\n",
      "Epoch: 186/200, Iteration: 9272/10000 --- Training Loss:0.282596\n",
      "Epoch: 186/200, Iteration: 9274/10000 --- Training Loss:0.250442\n",
      "Epoch: 186/200, Iteration: 9276/10000 --- Training Loss:0.675531\n",
      "Epoch: 186/200, Iteration: 9278/10000 --- Training Loss:0.149497\n",
      "Epoch: 186/200, Iteration: 9280/10000 --- Training Loss:0.483976\n",
      "Epoch: 186/200, Iteration: 9282/10000 --- Training Loss:0.197994\n",
      "Epoch: 186/200, Iteration: 9284/10000 --- Training Loss:0.146064\n",
      "Epoch: 186/200, Iteration: 9286/10000 --- Training Loss:0.515789\n",
      "Epoch: 186/200, Iteration: 9288/10000 --- Training Loss:0.278612\n",
      "Epoch: 186/200, Iteration: 9290/10000 --- Training Loss:0.360318\n",
      "Epoch: 186/200, Iteration: 9292/10000 --- Training Loss:0.344710\n",
      "Epoch: 186/200, Iteration: 9294/10000 --- Training Loss:0.549323\n",
      "Epoch: 186/200, Iteration: 9296/10000 --- Training Loss:0.938526\n",
      "Epoch: 186/200, Iteration: 9298/10000 --- Training Loss:0.398048\n",
      "Epoch: 186/200, Iteration: 9300/10000 --- Training Loss:0.476440\n",
      "Epoch: 186 finished ! Train Loss: 0.57170, Test Loss: 1.78779\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 187/200, Iteration: 9302/10000 --- Training Loss:0.870845\n",
      "Epoch: 187/200, Iteration: 9304/10000 --- Training Loss:0.809530\n",
      "Epoch: 187/200, Iteration: 9306/10000 --- Training Loss:1.040603\n",
      "Epoch: 187/200, Iteration: 9308/10000 --- Training Loss:1.093084\n",
      "Epoch: 187/200, Iteration: 9310/10000 --- Training Loss:0.113723\n",
      "Epoch: 187/200, Iteration: 9312/10000 --- Training Loss:0.551625\n",
      "Epoch: 187/200, Iteration: 9314/10000 --- Training Loss:0.186748\n",
      "Epoch: 187/200, Iteration: 9316/10000 --- Training Loss:0.490593\n",
      "Epoch: 187/200, Iteration: 9318/10000 --- Training Loss:0.111076\n",
      "Epoch: 187/200, Iteration: 9320/10000 --- Training Loss:0.927712\n",
      "Epoch: 187/200, Iteration: 9322/10000 --- Training Loss:0.182769\n",
      "Epoch: 187/200, Iteration: 9324/10000 --- Training Loss:0.487247\n",
      "Epoch: 187/200, Iteration: 9326/10000 --- Training Loss:0.913786\n",
      "Epoch: 187/200, Iteration: 9328/10000 --- Training Loss:0.431988\n",
      "Epoch: 187/200, Iteration: 9330/10000 --- Training Loss:0.715826\n",
      "Epoch: 187/200, Iteration: 9332/10000 --- Training Loss:0.608868\n",
      "Epoch: 187/200, Iteration: 9334/10000 --- Training Loss:0.565661\n",
      "Epoch: 187/200, Iteration: 9336/10000 --- Training Loss:0.285384\n",
      "Epoch: 187/200, Iteration: 9338/10000 --- Training Loss:0.745784\n",
      "Epoch: 187/200, Iteration: 9340/10000 --- Training Loss:0.412751\n",
      "Epoch: 187/200, Iteration: 9342/10000 --- Training Loss:0.968687\n",
      "Epoch: 187/200, Iteration: 9344/10000 --- Training Loss:0.898002\n",
      "Epoch: 187/200, Iteration: 9346/10000 --- Training Loss:0.984047\n",
      "Epoch: 187/200, Iteration: 9348/10000 --- Training Loss:1.746346\n",
      "Epoch: 187/200, Iteration: 9350/10000 --- Training Loss:0.304302\n",
      "Epoch: 187 finished ! Train Loss: 0.73339, Test Loss: 1.48372\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 188/200, Iteration: 9352/10000 --- Training Loss:2.516958\n",
      "Epoch: 188/200, Iteration: 9354/10000 --- Training Loss:0.382587\n",
      "Epoch: 188/200, Iteration: 9356/10000 --- Training Loss:0.984528\n",
      "Epoch: 188/200, Iteration: 9358/10000 --- Training Loss:0.512477\n",
      "Epoch: 188/200, Iteration: 9360/10000 --- Training Loss:0.598618\n",
      "Epoch: 188/200, Iteration: 9362/10000 --- Training Loss:0.303627\n",
      "Epoch: 188/200, Iteration: 9364/10000 --- Training Loss:0.504019\n",
      "Epoch: 188/200, Iteration: 9366/10000 --- Training Loss:0.658829\n",
      "Epoch: 188/200, Iteration: 9368/10000 --- Training Loss:0.571867\n",
      "Epoch: 188/200, Iteration: 9370/10000 --- Training Loss:0.421244\n",
      "Epoch: 188/200, Iteration: 9372/10000 --- Training Loss:0.419542\n",
      "Epoch: 188/200, Iteration: 9374/10000 --- Training Loss:0.269925\n",
      "Epoch: 188/200, Iteration: 9376/10000 --- Training Loss:0.237634\n",
      "Epoch: 188/200, Iteration: 9378/10000 --- Training Loss:0.364323\n",
      "Epoch: 188/200, Iteration: 9380/10000 --- Training Loss:0.254777\n",
      "Epoch: 188/200, Iteration: 9382/10000 --- Training Loss:0.253670\n",
      "Epoch: 188/200, Iteration: 9384/10000 --- Training Loss:0.092610\n",
      "Epoch: 188/200, Iteration: 9386/10000 --- Training Loss:0.307987\n",
      "Epoch: 188/200, Iteration: 9388/10000 --- Training Loss:0.161848\n",
      "Epoch: 188/200, Iteration: 9390/10000 --- Training Loss:0.281813\n",
      "Epoch: 188/200, Iteration: 9392/10000 --- Training Loss:0.408266\n",
      "Epoch: 188/200, Iteration: 9394/10000 --- Training Loss:0.275962\n",
      "Epoch: 188/200, Iteration: 9396/10000 --- Training Loss:0.287284\n",
      "Epoch: 188/200, Iteration: 9398/10000 --- Training Loss:0.625013\n",
      "Epoch: 188/200, Iteration: 9400/10000 --- Training Loss:0.177664\n",
      "Epoch: 188 finished ! Train Loss: 0.53981, Test Loss: 1.37991\n",
      "Epoch consuming time: 0m 0s\n",
      "Trained model saved: 94 percent completed\n",
      "Epoch: 189/200, Iteration: 9402/10000 --- Training Loss:0.267751\n",
      "Epoch: 189/200, Iteration: 9404/10000 --- Training Loss:0.378009\n",
      "Epoch: 189/200, Iteration: 9406/10000 --- Training Loss:0.467273\n",
      "Epoch: 189/200, Iteration: 9408/10000 --- Training Loss:0.344733\n",
      "Epoch: 189/200, Iteration: 9410/10000 --- Training Loss:0.181743\n",
      "Epoch: 189/200, Iteration: 9412/10000 --- Training Loss:0.259573\n",
      "Epoch: 189/200, Iteration: 9414/10000 --- Training Loss:0.359728\n",
      "Epoch: 189/200, Iteration: 9416/10000 --- Training Loss:0.238859\n",
      "Epoch: 189/200, Iteration: 9418/10000 --- Training Loss:0.288900\n",
      "Epoch: 189/200, Iteration: 9420/10000 --- Training Loss:1.334243\n",
      "Epoch: 189/200, Iteration: 9422/10000 --- Training Loss:0.708127\n",
      "Epoch: 189/200, Iteration: 9424/10000 --- Training Loss:0.306959\n",
      "Epoch: 189/200, Iteration: 9426/10000 --- Training Loss:0.312343\n",
      "Epoch: 189/200, Iteration: 9428/10000 --- Training Loss:0.065580\n",
      "Epoch: 189/200, Iteration: 9430/10000 --- Training Loss:0.628902\n",
      "Epoch: 189/200, Iteration: 9432/10000 --- Training Loss:0.303275\n",
      "Epoch: 189/200, Iteration: 9434/10000 --- Training Loss:0.490008\n",
      "Epoch: 189/200, Iteration: 9436/10000 --- Training Loss:0.905711\n",
      "Epoch: 189/200, Iteration: 9438/10000 --- Training Loss:0.671396\n",
      "Epoch: 189/200, Iteration: 9440/10000 --- Training Loss:0.359009\n",
      "Epoch: 189/200, Iteration: 9442/10000 --- Training Loss:0.663970\n",
      "Epoch: 189/200, Iteration: 9444/10000 --- Training Loss:0.520735\n",
      "Epoch: 189/200, Iteration: 9446/10000 --- Training Loss:0.935002\n",
      "Epoch: 189/200, Iteration: 9448/10000 --- Training Loss:0.493648\n",
      "Epoch: 189/200, Iteration: 9450/10000 --- Training Loss:0.740079\n",
      "Epoch: 189 finished ! Train Loss: 0.53340, Test Loss: 2.22259\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 190/200, Iteration: 9452/10000 --- Training Loss:0.277939\n",
      "Epoch: 190/200, Iteration: 9454/10000 --- Training Loss:0.999074\n",
      "Epoch: 190/200, Iteration: 9456/10000 --- Training Loss:0.929107\n",
      "Epoch: 190/200, Iteration: 9458/10000 --- Training Loss:1.388551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190/200, Iteration: 9460/10000 --- Training Loss:0.734176\n",
      "Epoch: 190/200, Iteration: 9462/10000 --- Training Loss:0.508850\n",
      "Epoch: 190/200, Iteration: 9464/10000 --- Training Loss:0.333976\n",
      "Epoch: 190/200, Iteration: 9466/10000 --- Training Loss:0.308136\n",
      "Epoch: 190/200, Iteration: 9468/10000 --- Training Loss:0.472934\n",
      "Epoch: 190/200, Iteration: 9470/10000 --- Training Loss:0.332277\n",
      "Epoch: 190/200, Iteration: 9472/10000 --- Training Loss:0.235010\n",
      "Epoch: 190/200, Iteration: 9474/10000 --- Training Loss:0.153347\n",
      "Epoch: 190/200, Iteration: 9476/10000 --- Training Loss:0.871339\n",
      "Epoch: 190/200, Iteration: 9478/10000 --- Training Loss:0.474098\n",
      "Epoch: 190/200, Iteration: 9480/10000 --- Training Loss:0.730334\n",
      "Epoch: 190/200, Iteration: 9482/10000 --- Training Loss:0.658374\n",
      "Epoch: 190/200, Iteration: 9484/10000 --- Training Loss:0.233004\n",
      "Epoch: 190/200, Iteration: 9486/10000 --- Training Loss:0.702071\n",
      "Epoch: 190/200, Iteration: 9488/10000 --- Training Loss:1.219908\n",
      "Epoch: 190/200, Iteration: 9490/10000 --- Training Loss:0.447200\n",
      "Epoch: 190/200, Iteration: 9492/10000 --- Training Loss:0.195357\n",
      "Epoch: 190/200, Iteration: 9494/10000 --- Training Loss:0.242175\n",
      "Epoch: 190/200, Iteration: 9496/10000 --- Training Loss:0.409424\n",
      "Epoch: 190/200, Iteration: 9498/10000 --- Training Loss:0.118736\n",
      "Epoch: 190/200, Iteration: 9500/10000 --- Training Loss:0.145791\n",
      "Epoch: 190 finished ! Train Loss: 0.50884, Test Loss: 1.51748\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 191/200, Iteration: 9502/10000 --- Training Loss:0.649870\n",
      "Epoch: 191/200, Iteration: 9504/10000 --- Training Loss:0.337808\n",
      "Epoch: 191/200, Iteration: 9506/10000 --- Training Loss:0.136229\n",
      "Epoch: 191/200, Iteration: 9508/10000 --- Training Loss:0.636550\n",
      "Epoch: 191/200, Iteration: 9510/10000 --- Training Loss:0.594090\n",
      "Epoch: 191/200, Iteration: 9512/10000 --- Training Loss:0.213222\n",
      "Epoch: 191/200, Iteration: 9514/10000 --- Training Loss:0.939372\n",
      "Epoch: 191/200, Iteration: 9516/10000 --- Training Loss:0.164619\n",
      "Epoch: 191/200, Iteration: 9518/10000 --- Training Loss:0.277172\n",
      "Epoch: 191/200, Iteration: 9520/10000 --- Training Loss:0.609412\n",
      "Epoch: 191/200, Iteration: 9522/10000 --- Training Loss:0.224253\n",
      "Epoch: 191/200, Iteration: 9524/10000 --- Training Loss:0.445041\n",
      "Epoch: 191/200, Iteration: 9526/10000 --- Training Loss:0.229160\n",
      "Epoch: 191/200, Iteration: 9528/10000 --- Training Loss:0.275438\n",
      "Epoch: 191/200, Iteration: 9530/10000 --- Training Loss:0.271178\n",
      "Epoch: 191/200, Iteration: 9532/10000 --- Training Loss:0.520000\n",
      "Epoch: 191/200, Iteration: 9534/10000 --- Training Loss:0.604021\n",
      "Epoch: 191/200, Iteration: 9536/10000 --- Training Loss:0.321756\n",
      "Epoch: 191/200, Iteration: 9538/10000 --- Training Loss:0.734820\n",
      "Epoch: 191/200, Iteration: 9540/10000 --- Training Loss:0.324942\n",
      "Epoch: 191/200, Iteration: 9542/10000 --- Training Loss:0.551575\n",
      "Epoch: 191/200, Iteration: 9544/10000 --- Training Loss:0.781005\n",
      "Epoch: 191/200, Iteration: 9546/10000 --- Training Loss:0.890100\n",
      "Epoch: 191/200, Iteration: 9548/10000 --- Training Loss:0.321269\n",
      "Epoch: 191/200, Iteration: 9550/10000 --- Training Loss:0.497841\n",
      "Epoch: 191 finished ! Train Loss: 0.54908, Test Loss: 2.16666\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 192/200, Iteration: 9552/10000 --- Training Loss:0.470732\n",
      "Epoch: 192/200, Iteration: 9554/10000 --- Training Loss:0.209134\n",
      "Epoch: 192/200, Iteration: 9556/10000 --- Training Loss:0.249917\n",
      "Epoch: 192/200, Iteration: 9558/10000 --- Training Loss:0.410072\n",
      "Epoch: 192/200, Iteration: 9560/10000 --- Training Loss:0.222420\n",
      "Epoch: 192/200, Iteration: 9562/10000 --- Training Loss:0.257312\n",
      "Epoch: 192/200, Iteration: 9564/10000 --- Training Loss:0.519549\n",
      "Epoch: 192/200, Iteration: 9566/10000 --- Training Loss:0.206225\n",
      "Epoch: 192/200, Iteration: 9568/10000 --- Training Loss:0.150130\n",
      "Epoch: 192/200, Iteration: 9570/10000 --- Training Loss:0.969493\n",
      "Epoch: 192/200, Iteration: 9572/10000 --- Training Loss:0.864923\n",
      "Epoch: 192/200, Iteration: 9574/10000 --- Training Loss:0.667150\n",
      "Epoch: 192/200, Iteration: 9576/10000 --- Training Loss:0.776943\n",
      "Epoch: 192/200, Iteration: 9578/10000 --- Training Loss:0.668174\n",
      "Epoch: 192/200, Iteration: 9580/10000 --- Training Loss:0.380637\n",
      "Epoch: 192/200, Iteration: 9582/10000 --- Training Loss:0.231587\n",
      "Epoch: 192/200, Iteration: 9584/10000 --- Training Loss:0.859542\n",
      "Epoch: 192/200, Iteration: 9586/10000 --- Training Loss:0.322278\n",
      "Epoch: 192/200, Iteration: 9588/10000 --- Training Loss:0.513855\n",
      "Epoch: 192/200, Iteration: 9590/10000 --- Training Loss:1.357632\n",
      "Epoch: 192/200, Iteration: 9592/10000 --- Training Loss:0.512877\n",
      "Epoch: 192/200, Iteration: 9594/10000 --- Training Loss:0.755671\n",
      "Epoch: 192/200, Iteration: 9596/10000 --- Training Loss:0.521111\n",
      "Epoch: 192/200, Iteration: 9598/10000 --- Training Loss:0.664836\n",
      "Epoch: 192/200, Iteration: 9600/10000 --- Training Loss:0.510451\n",
      "Epoch: 192 finished ! Train Loss: 0.55864, Test Loss: 2.80464\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 193/200, Iteration: 9602/10000 --- Training Loss:0.919484\n",
      "Epoch: 193/200, Iteration: 9604/10000 --- Training Loss:1.038843\n",
      "Epoch: 193/200, Iteration: 9606/10000 --- Training Loss:0.460308\n",
      "Epoch: 193/200, Iteration: 9608/10000 --- Training Loss:1.528266\n",
      "Epoch: 193/200, Iteration: 9610/10000 --- Training Loss:0.390543\n",
      "Epoch: 193/200, Iteration: 9612/10000 --- Training Loss:0.222217\n",
      "Epoch: 193/200, Iteration: 9614/10000 --- Training Loss:0.260735\n",
      "Epoch: 193/200, Iteration: 9616/10000 --- Training Loss:0.673855\n",
      "Epoch: 193/200, Iteration: 9618/10000 --- Training Loss:0.486762\n",
      "Epoch: 193/200, Iteration: 9620/10000 --- Training Loss:0.329860\n",
      "Epoch: 193/200, Iteration: 9622/10000 --- Training Loss:0.528506\n",
      "Epoch: 193/200, Iteration: 9624/10000 --- Training Loss:0.707972\n",
      "Epoch: 193/200, Iteration: 9626/10000 --- Training Loss:0.528461\n",
      "Epoch: 193/200, Iteration: 9628/10000 --- Training Loss:0.387013\n",
      "Epoch: 193/200, Iteration: 9630/10000 --- Training Loss:0.357068\n",
      "Epoch: 193/200, Iteration: 9632/10000 --- Training Loss:0.839354\n",
      "Epoch: 193/200, Iteration: 9634/10000 --- Training Loss:0.293918\n",
      "Epoch: 193/200, Iteration: 9636/10000 --- Training Loss:0.293067\n",
      "Epoch: 193/200, Iteration: 9638/10000 --- Training Loss:0.207963\n",
      "Epoch: 193/200, Iteration: 9640/10000 --- Training Loss:0.204538\n",
      "Epoch: 193/200, Iteration: 9642/10000 --- Training Loss:0.252986\n",
      "Epoch: 193/200, Iteration: 9644/10000 --- Training Loss:0.269831\n",
      "Epoch: 193/200, Iteration: 9646/10000 --- Training Loss:0.117278\n",
      "Epoch: 193/200, Iteration: 9648/10000 --- Training Loss:0.432338\n",
      "Epoch: 193/200, Iteration: 9650/10000 --- Training Loss:0.377766\n",
      "Epoch: 193 finished ! Train Loss: 0.57425, Test Loss: 1.98308\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 194/200, Iteration: 9652/10000 --- Training Loss:0.270833\n",
      "Epoch: 194/200, Iteration: 9654/10000 --- Training Loss:0.324915\n",
      "Epoch: 194/200, Iteration: 9656/10000 --- Training Loss:0.629335\n",
      "Epoch: 194/200, Iteration: 9658/10000 --- Training Loss:0.487143\n",
      "Epoch: 194/200, Iteration: 9660/10000 --- Training Loss:0.183205\n",
      "Epoch: 194/200, Iteration: 9662/10000 --- Training Loss:0.548993\n",
      "Epoch: 194/200, Iteration: 9664/10000 --- Training Loss:0.884270\n",
      "Epoch: 194/200, Iteration: 9666/10000 --- Training Loss:0.305787\n",
      "Epoch: 194/200, Iteration: 9668/10000 --- Training Loss:0.269422\n",
      "Epoch: 194/200, Iteration: 9670/10000 --- Training Loss:0.278863\n",
      "Epoch: 194/200, Iteration: 9672/10000 --- Training Loss:0.408340\n",
      "Epoch: 194/200, Iteration: 9674/10000 --- Training Loss:1.015863\n",
      "Epoch: 194/200, Iteration: 9676/10000 --- Training Loss:0.474305\n",
      "Epoch: 194/200, Iteration: 9678/10000 --- Training Loss:0.275128\n",
      "Epoch: 194/200, Iteration: 9680/10000 --- Training Loss:0.254946\n",
      "Epoch: 194/200, Iteration: 9682/10000 --- Training Loss:0.382363\n",
      "Epoch: 194/200, Iteration: 9684/10000 --- Training Loss:0.358874\n",
      "Epoch: 194/200, Iteration: 9686/10000 --- Training Loss:0.258897\n",
      "Epoch: 194/200, Iteration: 9688/10000 --- Training Loss:0.217467\n",
      "Epoch: 194/200, Iteration: 9690/10000 --- Training Loss:0.252024\n",
      "Epoch: 194/200, Iteration: 9692/10000 --- Training Loss:0.901747\n",
      "Epoch: 194/200, Iteration: 9694/10000 --- Training Loss:0.263137\n",
      "Epoch: 194/200, Iteration: 9696/10000 --- Training Loss:0.206071\n",
      "Epoch: 194/200, Iteration: 9698/10000 --- Training Loss:0.262558\n",
      "Epoch: 194/200, Iteration: 9700/10000 --- Training Loss:0.459758\n",
      "Epoch: 194 finished ! Train Loss: 0.43797, Test Loss: 1.70858\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 195/200, Iteration: 9702/10000 --- Training Loss:0.549578\n",
      "Epoch: 195/200, Iteration: 9704/10000 --- Training Loss:0.231602\n",
      "Epoch: 195/200, Iteration: 9706/10000 --- Training Loss:0.367760\n",
      "Epoch: 195/200, Iteration: 9708/10000 --- Training Loss:0.205474\n",
      "Epoch: 195/200, Iteration: 9710/10000 --- Training Loss:0.429298\n",
      "Epoch: 195/200, Iteration: 9712/10000 --- Training Loss:0.383116\n",
      "Epoch: 195/200, Iteration: 9714/10000 --- Training Loss:0.593845\n",
      "Epoch: 195/200, Iteration: 9716/10000 --- Training Loss:0.461431\n",
      "Epoch: 195/200, Iteration: 9718/10000 --- Training Loss:0.561687\n",
      "Epoch: 195/200, Iteration: 9720/10000 --- Training Loss:0.430288\n",
      "Epoch: 195/200, Iteration: 9722/10000 --- Training Loss:0.239597\n",
      "Epoch: 195/200, Iteration: 9724/10000 --- Training Loss:0.511401\n",
      "Epoch: 195/200, Iteration: 9726/10000 --- Training Loss:0.445058\n",
      "Epoch: 195/200, Iteration: 9728/10000 --- Training Loss:1.712852\n",
      "Epoch: 195/200, Iteration: 9730/10000 --- Training Loss:0.606534\n",
      "Epoch: 195/200, Iteration: 9732/10000 --- Training Loss:0.859788\n",
      "Epoch: 195/200, Iteration: 9734/10000 --- Training Loss:0.568315\n",
      "Epoch: 195/200, Iteration: 9736/10000 --- Training Loss:0.384927\n",
      "Epoch: 195/200, Iteration: 9738/10000 --- Training Loss:0.897012\n",
      "Epoch: 195/200, Iteration: 9740/10000 --- Training Loss:0.268210\n",
      "Epoch: 195/200, Iteration: 9742/10000 --- Training Loss:0.646596\n",
      "Epoch: 195/200, Iteration: 9744/10000 --- Training Loss:0.957650\n",
      "Epoch: 195/200, Iteration: 9746/10000 --- Training Loss:0.258446\n",
      "Epoch: 195/200, Iteration: 9748/10000 --- Training Loss:0.509097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195/200, Iteration: 9750/10000 --- Training Loss:0.398403\n",
      "Epoch: 195 finished ! Train Loss: 0.55729, Test Loss: 2.28898\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 196/200, Iteration: 9752/10000 --- Training Loss:0.654224\n",
      "Epoch: 196/200, Iteration: 9754/10000 --- Training Loss:0.300763\n",
      "Epoch: 196/200, Iteration: 9756/10000 --- Training Loss:0.487577\n",
      "Epoch: 196/200, Iteration: 9758/10000 --- Training Loss:0.431493\n",
      "Epoch: 196/200, Iteration: 9760/10000 --- Training Loss:0.492688\n",
      "Epoch: 196/200, Iteration: 9762/10000 --- Training Loss:0.121832\n",
      "Epoch: 196/200, Iteration: 9764/10000 --- Training Loss:0.559517\n",
      "Epoch: 196/200, Iteration: 9766/10000 --- Training Loss:0.192561\n",
      "Epoch: 196/200, Iteration: 9768/10000 --- Training Loss:0.414409\n",
      "Epoch: 196/200, Iteration: 9770/10000 --- Training Loss:0.377698\n",
      "Epoch: 196/200, Iteration: 9772/10000 --- Training Loss:0.223236\n",
      "Epoch: 196/200, Iteration: 9774/10000 --- Training Loss:0.230013\n",
      "Epoch: 196/200, Iteration: 9776/10000 --- Training Loss:0.313920\n",
      "Epoch: 196/200, Iteration: 9778/10000 --- Training Loss:0.171564\n",
      "Epoch: 196/200, Iteration: 9780/10000 --- Training Loss:0.900922\n",
      "Epoch: 196/200, Iteration: 9782/10000 --- Training Loss:0.166973\n",
      "Epoch: 196/200, Iteration: 9784/10000 --- Training Loss:0.416203\n",
      "Epoch: 196/200, Iteration: 9786/10000 --- Training Loss:0.485227\n",
      "Epoch: 196/200, Iteration: 9788/10000 --- Training Loss:0.425045\n",
      "Epoch: 196/200, Iteration: 9790/10000 --- Training Loss:0.911810\n",
      "Epoch: 196/200, Iteration: 9792/10000 --- Training Loss:0.208732\n",
      "Epoch: 196/200, Iteration: 9794/10000 --- Training Loss:0.507269\n",
      "Epoch: 196/200, Iteration: 9796/10000 --- Training Loss:0.434904\n",
      "Epoch: 196/200, Iteration: 9798/10000 --- Training Loss:0.363054\n",
      "Epoch: 196/200, Iteration: 9800/10000 --- Training Loss:0.241753\n",
      "Epoch: 196 finished ! Train Loss: 0.45510, Test Loss: 2.06103\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 197/200, Iteration: 9802/10000 --- Training Loss:0.593916\n",
      "Epoch: 197/200, Iteration: 9804/10000 --- Training Loss:1.867673\n",
      "Epoch: 197/200, Iteration: 9806/10000 --- Training Loss:0.747815\n",
      "Epoch: 197/200, Iteration: 9808/10000 --- Training Loss:0.147125\n",
      "Epoch: 197/200, Iteration: 9810/10000 --- Training Loss:0.581656\n",
      "Epoch: 197/200, Iteration: 9812/10000 --- Training Loss:0.326728\n",
      "Epoch: 197/200, Iteration: 9814/10000 --- Training Loss:0.248499\n",
      "Epoch: 197/200, Iteration: 9816/10000 --- Training Loss:0.581048\n",
      "Epoch: 197/200, Iteration: 9818/10000 --- Training Loss:0.130450\n",
      "Epoch: 197/200, Iteration: 9820/10000 --- Training Loss:0.545737\n",
      "Epoch: 197/200, Iteration: 9822/10000 --- Training Loss:0.264871\n",
      "Epoch: 197/200, Iteration: 9824/10000 --- Training Loss:0.143638\n",
      "Epoch: 197/200, Iteration: 9826/10000 --- Training Loss:0.461034\n",
      "Epoch: 197/200, Iteration: 9828/10000 --- Training Loss:1.336864\n",
      "Epoch: 197/200, Iteration: 9830/10000 --- Training Loss:0.806264\n",
      "Epoch: 197/200, Iteration: 9832/10000 --- Training Loss:0.468957\n",
      "Epoch: 197/200, Iteration: 9834/10000 --- Training Loss:0.491665\n",
      "Epoch: 197/200, Iteration: 9836/10000 --- Training Loss:0.515719\n",
      "Epoch: 197/200, Iteration: 9838/10000 --- Training Loss:0.259570\n",
      "Epoch: 197/200, Iteration: 9840/10000 --- Training Loss:0.618987\n",
      "Epoch: 197/200, Iteration: 9842/10000 --- Training Loss:0.326442\n",
      "Epoch: 197/200, Iteration: 9844/10000 --- Training Loss:0.371630\n",
      "Epoch: 197/200, Iteration: 9846/10000 --- Training Loss:0.326517\n",
      "Epoch: 197/200, Iteration: 9848/10000 --- Training Loss:0.194751\n",
      "Epoch: 197/200, Iteration: 9850/10000 --- Training Loss:0.194088\n",
      "Epoch: 197 finished ! Train Loss: 0.48254, Test Loss: 2.69053\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 198/200, Iteration: 9852/10000 --- Training Loss:0.167067\n",
      "Epoch: 198/200, Iteration: 9854/10000 --- Training Loss:0.500354\n",
      "Epoch: 198/200, Iteration: 9856/10000 --- Training Loss:1.450303\n",
      "Epoch: 198/200, Iteration: 9858/10000 --- Training Loss:1.171508\n",
      "Epoch: 198/200, Iteration: 9860/10000 --- Training Loss:0.172535\n",
      "Epoch: 198/200, Iteration: 9862/10000 --- Training Loss:0.234655\n",
      "Epoch: 198/200, Iteration: 9864/10000 --- Training Loss:0.294129\n",
      "Epoch: 198/200, Iteration: 9866/10000 --- Training Loss:0.286103\n",
      "Epoch: 198/200, Iteration: 9868/10000 --- Training Loss:0.558989\n",
      "Epoch: 198/200, Iteration: 9870/10000 --- Training Loss:0.330979\n",
      "Epoch: 198/200, Iteration: 9872/10000 --- Training Loss:0.591612\n",
      "Epoch: 198/200, Iteration: 9874/10000 --- Training Loss:0.270194\n",
      "Epoch: 198/200, Iteration: 9876/10000 --- Training Loss:0.191801\n",
      "Epoch: 198/200, Iteration: 9878/10000 --- Training Loss:0.270192\n",
      "Epoch: 198/200, Iteration: 9880/10000 --- Training Loss:0.815855\n",
      "Epoch: 198/200, Iteration: 9882/10000 --- Training Loss:0.432774\n",
      "Epoch: 198/200, Iteration: 9884/10000 --- Training Loss:0.280576\n",
      "Epoch: 198/200, Iteration: 9886/10000 --- Training Loss:1.149534\n",
      "Epoch: 198/200, Iteration: 9888/10000 --- Training Loss:0.912253\n",
      "Epoch: 198/200, Iteration: 9890/10000 --- Training Loss:1.208053\n",
      "Epoch: 198/200, Iteration: 9892/10000 --- Training Loss:0.100126\n",
      "Epoch: 198/200, Iteration: 9894/10000 --- Training Loss:0.655090\n",
      "Epoch: 198/200, Iteration: 9896/10000 --- Training Loss:0.171464\n",
      "Epoch: 198/200, Iteration: 9898/10000 --- Training Loss:1.496545\n",
      "Epoch: 198/200, Iteration: 9900/10000 --- Training Loss:0.587291\n",
      "Epoch: 198 finished ! Train Loss: 0.59522, Test Loss: 4.41411\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 199/200, Iteration: 9902/10000 --- Training Loss:0.804726\n",
      "Epoch: 199/200, Iteration: 9904/10000 --- Training Loss:0.787375\n",
      "Epoch: 199/200, Iteration: 9906/10000 --- Training Loss:1.032552\n",
      "Epoch: 199/200, Iteration: 9908/10000 --- Training Loss:0.781851\n",
      "Epoch: 199/200, Iteration: 9910/10000 --- Training Loss:0.353874\n",
      "Epoch: 199/200, Iteration: 9912/10000 --- Training Loss:0.860979\n",
      "Epoch: 199/200, Iteration: 9914/10000 --- Training Loss:0.138224\n",
      "Epoch: 199/200, Iteration: 9916/10000 --- Training Loss:0.501005\n",
      "Epoch: 199/200, Iteration: 9918/10000 --- Training Loss:0.234887\n",
      "Epoch: 199/200, Iteration: 9920/10000 --- Training Loss:1.369430\n",
      "Epoch: 199/200, Iteration: 9922/10000 --- Training Loss:0.435423\n",
      "Epoch: 199/200, Iteration: 9924/10000 --- Training Loss:0.488006\n",
      "Epoch: 199/200, Iteration: 9926/10000 --- Training Loss:0.300360\n",
      "Epoch: 199/200, Iteration: 9928/10000 --- Training Loss:0.390540\n",
      "Epoch: 199/200, Iteration: 9930/10000 --- Training Loss:0.197678\n",
      "Epoch: 199/200, Iteration: 9932/10000 --- Training Loss:0.240565\n",
      "Epoch: 199/200, Iteration: 9934/10000 --- Training Loss:0.357684\n",
      "Epoch: 199/200, Iteration: 9936/10000 --- Training Loss:0.203642\n",
      "Epoch: 199/200, Iteration: 9938/10000 --- Training Loss:0.255745\n",
      "Epoch: 199/200, Iteration: 9940/10000 --- Training Loss:0.547207\n",
      "Epoch: 199/200, Iteration: 9942/10000 --- Training Loss:0.502095\n",
      "Epoch: 199/200, Iteration: 9944/10000 --- Training Loss:1.200990\n",
      "Epoch: 199/200, Iteration: 9946/10000 --- Training Loss:0.354357\n",
      "Epoch: 199/200, Iteration: 9948/10000 --- Training Loss:0.375590\n",
      "Epoch: 199/200, Iteration: 9950/10000 --- Training Loss:0.418900\n",
      "Epoch: 199 finished ! Train Loss: 0.63566, Test Loss: 1.82943\n",
      "Epoch consuming time: 0m 0s\n",
      "Epoch: 200/200, Iteration: 9952/10000 --- Training Loss:0.755944\n",
      "Epoch: 200/200, Iteration: 9954/10000 --- Training Loss:0.322211\n",
      "Epoch: 200/200, Iteration: 9956/10000 --- Training Loss:0.846882\n",
      "Epoch: 200/200, Iteration: 9958/10000 --- Training Loss:0.208976\n",
      "Epoch: 200/200, Iteration: 9960/10000 --- Training Loss:0.626806\n",
      "Epoch: 200/200, Iteration: 9962/10000 --- Training Loss:0.342080\n",
      "Epoch: 200/200, Iteration: 9964/10000 --- Training Loss:0.200850\n",
      "Epoch: 200/200, Iteration: 9966/10000 --- Training Loss:0.440299\n",
      "Epoch: 200/200, Iteration: 9968/10000 --- Training Loss:0.167016\n",
      "Epoch: 200/200, Iteration: 9970/10000 --- Training Loss:0.265198\n",
      "Epoch: 200/200, Iteration: 9972/10000 --- Training Loss:0.457302\n",
      "Epoch: 200/200, Iteration: 9974/10000 --- Training Loss:0.219237\n",
      "Epoch: 200/200, Iteration: 9976/10000 --- Training Loss:0.157879\n",
      "Epoch: 200/200, Iteration: 9978/10000 --- Training Loss:0.399202\n",
      "Epoch: 200/200, Iteration: 9980/10000 --- Training Loss:0.349639\n",
      "Epoch: 200/200, Iteration: 9982/10000 --- Training Loss:0.355880\n",
      "Epoch: 200/200, Iteration: 9984/10000 --- Training Loss:0.863518\n",
      "Epoch: 200/200, Iteration: 9986/10000 --- Training Loss:0.405450\n",
      "Epoch: 200/200, Iteration: 9988/10000 --- Training Loss:0.338603\n",
      "Epoch: 200/200, Iteration: 9990/10000 --- Training Loss:0.331418\n",
      "Epoch: 200/200, Iteration: 9992/10000 --- Training Loss:0.874499\n",
      "Epoch: 200/200, Iteration: 9994/10000 --- Training Loss:1.367763\n",
      "Epoch: 200/200, Iteration: 9996/10000 --- Training Loss:0.551150\n",
      "Epoch: 200/200, Iteration: 9998/10000 --- Training Loss:0.802319\n",
      "Epoch: 200/200, Iteration: 10000/10000 --- Training Loss:0.368563\n",
      "Epoch: 200 finished ! Train Loss: 0.41950, Test Loss: 1.73909\n",
      "Epoch consuming time: 0m 0s\n",
      "Training complete in 0m  14s\n"
     ]
    }
   ],
   "source": [
    "#predict radius\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device         = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "net_r = NetModel_r(in_channels=Inchannels,is_deconv=True,is_batchnorm=True,data_dim=label_dsp_dim,bubble_num=Bubble_num)\n",
    "if torch.cuda.is_available():\n",
    "    net_r.cuda()\n",
    "\n",
    "# Optimizer we want to use\n",
    "optimizer = torch.optim.Adam(net_r.parameters(),lr=LearnRate)\n",
    "\n",
    "train_r        = data_utils.TensorDataset(torch.from_numpy(Prediction_train),torch.from_numpy(radius_train))\n",
    "train_loader_r = data_utils.DataLoader(train_r,batch_size=BatchSize,shuffle=True)\n",
    "test_r         = data_utils.TensorDataset(torch.from_numpy(Prediction_test),torch.from_numpy(radius_test))\n",
    "test_loader_r  = data_utils.DataLoader(test_r,batch_size=TestBatchSize,shuffle=False)\n",
    "\n",
    "# Initialization\n",
    "loss1  = 0.0\n",
    "loss2  = 0.0\n",
    "step   = np.int(TrainSize/BatchSize)\n",
    "start  = time.time()\n",
    "min_loss  = float('inf')\n",
    "\n",
    "for epoch in range(Epochs): \n",
    "    epoch_loss = 0.0\n",
    "    since      = time.time()\n",
    "    for i, (images,labels) in enumerate(train_loader_r):        \n",
    "        iteration  = epoch*step+i+1\n",
    "        # Set Net with train condition\n",
    "        net_r.train()\n",
    "        \n",
    "        # Reshape data size\n",
    "        images = images.view(BatchSize,-1).float()\n",
    "        labels = labels.view(BatchSize, -1).float()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradient buffer\n",
    "        optimizer.zero_grad()     \n",
    "        \n",
    "        # Forward prediction\n",
    "        outputs = net_r(images)\n",
    "        \n",
    "        # Calculate the MSE\n",
    "        loss    = F.mse_loss(outputs,labels,reduction='sum')/(Bubble_num*BatchSize)\n",
    "        \n",
    "        if np.isnan(float(loss.item())):\n",
    "            raise ValueError('loss is nan while training')\n",
    "            \n",
    "        epoch_loss += loss.item()    \n",
    "        # Loss backward propagation    \n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss\n",
    "        if iteration % DisplayStep == 0:\n",
    "            print('Epoch: {}/{}, Iteration: {}/{} --- Training Loss:{:.6f}'.format(epoch+1, \\\n",
    "                                                                               Epochs,iteration, \\\n",
    "                                                                              step*Epochs,loss.item()))        \n",
    "    with torch.no_grad():\n",
    "        epoch_loss_test = 0.0\n",
    "        for j, (images,labels) in enumerate(test_loader_r):\n",
    "            net_r.eval()\n",
    "            images = images.view(TestBatchSize,-1).float()\n",
    "            labels = labels.view(TestBatchSize,-1).float()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net_r(images)\n",
    "            loss    = F.mse_loss(outputs,labels,reduction='sum')/(Bubble_num*TestBatchSize)\n",
    "            epoch_loss_test += loss.item()\n",
    "    # Print loss and consuming time every epoch\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        #print ('Epoch [%d/%d], Loss: %.10f' % (epoch+1,Epochs,loss.item()))          \n",
    "        print('Epoch: {:d} finished ! Train Loss: {:.5f}, Test Loss: {:.5f}'.format(epoch+1,epoch_loss/i,epoch_loss_test/j))\n",
    "        loss1 = np.append(loss1,epoch_loss/i)\n",
    "        loss2 = np.append(loss2,epoch_loss_test/j)\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Epoch consuming time: {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "    # Save net_r parameters with lowest loss\n",
    "    if epoch_loss_test < min_loss:\n",
    "        torch.save(net_r.state_dict(),models_dir+modelname+'_epoch'+'_best'+'_radius_'+str(Bubble_num)+'.pkl')\n",
    "        min_loss = epoch_loss_test\n",
    "        print ('Trained model saved: %d percent completed'% int((epoch+1)*100/Epochs))\n",
    "    \n",
    "\n",
    "# Record the consuming time\n",
    "time_elapsed = time.time() - start\n",
    "print('Training complete in {:.0f}m  {:.0f}s' .format(time_elapsed //60 , time_elapsed % 60))\n",
    "\n",
    "# Save the loss\n",
    "SaveTrainResults_radius(loss=loss1,loss_test=loss2,SavePath=results_dir,bubble_num=Bubble_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7efa8a",
   "metadata": {},
   "source": [
    "# Load the Trained Radius Inversion Network with Lowest Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e34049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** Loading the best trained model *****************\n",
      "\n",
      "Finish downloading: /home/pingchuan/TokyoBayInversion_2022_3_14/models/SimulataModel/Simulate_FCNVMBModel_TrainSize500_Epoch200_BatchSize10_LR0.005_epoch_best_radius_1.pkl\n"
     ]
    }
   ],
   "source": [
    "#radius\n",
    "if ReUse_best:\n",
    "    print('***************** Loading the best trained model *****************')\n",
    "    print('')\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    device         = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "    net_r = NetModel_r(in_channels=Inchannels,is_deconv=True,is_batchnorm=True,data_dim=label_dsp_dim,bubble_num=Bubble_num)\n",
    "    premodel_file = models_dir+modelname+'_epoch'+'_best'+'_radius_'+str(Bubble_num)+'.pkl'\n",
    "    if torch.cuda.is_available():\n",
    "        net_r.cuda()\n",
    "    ##Load generator parameters\n",
    "    net_r.load_state_dict(torch.load(premodel_file))\n",
    "    net_r.to(device)\n",
    "    print('Finish downloading:',str(premodel_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b466690",
   "metadata": {},
   "source": [
    "# Predicting Radius From Predicted Velocity by Radius Inversion Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "511ec575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******************************************\n",
      "*******************************************\n",
      "            START TESTING                  \n",
      "*******************************************\n",
      "*******************************************\n",
      "\n",
      "Testing complete in  0m 0s\n"
     ]
    }
   ],
   "source": [
    "#test radius\n",
    "print() \n",
    "print('*******************************************') \n",
    "print('*******************************************') \n",
    "print('            START TESTING                  ') \n",
    "print('*******************************************') \n",
    "print('*******************************************') \n",
    "print()\n",
    "\n",
    "# Initialization\n",
    "since      = time.time()\n",
    "Prediction = np.zeros((TestSize,Bubble_num),dtype=float)\n",
    "GT         = np.zeros((TestSize,Bubble_num),dtype=float) #GT = Ground Truth\n",
    "total      = 0\n",
    "for i, (images,labels) in enumerate(test_loader_r):        \n",
    "    images = images.view(TestBatchSize, -1).float()\n",
    "    labels = labels.view(TestBatchSize, -1).float()\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Predictions\n",
    "    net_r.eval() \n",
    "    outputs  = net_r(images)\n",
    "    outputs  = outputs.view(TestBatchSize,-1)\n",
    "    outputs  = outputs.data.cpu().numpy()\n",
    "    gts      = labels.data.cpu().numpy()\n",
    "    \n",
    "    # Calculate Prediction and GroundTruth\n",
    "    for k in range(TestBatchSize):\n",
    "        pd   = outputs[k,:].reshape(-1)\n",
    "        gt   = gts[k,:].reshape(-1)\n",
    "        Prediction[i*TestBatchSize+k,:] = pd\n",
    "        GT[i*TestBatchSize+k,:] = gt\n",
    "        total = total + 1\n",
    "\n",
    "# Save Results\n",
    "SaveTestResults_radius(Prediction,GT,results_dir,Bubble_num)\n",
    "        \n",
    "# Record the consuming time\n",
    "time_elapsed = time.time() - since\n",
    "print('Testing complete in  {:.0f}m {:.0f}s' .format(time_elapsed // 60, time_elapsed % 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
